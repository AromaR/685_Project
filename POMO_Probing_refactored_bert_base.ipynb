{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POMO_Probing_refactored_bert_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AromaR/685_Project/blob/main/POMO_Probing_refactored_bert_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_YUtsc861uh"
      },
      "source": [
        "Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_EqQmcQ65EC",
        "outputId": "80fe625d-1f06-4bb6-cd61-e12e1c9e4c31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv0b-pJhqOlk",
        "outputId": "5ccaa892-ee4e-42ca-e1e0-5a85e27b83ed"
      },
      "source": [
        "import torch\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoSz7CYvA9OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe998fe-aa49-4874-ae5e-5b0af3cac4dd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGJX0GWD7Ha8"
      },
      "source": [
        "# from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/spanbert-large-finetuned-tacred\")\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"mrm8488/spanbert-large-finetuned-tacred\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AK4EsKwYJIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6d0ab251-c95a-4c04-f7b9-8228ec56d193"
      },
      "source": [
        "train_data = []\n",
        "with open(\"/content/drive/My Drive/POMO/emb_04_train_base\", \"r\") as f:\n",
        "  s = f.read()\n",
        "  s = s.replace(\"grad_fn=<SelectBackward>\",\"\")\n",
        "  s = s.replace(\" \",\"\")\n",
        "  s = s.replace(\"(\",\"\")\n",
        "  s = s.replace(\")\",\"\")\n",
        "  s = s.replace(\"[\",\"\")\n",
        "  s = s.replace(\"]\",\"\")\n",
        "  s = s.replace(\"tensor\",\"\")\n",
        "  s = s.replace(\"\\n\",\"\")\n",
        "  s = s.replace(\"'\",\"\")\n",
        "  h = s.split(\"|\")\n",
        "  for i in h:\n",
        "    if i:\n",
        "      y = i.split(\"$\")#y[1] is 0/1\n",
        "      x = y[0].split(\",\")\n",
        "      temp = []\n",
        "      for a in x:\n",
        "        if a:\n",
        "          c = a.split(\"e\")\n",
        "          if len(c)>1:\n",
        "            c[1] = c[1].replace(\"0\",\"\",1)\n",
        "            c[0] = c[0]+\"e\"+c[1]\n",
        "          temp.append(float(c[0]))\n",
        "      if len(y)>1:\n",
        "        train_data.append([[[temp]],int(y[1])])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9126e1facf7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"e\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVBE7flSbY3s"
      },
      "source": [
        "with open(\"/content/drive/My Drive/POMO/emb_05_train_base\", \"r\") as f:\n",
        "  s = f.read()\n",
        "  s = s.replace(\"grad_fn=<SelectBackward>\",\"\")\n",
        "  s = s.replace(\" \",\"\")\n",
        "  s = s.replace(\"(\",\"\")\n",
        "  s = s.replace(\")\",\"\")\n",
        "  s = s.replace(\"[\",\"\")\n",
        "  s = s.replace(\"]\",\"\")\n",
        "  s = s.replace(\"tensor\",\"\")\n",
        "  s = s.replace(\"\\n\",\"\")\n",
        "  s = s.replace(\"'\",\"\")\n",
        "  h = s.split(\"|\")\n",
        "  for i in h:\n",
        "    if i:\n",
        "      y = i.split(\"$\")#y[1] is 0/1\n",
        "      x = y[0].split(\",\")\n",
        "      temp = []\n",
        "      for a in x:\n",
        "        if a:\n",
        "          c = a.split(\"e\")\n",
        "          if len(c)>1:\n",
        "            c[1] = c[1].replace(\"0\",\"\",1)\n",
        "            c[0] = c[0]+\"e\"+c[1]\n",
        "          temp.append(float(c[0]))\n",
        "      if len(y)>1:\n",
        "        train_data.append([[[temp]],int(y[1])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7VUlT3jg9LV"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_ibcpAhK9_"
      },
      "source": [
        "test_data = []\n",
        "with open(\"/content/drive/My Drive/POMO/emb_04_test_base\", \"r\") as f:\n",
        "  s = f.read()\n",
        "  s = s.replace(\"grad_fn=<SelectBackward>\",\"\")\n",
        "  s = s.replace(\" \",\"\")\n",
        "  s = s.replace(\"(\",\"\")\n",
        "  s = s.replace(\")\",\"\")\n",
        "  s = s.replace(\"[\",\"\")\n",
        "  s = s.replace(\"]\",\"\")\n",
        "  s = s.replace(\"tensor\",\"\")\n",
        "  s = s.replace(\"\\n\",\"\")\n",
        "  s = s.replace(\"'\",\"\")\n",
        "  h = s.split(\"|\")\n",
        "  for i in h:\n",
        "    if i:\n",
        "      y = i.split(\"$\")#y[1] is 0/1\n",
        "      x = y[0].split(\",\")\n",
        "      temp = []\n",
        "      for a in x:\n",
        "        if a:\n",
        "          c = a.split(\"e\")\n",
        "          if len(c)>1:\n",
        "            c[1] = c[1].replace(\"0\",\"\",1)\n",
        "            c[0] = c[0]+\"e\"+c[1]\n",
        "          temp.append(float(c[0]))\n",
        "      if len(y)>1:\n",
        "        test_data.append([[[temp]],int(y[1])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcIqg9-ShU3M"
      },
      "source": [
        "print(len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6H3Yu5vhy8B"
      },
      "source": [
        "train = []\n",
        "test = []\n",
        "valid = []\n",
        "trainans = []\n",
        "testans = []\n",
        "validans = []\n",
        "for i in train_data:\n",
        "  train.append(i[0])\n",
        "  trainans.append(i[1])\n",
        "for i in test_data:\n",
        "  test.append(i[0])\n",
        "  testans.append(i[1])\n",
        "valid = train[15000:]\n",
        "validans = trainans[15000:]\n",
        "train = train[0:15000]\n",
        "trainans = trainans[0:15000]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0luR9oF6pRt"
      },
      "source": [
        "# dataset = pd.read_csv(\"/content/drive/My Drive/POMO/train.pm\",sep='\\\\t')\n",
        "# dataarr = []\n",
        "# ansarr = []\n",
        "# for i in range(15000,20000):\n",
        "#   dataarr.append(dataset.iloc[i,0])\n",
        "#   ansarr.append(0)\n",
        "#   dataarr.append(dataset.iloc[i,3])\n",
        "#   ansarr.append(1)\n",
        "# torch.set_printoptions(profile=\"full\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jpeqJwGVlHt"
      },
      "source": [
        "# with open(\"/content/drive/My Drive/POMO/emb_04_train\", \"w\") as writer:\n",
        "#   for i in tqdm(range(0, len(dataarr), 128)):\n",
        "#     batch = dataarr[i:i+128]\n",
        "#     ans = ansarr[i:i+128]\n",
        "#     inputs = tokenizer.batch_encode_plus(batch,\n",
        "#                 add_special_tokens=True,\n",
        "#                 truncation=True,\n",
        "#                 padding=True,\n",
        "#                 return_tensors=\"pt\",\n",
        "#             )\n",
        "#     for j in range(0,128):\n",
        "#       try:\n",
        "#         writer.write(\"|\")\n",
        "#         writer.write(str(model(torch.tensor(inputs['input_ids'][j]).unsqueeze(0))[0][0][0]))\n",
        "#         writer.write(\"$\")\n",
        "#         writer.write(str(ans[j]))\n",
        "#       except:\n",
        "#         continue\n",
        "# torch.set_printoptions(profile=\"default\")\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrGFstbx6qv7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT9rsChkEaQD"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/My Drive/POMO/test.pm\",sep='\\\\t')\n",
        "dataarr_test = []\n",
        "# ansarr_test = []\n",
        "# print(\"here we add to a 2d array, a sentence and a number 1/0 according to whether it contains appositions. The first sentence at pos 0 is not appositional, the one at position 3 is\")\n",
        "for i in range(0,len(dataset)):\n",
        "  dataarr_test.append(dataset.iloc[i,0])\n",
        "#   ansarr_test.append(0)\n",
        "  dataarr_test.append(dataset.iloc[i,3])\n",
        "#   ansarr_test.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euSw5wa2Em_u"
      },
      "source": [
        "# dataset = pd.read_csv(\"/content/drive/My Drive/POMO/valid.pm\",sep='\\\\t')\n",
        "# dataarr_valid = []\n",
        "# ansarr_valid = []\n",
        "# print(\"here we add to a 2d array, a sentence and a number 1/0 according to whether it contains appositions. The first sentence at pos 0 is not appositional, the one at position 3 is\")\n",
        "# for i in range(0,len(dataset)):\n",
        "#   dataarr_valid.append(dataset.iloc[i,0])\n",
        "#   ansarr_valid.append(0)\n",
        "#   dataarr_valid.append(dataset.iloc[i,3])\n",
        "#   ansarr_valid.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxPzMpeoeyT7"
      },
      "source": [
        "# print(len(dataarr))\n",
        "# print(dataarr[0])\n",
        "# print(ansarr[0])\n",
        "# print(dataarr[1])\n",
        "# print(ansarr[1])\n",
        "# h = []\n",
        "# for j in ansarr:\n",
        "#   h.append(torch.LongTensor([j]).unsqueeze(0))\n",
        "# ansarr1 = torch.stack((h))\n",
        "# print(ansarr1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pvqeOlVE0T9"
      },
      "source": [
        "# print(len(dataarr_test))\n",
        "# print(dataarr_test[0])\n",
        "# print(ansarr_test[0])\n",
        "# print(dataarr_test[1])\n",
        "# print(ansarr_test[1])\n",
        "# ansarr1_test = torch.from_numpy(np.array(ansarr_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzDaXXrbE_kR"
      },
      "source": [
        "# print(len(dataarr_valid))\n",
        "# print(dataarr_valid[0])\n",
        "# print(ansarr_valid[0])\n",
        "# print(dataarr_valid[1])\n",
        "# print(ansarr_valid[1])\n",
        "# ansarr1_valid = torch.from_numpy(np.array(ansarr_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvsP0z4s65WQ"
      },
      "source": [
        "Creation of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4XeF0yc7HGx"
      },
      "source": [
        "Conversion to hugging faces BERT-Tacred fine-tuned representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGA-kztmBb4I"
      },
      "source": [
        "class POMO_Dataset(torch.utils.data.dataset.Dataset):\n",
        "    def __init__(self, _dataset):\n",
        "        self.dataset = _dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example= self.dataset[0][index]\n",
        "        target = self.dataset[1][index]\n",
        "        target.squeeze(0)\n",
        "        return np.array(example), target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXFtnGCrFoae"
      },
      "source": [
        "# emb_data = tokenizer.batch_encode_plus(dataarr_valid)\n",
        "# k = [torch.LongTensor(x) for x in emb_data['input_ids']]\n",
        "# max_len = max([len(x) for x in emb_data['input_ids']])\n",
        "# data = [torch.nn.functional.pad(x, pad=(0, max_len - x.numel()), mode='constant', value=0) for x in k]\n",
        "# data = torch.stack(data)\n",
        "# valid = []\n",
        "# for i in range(0,len(data)):\n",
        "#   valid.append([data[i],ansarr1_valid[i]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GIurOTTpGe4"
      },
      "source": [
        "# emb_data = tokenizer.batch_encode_plus(dataarr)\n",
        "# k = [torch.LongTensor(x) for x in emb_data['input_ids']]\n",
        "# max_len = max([len(x) for x in emb_data['input_ids']])\n",
        "# data = [torch.nn.functional.pad(x, pad=(0, max_len - x.numel()), mode='constant', value=0) for x in k]\n",
        "# data = torch.stack(data)\n",
        "# train = []\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-HJ4eyuP9W"
      },
      "source": [
        "Get Bert embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pMQoKcmuR-P"
      },
      "source": [
        "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my name is Aroma\")).unsqueeze(0)  # Batch size 1\n",
        "# outputs = model(input_ids)\n",
        "# last_hidden_states = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hrIF0KXwY88"
      },
      "source": [
        "# print(len(last_hidden_states[0][0])) #CLS baby!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvEvbvF1vArA"
      },
      "source": [
        "# ansarr.unsqueeze(0).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG3gvw5YcnyR"
      },
      "source": [
        "# a = []\n",
        "# for i in data:\n",
        "#   a.append(i.unsqueeze(0).unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B4lg456ue0Y"
      },
      "source": [
        "# a = torch.stack(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1AvE6uHsNJ5"
      },
      "source": [
        "# print(a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z43Pd0S5Z8Yj"
      },
      "source": [
        "# train = []\n",
        "# train = [a,ansarr1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_bJilzNbzv4"
      },
      "source": [
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp15CDMqbNh5"
      },
      "source": [
        "# print((train[0][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPNDmsXeoMXC"
      },
      "source": [
        "\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=POMO_Dataset(test),\n",
        "#                                            batch_size=4,\n",
        "#                                            shuffle=True)\n",
        "# valid_loader = torch.utils.data.DataLoader(dataset=POMO_Dataset(valid),\n",
        "#                                            batch_size=4,\n",
        "#                                            shuffle=True)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset=POMO_Dataset(train),\n",
        "#                                            batch_size=4,\n",
        "#                                            shuffle=True)\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thh1ZvJpcQGD"
      },
      "source": [
        "# for batch_images, targets in train_loader:\n",
        "#     print(batch_images.shape)\n",
        "#     print(targets)\n",
        "#     break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7j5yy4VHAJW"
      },
      "source": [
        "metrics = [] #we will add mean accuracy on task, control task 1 and control task 2 here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYIYLFgR6_D1"
      },
      "source": [
        "Probing task level 1 : Classifier - binary, Aroma's CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwjxeL-fv1Qg"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 2, 1,)\n",
        "#         self.fc1 = nn.Linear(801, 120)\n",
        "#         self.fc2 = nn.Linear(120, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = (F.relu(self.conv1(x.long()))).long()\n",
        "#         x = F.relu(self.fc1(x)).long()\n",
        "#         x = (self.fc2(x)).long()\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KtTpdHz_KKk"
      },
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-0IkvdoSWmW"
      },
      "source": [
        "# for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_loader, 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = net(inputs.long())\n",
        "#         print(labels)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "#             print('[%d, %5d] loss: %.3f' %\n",
        "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dgOPSC_PNY"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uj2oGnhfJze"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=1, activation='relu', input_shape=(1,1,768)))\n",
        "model.add(Conv2D(32, kernel_size=1, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6k6X48EfsdX"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwtARYEluiM"
      },
      "source": [
        "train.extend(valid)\n",
        "trainans.extend(validans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFmIXwVTf-L5"
      },
      "source": [
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV3MEIWEyrfL"
      },
      "source": [
        "Control Task: Let us see if our model is too complex and makes sense of nonsense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsEgqTM2zHJr"
      },
      "source": [
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss9uodp31OmC"
      },
      "source": [
        "This is a prediction rate that is about the same as a random rate. Let us see if the information learned by the model is by virtue of the language embeddings or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZUK-djd3fxa"
      },
      "source": [
        "cont_train = []\n",
        "for i in train:\n",
        "  cont_train.append([[np.add(np.array(i[0][0]),np.random.uniform(low=-1, high=1, size=len(i[0][0]) )).tolist()]])\n",
        "cont_test = []\n",
        "for i in test:\n",
        "  cont_test.append([[np.add(np.array(i[0][0]),np.random.uniform(low=-1, high=1, size=len(i[0][0]) )).tolist()]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qy_1AWi4oyf"
      },
      "source": [
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omr-ZB8V11To"
      },
      "source": [
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hu21hQQB49X"
      },
      "source": [
        "As we can see the randomized addition to the embeddings has resulted in worse accuracy. Let us now experiment with a different version of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf2tbmUyCE1M"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=1, activation='relu', input_shape=(1,1,768)))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=1, activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=1, activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6533B3rrGd-z"
      },
      "source": [
        "a,b,c,d,e,f = [],[],[],[],[],[]\n",
        "for i in train:\n",
        "  a.append([i[0][0]])\n",
        "for j in test:\n",
        "  b.append([j[0][0]])\n",
        "for l in cont_train:\n",
        "  d.append([l[0][0]])\n",
        "for n in cont_test:\n",
        "  f.append([n[0][0]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjoFJn4IKUfR"
      },
      "source": [
        "print(len(a[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAy35qTgK32i"
      },
      "source": [
        "train = a\n",
        "test = b\n",
        "cont_train = d\n",
        "cont_test = f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCSrdrHkE9VE"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Embedding\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu',input_shape=(1,768)))\n",
        "model.add(Dropout(0.50))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mv2dxggOdT5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Embedding\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu',input_shape=(1,768)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID9N197tPEnQ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Embedding\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu',input_shape=(1,768)))\n",
        "model.add(Conv1D(filters=16,kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffRHg0dzQ3yT"
      },
      "source": [
        "!pip install keras-tuner\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OweZ-RLgRc6P"
      },
      "source": [
        "import kerastuner as kt\n",
        "import keras\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-O3Sn2YOa6"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0005)\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEpzFmPRaXa2"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0005)\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewBumFx9b_I4"
      },
      "source": [
        "\n",
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0001)\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ8Sc_vig2fR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0HpZ41oeGTc"
      },
      "source": [
        "\n",
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.01)\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wBxxwdIbUP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St1LIXbhlDD9"
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0005)\n",
        "hist = model.fit(train, trainans, validation_data=(test, testans), epochs=30)\n",
        "preds = model.predict(test)\n",
        "a = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(train, np.random.choice([0, 1], size=len(trainans)).tolist(), validation_data=(test, np.random.choice([0, 1], size=len(testans)).tolist()), epochs=30)\n",
        "b = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "hist = model.fit(cont_train, trainans, validation_data=(cont_test, testans), epochs=30)\n",
        "c = (np.mean(hist.history.get(\"val_accuracy\")))\n",
        "metrics.append((a,b,c))\n",
        "almostright = []\n",
        "totallywrong = []\n",
        "m = 0\n",
        "for i,j in zip(preds,testans):\n",
        "  if (i>0.4 and i<0.5 and j==1) or (i<0.6 and i>0.5 and j==0):\n",
        "    almostright.append((m,i))\n",
        "  if (i<0.20 and j==1) or (i>0.80 and j==0):\n",
        "    totallywrong.append((m,i))\n",
        "  m = m+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs7q-NV5OB-N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql3yocZ4l5vo"
      },
      "source": [
        "for i in almostright[0:10]:\n",
        "  print(dataarr_test[i[0]],i[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_J6ta1KmgzH"
      },
      "source": [
        "for i in totallywrong[0:10]:\n",
        "  print(dataarr_test[i[0]],i[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}