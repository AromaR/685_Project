{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "685_probing+training",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AromaR/685_Project/blob/main/685_probing%2Btraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkZ7RqHG2blu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d242386-5f9c-43ca-9028-bc77233605cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHbSORAiuwU4",
        "outputId": "62b8fdee-c184-42d5-9e96-82a19187f861"
      },
      "source": [
        "!pip install -r /content/drive/My\\ Drive/685_project/REval/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/SentEval.git (from -r /content/drive/My Drive/685_project/REval/requirements.txt (line 4))\n",
            "  Cloning https://github.com/facebookresearch/SentEval.git to /tmp/pip-req-build-h54ewur1\n",
            "  Running command git clone -q https://github.com/facebookresearch/SentEval.git /tmp/pip-req-build-h54ewur1\n",
            "Collecting git+https://github.com/DFKI-NLP/RelEx.git (from -r /content/drive/My Drive/685_project/REval/requirements.txt (line 5))\n",
            "  Cloning https://github.com/DFKI-NLP/RelEx.git to /tmp/pip-req-build-rc5sirdm\n",
            "  Running command git clone -q https://github.com/DFKI-NLP/RelEx.git /tmp/pip-req-build-rc5sirdm\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/685_project/REval/requirements.txt (line 2)) (0.0)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.7MB/s \n",
            "\u001b[?25hCollecting allennlp~=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 57.1MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/71/4f04aed03ca35f2d02e1732ca6e996b2d7b40232fb7f1b58ff35f9a89b7b/responses-0.12.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.4.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.9MB/s \n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/8f/93e490e99c25bebbc79f762448866acf4b942ea6666423ac308c2aedb800/boto3-1.16.29-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.2MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.10.0)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.18.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2018.9)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.1MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.7.0+cu101)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.2.5)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.1.2)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 109kB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.6.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.12.4)\n",
            "Collecting urllib3>=1.25.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.0.4)\n",
            "Collecting botocore<1.20.0,>=1.19.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/90/bb838125929b0226ea672f5ba80469aabc915ec10b3f9a8e75ede1f062a7/botocore-1.19.29-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 54.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (50.3.2)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.8)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (7.1.2)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.8MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.8.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (8.6.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (20.4)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.16)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->relex==0.0.2a0->-r /content/drive/My Drive/685_project/REval/requirements.txt (line 5)) (1.1.4)\n",
            "Building wheels for collected packages: fire, SentEval, relex, overrides, ftfy, word2number, jsonnet, parsimonious\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=b6455b855f1c7f10dc66f66f2314a375e1f451346445b7b85a292d79bde3bbbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "  Building wheel for SentEval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SentEval: filename=SentEval-0.1.0-cp36-none-any.whl size=34995 sha256=0cce4cdbf6064499d9f418ffabb399cec3609021ec780fa8e1544fb553c698f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ynejrouj/wheels/cf/96/1e/e160918167cb931b60ea4f65eaa7dd152032a93da053e34896\n",
            "  Building wheel for relex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for relex: filename=relex-0.0.2a0-cp36-none-any.whl size=34958 sha256=202bbffe36303d628a6c5b7eea0538c7b5c9254d710ea6cb2ac212d2dc930140\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ynejrouj/wheels/c1/e6/62/3b4cf92294e41c5126af57b4e2b4daeb0a17858342fa29be28\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=6eb44333b1eae3c14c1484e4cc5674d381583b3d84715735f48a1e05f88d9a9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=6a3e36517112dad802a90eb7affcbf2101e1ad808ce571454ce217fcea956e72\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=bb562eccc1ccbca0f9daf4223f66ed3b33ee5f4a8a6f267fa0db5b365464ed07\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387897 sha256=19aac3a2bec910d8c7d1fd954e4d3f5364adf56e55e4502d4ad80039fd7b77d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=543e1837392f22810e28061a14c870fcaeeb59539a3531936573a01a8d659888\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "Successfully built fire SentEval relex overrides ftfy word2number jsonnet parsimonious\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, SentEval, tensorboardX, urllib3, responses, unidecode, sentencepiece, jmespath, botocore, s3transfer, boto3, pytorch-transformers, greenlet, zope.event, zope.interface, gevent, overrides, ftfy, conllu, word2number, pytorch-pretrained-bert, jsonnet, flask-cors, flaky, numpydoc, blis, preshed, plac, thinc, spacy, parsimonious, jsonpickle, allennlp, relex\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.4\n",
            "    Uninstalling preshed-3.0.4:\n",
            "      Successfully uninstalled preshed-3.0.4\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed SentEval-0.1.0 allennlp-0.9.0 blis-0.2.4 boto3-1.16.29 botocore-1.19.29 conllu-1.3.1 fire-0.3.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.9.0 greenlet-0.4.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 relex-0.0.2a0 responses-0.12.1 s3transfer-0.3.3 sentencepiece-0.1.94 spacy-2.1.9 tensorboardX-2.1 thinc-7.0.8 unidecode-1.1.1 urllib3-1.26.2 word2number-1.1 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GXVsoW0aHeU"
      },
      "source": [
        "# Part 0. Set Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axGrBKGnaL9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bccaac8-f1a8-4866-f1b7-fb6c68a73deb"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8PLI0e3Ta37",
        "outputId": "cf1d3009-aa9e-4759-ad7c-c821cce4748a"
      },
      "source": [
        "%cd '/content/drive/My Drive/685_project/RelEx'\n",
        "!pip install -r ./requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1CufDBo-hpFfbRuuFiQygHzePR82CMlmm/685_project/RelEx\n",
            "Requirement already satisfied: allennlp~=0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r ./requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from -r ./requirements.txt (line 5)) (3.6.4)\n",
            "Requirement already satisfied: conllu==1.3.1 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.1)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (20.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.16.29)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (5.8)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp~=0.9.0->-r ./requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (50.3.2)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (20.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (8.6.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->-r ./requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.0.4)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.1.94)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (5.2.0)\n",
            "Requirement already satisfied: greenlet>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.4.17)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.29 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.19.29)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: urllib3>=1.25.10 in /usr/local/lib/python3.6/dist-packages (from responses>=0.7->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.26.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2020.11.8)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.2.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (20.4)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.2.4)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.16)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp~=0.9.0->-r ./requirements.txt (line 2)) (1.1.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJIz1K-CaObF"
      },
      "source": [
        "# !git clone https://github.com/DFKI-NLP/RelEx\n",
        "# %cd RelEx\n",
        "# !pip install ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwFT9EK3BWHd"
      },
      "source": [
        "# Part 1. Run probing evaluation using pretrained cnn_semval model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDJxUyCKIjTn"
      },
      "source": [
        "# Generate SemEval probing data from Google drive\n",
        "# THIS CELL DOESN'T NEED TO BE RUN AGAIN. IF YOU WANT TO TRY OUT, CHANGE TO A NEW DIRECTORY\n",
        "!python /content/drive/My\\ Drive/685_project/REval/reval.py generate-all-from-semeval \\\n",
        "         --train-file /content/drive/My\\ Drive/685_project/SemEval_original_data/train.json \\\n",
        "         --validation-file /content/drive/My\\ Drive/685_project/SemEval_original_data/dev.json \\\n",
        "         --test-file /content/drive/My\\ Drive/685_project/SemEval_original_data/test.json \\\n",
        "         --output-dir /content/drive/My\\ Drive/685_project/REval/data/semeval/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tdNcBvpZK0ed"
      },
      "source": [
        "# Generate TACRED probing data from Google drive\n",
        "# THIS CELL DOESN'T NEED TO BE RUN AGAIN. IF YOU WANT TO TRY OUT, CHANGE TO A NEW DIRECTORY\n",
        "!python /content/drive/My\\ Drive/685_project/REval/reval.py generate-all-from-semeval \\\n",
        "         --train-file /content/drive/My\\ Drive/685_project/TACRED_original_data/train.json \\\n",
        "         --validation-file /content/drive/My\\ Drive/685_project/TACRED_original_data/dev.json \\\n",
        "         --test-file /content/drive/My\\ Drive/685_project/TACRED_original_data/test.json \\\n",
        "         --output-dir /content/drive/My\\ Drive/685_project/REval/data/TACRED/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc1Wg2y5CGYY",
        "outputId": "50a3294d-4960-4bd5-fd35-54d95692f29b"
      },
      "source": [
        "# Probing Task Evaluation on SemEval\n",
        "# Run this to see how it's working\n",
        "!python /content/drive/My\\ Drive/685_project/REval/probing_task_evaluation.py \\\n",
        "        --model-dir /content/drive/My\\ Drive/685_project/REval/models/cnn_semeval/ \\\n",
        "        --data-dir /content/drive/My\\ Drive/685_project/REval/data/semeval/ \\\n",
        "        --dataset semeval2010 --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/685_project/REval/probing_task_evaluation.py\", line 249, in <module>\n",
            "    result_file_name=args.result_file_name,\n",
            "  File \"/content/drive/My Drive/685_project/REval/probing_task_evaluation.py\", line 122, in run_evaluation\n",
            "    weights_file=None,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/relex/predictors/predictor_utils.py\", line 14, in load_predictor\n",
            "    archive = load_archive(archive_path, cuda_device, weights_file)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/models/archival.py\", line 170, in load_archive\n",
            "    resolved_archive_file = cached_path(archive_file)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/file_utils.py\", line 106, in cached_path\n",
            "    raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n",
            "FileNotFoundError: file /content/drive/My Drive/685_project/REval/models/cnn_semeval/model.tar.gz not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgW4EetxyISg"
      },
      "source": [
        "# Part 2. Train individual models using RelEx code and original TACRED data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEjbinAg-DgU",
        "outputId": "28a6b109-a15d-4ff3-ff59-10634001a881"
      },
      "source": [
        "%cd '/content/drive/My Drive/685_project/RelEx'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1CufDBo-hpFfbRuuFiQygHzePR82CMlmm/685_project/RelEx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPuhfJoL9ePm",
        "outputId": "173b83d7-53a2-48a5-877e-30685ca56ab0"
      },
      "source": [
        "# Shows current Colab directory\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1CufDBo-hpFfbRuuFiQygHzePR82CMlmm/685_project/RelEx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCYgMY4ByZkx",
        "outputId": "6d332879-b110-4d8e-fe7b-3901ac849a95"
      },
      "source": [
        "# Train a baseline GCN model for relation extraction (TACRED)\n",
        "!allennlp train \\\n",
        "     ./configs/relation_classification/tacred/baseline_gcn_tacred.jsonnet \\\n",
        "    -s '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred' \\   # Save directory. RENAME it for new models\n",
        "    --include-package relex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-11-26 09:56:29,397 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 09:56:29,921 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 09:56:29,925 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 09:56:30,401 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 09:56:30,402 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 09:56:30,403 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 09:56:30,404 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 09:56:31,210 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2020-11-26 09:56:31,211 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2020-11-26 09:56:31,211 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2020-11-26 09:56:31,217 - INFO - allennlp.common.checks - Pytorch version: 1.7.0+cu101\n",
            "2020-11-26 09:56:31,223 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2020-11-26 09:56:31,224 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2020-11-26 09:56:31,224 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'ner_tokens': {'type': 'ner_tag'}, 'pos_tokens': {'type': 'pos_tag'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'tacred'} and extras set()\n",
            "2020-11-26 09:56:31,225 - INFO - allennlp.common.params - dataset_reader.type = tacred\n",
            "2020-11-26 09:56:31,225 - INFO - allennlp.common.from_params - instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'ner_tokens': {'type': 'ner_tag'}, 'pos_tokens': {'type': 'pos_tag'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2020-11-26 09:56:31,226 - INFO - allennlp.common.params - dataset_reader.max_len = 100\n",
            "2020-11-26 09:56:31,226 - INFO - allennlp.common.params - dataset_reader.masking_mode = NER+Grammar\n",
            "2020-11-26 09:56:31,226 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-11-26 09:56:31,227 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'ner_tag'} and extras set()\n",
            "2020-11-26 09:56:31,227 - INFO - allennlp.common.params - dataset_reader.token_indexers.ner_tokens.type = ner_tag\n",
            "2020-11-26 09:56:31,227 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer from params {} and extras set()\n",
            "2020-11-26 09:56:31,227 - INFO - allennlp.common.params - dataset_reader.token_indexers.ner_tokens.namespace = ner_tokens\n",
            "2020-11-26 09:56:31,228 - INFO - allennlp.common.params - dataset_reader.token_indexers.ner_tokens.token_min_padding_length = 0\n",
            "2020-11-26 09:56:31,228 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'pos_tag'} and extras set()\n",
            "2020-11-26 09:56:31,228 - INFO - allennlp.common.params - dataset_reader.token_indexers.pos_tokens.type = pos_tag\n",
            "2020-11-26 09:56:31,229 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer from params {} and extras set()\n",
            "2020-11-26 09:56:31,229 - INFO - allennlp.common.params - dataset_reader.token_indexers.pos_tokens.namespace = pos_tokens\n",
            "2020-11-26 09:56:31,229 - INFO - allennlp.common.params - dataset_reader.token_indexers.pos_tokens.coarse_tags = False\n",
            "2020-11-26 09:56:31,229 - INFO - allennlp.common.params - dataset_reader.token_indexers.pos_tokens.token_min_padding_length = 0\n",
            "2020-11-26 09:56:31,230 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2020-11-26 09:56:31,230 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-11-26 09:56:31,230 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
            "2020-11-26 09:56:31,231 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-11-26 09:56:31,231 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2020-11-26 09:56:31,231 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-11-26 09:56:31,231 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-11-26 09:56:31,232 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-11-26 09:56:31,232 - INFO - allennlp.common.params - dataset_reader.dep_pruning = 1\n",
            "2020-11-26 09:56:31,232 - INFO - allennlp.common.params - train_data_path = ../relex-data/tacred/train.json\n",
            "2020-11-26 09:56:31,232 - INFO - allennlp.training.util - Reading training data from ../relex-data/tacred/train.json\n",
            "0it [00:00, ?it/s]2020-11-26 09:56:31,235 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/train.json\n",
            "68124it [00:28, 2351.85it/s]\n",
            "2020-11-26 09:57:00,200 - INFO - allennlp.common.params - validation_data_path = ../relex-data/tacred/dev.json\n",
            "2020-11-26 09:57:00,201 - INFO - allennlp.training.util - Reading validation data from ../relex-data/tacred/dev.json\n",
            "0it [00:00, ?it/s]2020-11-26 09:57:00,202 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/dev.json\n",
            "22631it [00:09, 2408.09it/s]\n",
            "2020-11-26 09:57:09,600 - INFO - allennlp.common.params - test_data_path = None\n",
            "2020-11-26 09:57:09,772 - INFO - allennlp.training.trainer_pieces - From dataset instances, validation, train will be considered for vocabulary creation.\n",
            "2020-11-26 09:57:09,772 - INFO - allennlp.common.params - vocabulary.type = None\n",
            "2020-11-26 09:57:09,773 - INFO - allennlp.common.params - vocabulary.extend = False\n",
            "2020-11-26 09:57:09,773 - INFO - allennlp.common.params - vocabulary.directory_path = None\n",
            "2020-11-26 09:57:09,773 - INFO - allennlp.common.params - vocabulary.min_count = {'tokens': 2}\n",
            "2020-11-26 09:57:09,773 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2020-11-26 09:57:09,774 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2020-11-26 09:57:09,774 - INFO - allennlp.common.params - vocabulary.pretrained_files = {}\n",
            "2020-11-26 09:57:09,774 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2020-11-26 09:57:09,774 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2020-11-26 09:57:09,774 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2020-11-26 09:57:09,775 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "90755it [00:06, 14857.63it/s]\n",
            "2020-11-26 09:57:15,990 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['relu', 'relu', 'linear'], 'dropout': [0, 0, 0], 'hidden_dims': [200, 200, 42], 'input_dim': 600, 'num_layers': 3}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'text_encoder': {'dropout': 0.5, 'hidden_size': 200, 'input_size': 360, 'num_layers': 2, 'pooling': 'max', 'type': 'gcn'}, 'text_field_embedder': {'ner_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'pos_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'type': 'basic_relation_classifier', 'use_adjacency': True, 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-11-26 09:57:15,991 - INFO - allennlp.common.params - model.type = basic_relation_classifier\n",
            "2020-11-26 09:57:15,991 - INFO - allennlp.common.from_params - instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['relu', 'relu', 'linear'], 'dropout': [0, 0, 0], 'hidden_dims': [200, 200, 42], 'input_dim': 600, 'num_layers': 3}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'text_encoder': {'dropout': 0.5, 'hidden_size': 200, 'input_size': 360, 'num_layers': 2, 'pooling': 'max', 'type': 'gcn'}, 'text_field_embedder': {'ner_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'pos_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'use_adjacency': True, 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-11-26 09:57:15,992 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'ner_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'pos_tokens': {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\n",
            "2020-11-26 09:57:15,992 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-11-26 09:57:15,992 - INFO - allennlp.common.params - model.text_field_embedder.embedder_to_indexer_map = None\n",
            "2020-11-26 09:57:15,993 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = False\n",
            "2020-11-26 09:57:15,993 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders = None\n",
            "2020-11-26 09:57:15,993 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-11-26 09:57:15,993 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.type = embedding\n",
            "2020-11-26 09:57:15,994 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.num_embeddings = None\n",
            "2020-11-26 09:57:15,994 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.vocab_namespace = tokens\n",
            "2020-11-26 09:57:15,994 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.embedding_dim = 30\n",
            "2020-11-26 09:57:15,994 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.pretrained_file = None\n",
            "2020-11-26 09:57:15,995 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.projection_dim = None\n",
            "2020-11-26 09:57:15,995 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.trainable = True\n",
            "2020-11-26 09:57:15,995 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.padding_index = None\n",
            "2020-11-26 09:57:15,995 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.max_norm = None\n",
            "2020-11-26 09:57:15,995 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.norm_type = 2.0\n",
            "2020-11-26 09:57:15,996 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.scale_grad_by_freq = False\n",
            "2020-11-26 09:57:15,996 - INFO - allennlp.common.params - model.text_field_embedder.ner_tokens.sparse = False\n",
            "2020-11-26 09:57:16,007 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 30, 'trainable': True, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-11-26 09:57:16,007 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.type = embedding\n",
            "2020-11-26 09:57:16,007 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.num_embeddings = None\n",
            "2020-11-26 09:57:16,008 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.vocab_namespace = tokens\n",
            "2020-11-26 09:57:16,008 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.embedding_dim = 30\n",
            "2020-11-26 09:57:16,008 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.pretrained_file = None\n",
            "2020-11-26 09:57:16,008 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.projection_dim = None\n",
            "2020-11-26 09:57:16,009 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.trainable = True\n",
            "2020-11-26 09:57:16,009 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.padding_index = None\n",
            "2020-11-26 09:57:16,009 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.max_norm = None\n",
            "2020-11-26 09:57:16,009 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.norm_type = 2.0\n",
            "2020-11-26 09:57:16,009 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.scale_grad_by_freq = False\n",
            "2020-11-26 09:57:16,010 - INFO - allennlp.common.params - model.text_field_embedder.pos_tokens.sparse = False\n",
            "2020-11-26 09:57:16,019 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-11-26 09:57:16,019 - INFO - allennlp.common.params - model.text_field_embedder.tokens.type = embedding\n",
            "2020-11-26 09:57:16,020 - INFO - allennlp.common.params - model.text_field_embedder.tokens.num_embeddings = None\n",
            "2020-11-26 09:57:16,020 - INFO - allennlp.common.params - model.text_field_embedder.tokens.vocab_namespace = tokens\n",
            "2020-11-26 09:57:16,020 - INFO - allennlp.common.params - model.text_field_embedder.tokens.embedding_dim = 300\n",
            "2020-11-26 09:57:16,020 - INFO - allennlp.common.params - model.text_field_embedder.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz\n",
            "2020-11-26 09:57:16,021 - INFO - allennlp.common.params - model.text_field_embedder.tokens.projection_dim = None\n",
            "2020-11-26 09:57:16,021 - INFO - allennlp.common.params - model.text_field_embedder.tokens.trainable = False\n",
            "2020-11-26 09:57:16,021 - INFO - allennlp.common.params - model.text_field_embedder.tokens.padding_index = None\n",
            "2020-11-26 09:57:16,021 - INFO - allennlp.common.params - model.text_field_embedder.tokens.max_norm = None\n",
            "2020-11-26 09:57:16,021 - INFO - allennlp.common.params - model.text_field_embedder.tokens.norm_type = 2.0\n",
            "2020-11-26 09:57:16,022 - INFO - allennlp.common.params - model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
            "2020-11-26 09:57:16,022 - INFO - allennlp.common.params - model.text_field_embedder.tokens.sparse = False\n",
            "2020-11-26 09:57:16,029 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n",
            "2020-11-26 09:57:16,625 - INFO - allennlp.common.file_utils - https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz not found in cache, downloading to /tmp/tmp0v3shu4n\n",
            "100%|##########| 2176768669/2176768669 [01:41<00:00, 21372628.53B/s]\n",
            "2020-11-26 09:58:59,171 - INFO - allennlp.common.file_utils - copying /tmp/tmp0v3shu4n to cache at /root/.allennlp/cache/0b88846fe4d9635db0e2cc8f37422337080361ccc153c21b68e874ec49ec0a59.8de7c14eda15038006ebf553b708bcae9c046b2ecf71dbbbedd8b0b9d0d0a621\n",
            "2020-11-26 09:59:23,339 - INFO - allennlp.common.file_utils - creating metadata file for /root/.allennlp/cache/0b88846fe4d9635db0e2cc8f37422337080361ccc153c21b68e874ec49ec0a59.8de7c14eda15038006ebf553b708bcae9c046b2ecf71dbbbedd8b0b9d0d0a621\n",
            "2020-11-26 09:59:23,340 - INFO - allennlp.common.file_utils - removing temp file /tmp/tmp0v3shu4n\n",
            "2196017it [00:56, 38590.64it/s]\n",
            "2020-11-26 10:00:20,547 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n",
            "2020-11-26 10:00:21,263 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 34922 out of 41038 tokens\n",
            "2020-11-26 10:00:21,288 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'dropout': 0.5, 'hidden_size': 200, 'input_size': 360, 'num_layers': 2, 'pooling': 'max', 'type': 'gcn'} and extras {'vocab'}\n",
            "2020-11-26 10:00:21,288 - INFO - allennlp.common.params - model.text_encoder.type = gcn\n",
            "2020-11-26 10:00:21,288 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.seq2vec_encoders.gcn.GCN'> from params {'dropout': 0.5, 'hidden_size': 200, 'input_size': 360, 'num_layers': 2, 'pooling': 'max'} and extras {'vocab'}\n",
            "2020-11-26 10:00:21,289 - INFO - allennlp.common.params - model.text_encoder.input_size = 360\n",
            "2020-11-26 10:00:21,289 - INFO - allennlp.common.params - model.text_encoder.hidden_size = 200\n",
            "2020-11-26 10:00:21,289 - INFO - allennlp.common.params - model.text_encoder.num_layers = 2\n",
            "2020-11-26 10:00:21,290 - INFO - allennlp.common.params - model.text_encoder.dropout = 0.5\n",
            "2020-11-26 10:00:21,290 - INFO - allennlp.common.params - model.text_encoder.pooling = max\n",
            "2020-11-26 10:00:21,290 - INFO - allennlp.common.params - model.text_encoder.pooling_scope = None\n",
            "2020-11-26 10:00:21,290 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 10:00:21,292 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['relu', 'relu', 'linear'], 'dropout': [0, 0, 0], 'hidden_dims': [200, 200, 42], 'input_dim': 600, 'num_layers': 3} and extras {'vocab'}\n",
            "2020-11-26 10:00:21,293 - INFO - allennlp.common.params - model.classifier_feedforward.input_dim = 600\n",
            "2020-11-26 10:00:21,293 - INFO - allennlp.common.params - model.classifier_feedforward.num_layers = 3\n",
            "2020-11-26 10:00:21,293 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [200, 200, 42]\n",
            "2020-11-26 10:00:21,294 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [200, 200, 42]\n",
            "2020-11-26 10:00:21,294 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['relu', 'relu', 'linear']\n",
            "2020-11-26 10:00:21,294 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params ['relu', 'relu', 'linear'] and extras {'vocab'}\n",
            "2020-11-26 10:00:21,294 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['relu', 'relu', 'linear']\n",
            "2020-11-26 10:00:21,295 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params relu and extras {'vocab'}\n",
            "2020-11-26 10:00:21,295 - INFO - allennlp.common.params - type = relu\n",
            "2020-11-26 10:00:21,295 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params relu and extras {'vocab'}\n",
            "2020-11-26 10:00:21,296 - INFO - allennlp.common.params - type = relu\n",
            "2020-11-26 10:00:21,296 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-11-26 10:00:21,296 - INFO - allennlp.common.params - type = linear\n",
            "2020-11-26 10:00:21,297 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0, 0, 0]\n",
            "2020-11-26 10:00:21,297 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0, 0, 0]\n",
            "2020-11-26 10:00:21,300 - INFO - allennlp.common.params - model.word_dropout = 0.04\n",
            "2020-11-26 10:00:21,300 - INFO - allennlp.common.params - model.embedding_dropout = 0.5\n",
            "2020-11-26 10:00:21,300 - INFO - allennlp.common.params - model.encoding_dropout = 0.5\n",
            "2020-11-26 10:00:21,300 - INFO - allennlp.common.params - model.verbose_metrics = False\n",
            "2020-11-26 10:00:21,301 - INFO - allennlp.common.params - model.ignore_label = no_relation\n",
            "2020-11-26 10:00:21,301 - INFO - allennlp.common.params - model.f1_average = micro\n",
            "2020-11-26 10:00:21,301 - INFO - allennlp.common.params - model.use_adjacency = True\n",
            "2020-11-26 10:00:21,301 - INFO - allennlp.common.params - model.use_entity_offsets = False\n",
            "2020-11-26 10:00:21,302 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-11-26 10:00:21,302 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-11-26 10:00:21,303 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-26 10:00:21,303 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-26 10:00:21,303 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.1.bias\n",
            "2020-11-26 10:00:21,303 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.1.weight\n",
            "2020-11-26 10:00:21,303 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.2.bias\n",
            "2020-11-26 10:00:21,304 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.2.weight\n",
            "2020-11-26 10:00:21,304 - INFO - allennlp.nn.initializers -    text_encoder.gcn_layer_0.bias\n",
            "2020-11-26 10:00:21,304 - INFO - allennlp.nn.initializers -    text_encoder.gcn_layer_0.weight\n",
            "2020-11-26 10:00:21,304 - INFO - allennlp.nn.initializers -    text_encoder.gcn_layer_1.bias\n",
            "2020-11-26 10:00:21,305 - INFO - allennlp.nn.initializers -    text_encoder.gcn_layer_1.weight\n",
            "2020-11-26 10:00:21,305 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_ner_tokens.weight\n",
            "2020-11-26 10:00:21,305 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_pos_tokens.weight\n",
            "2020-11-26 10:00:21,305 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-26 10:00:21,435 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']], 'type': 'bucket'} and extras set()\n",
            "2020-11-26 10:00:21,436 - INFO - allennlp.common.params - iterator.type = bucket\n",
            "2020-11-26 10:00:21,436 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']]} and extras set()\n",
            "2020-11-26 10:00:21,437 - INFO - allennlp.common.params - iterator.sorting_keys = [['text', 'num_tokens']]\n",
            "2020-11-26 10:00:21,437 - INFO - allennlp.common.params - iterator.padding_noise = 0.1\n",
            "2020-11-26 10:00:21,437 - INFO - allennlp.common.params - iterator.biggest_batch_first = False\n",
            "2020-11-26 10:00:21,438 - INFO - allennlp.common.params - iterator.batch_size = 50\n",
            "2020-11-26 10:00:21,438 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2020-11-26 10:00:21,438 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2020-11-26 10:00:21,438 - INFO - allennlp.common.params - iterator.cache_instances = False\n",
            "2020-11-26 10:00:21,439 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2020-11-26 10:00:21,439 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2020-11-26 10:00:21,439 - INFO - allennlp.common.params - iterator.skip_smaller_batches = False\n",
            "2020-11-26 10:00:21,439 - INFO - allennlp.common.params - validation_iterator = None\n",
            "2020-11-26 10:00:21,439 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2020-11-26 10:00:21,440 - INFO - allennlp.training.trainer_pieces - Following parameters are Frozen  (without gradient):\n",
            "2020-11-26 10:00:21,440 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-26 10:00:21,440 - INFO - allennlp.training.trainer_pieces - Following parameters are Tunable (with gradient):\n",
            "2020-11-26 10:00:21,441 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_ner_tokens.weight\n",
            "2020-11-26 10:00:21,441 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_pos_tokens.weight\n",
            "2020-11-26 10:00:21,441 - INFO - allennlp.training.trainer_pieces - text_encoder.gcn_layer_0.weight\n",
            "2020-11-26 10:00:21,441 - INFO - allennlp.training.trainer_pieces - text_encoder.gcn_layer_0.bias\n",
            "2020-11-26 10:00:21,441 - INFO - allennlp.training.trainer_pieces - text_encoder.gcn_layer_1.weight\n",
            "2020-11-26 10:00:21,442 - INFO - allennlp.training.trainer_pieces - text_encoder.gcn_layer_1.bias\n",
            "2020-11-26 10:00:21,442 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-26 10:00:21,442 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-26 10:00:21,442 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.1.weight\n",
            "2020-11-26 10:00:21,442 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.1.bias\n",
            "2020-11-26 10:00:21,443 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.2.weight\n",
            "2020-11-26 10:00:21,443 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.2.bias\n",
            "2020-11-26 10:00:21,443 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2020-11-26 10:00:21,443 - INFO - allennlp.common.params - trainer.validation_metric = +f1-measure-overall\n",
            "2020-11-26 10:00:21,443 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2020-11-26 10:00:21,444 - INFO - allennlp.common.params - trainer.num_epochs = 100\n",
            "2020-11-26 10:00:21,444 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2020-11-26 10:00:21,444 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2020-11-26 10:00:21,444 - INFO - allennlp.common.params - trainer.grad_clipping = 5\n",
            "2020-11-26 10:00:21,445 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2020-11-26 10:00:29,730 - INFO - allennlp.common.params - trainer.optimizer.type = sgd\n",
            "2020-11-26 10:00:29,730 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2020-11-26 10:00:29,731 - INFO - allennlp.training.optimizers - Number of trainable parameters: 2743522\n",
            "2020-11-26 10:00:29,732 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2020-11-26 10:00:29,732 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-26 10:00:29,732 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-26 10:00:29,733 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.3\n",
            "2020-11-26 10:00:29,733 - INFO - allennlp.common.registrable - instantiating registered subclass sgd of <class 'allennlp.training.optimizers.Optimizer'>\n",
            "2020-11-26 10:00:29,733 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
            "2020-11-26 10:00:29,733 - INFO - allennlp.common.registrable - instantiating registered subclass reduce_on_plateau of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
            "2020-11-26 10:00:29,734 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-26 10:00:29,734 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-26 10:00:29,734 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.factor = 0.9\n",
            "2020-11-26 10:00:29,734 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.mode = max\n",
            "2020-11-26 10:00:29,735 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.patience = 1\n",
            "2020-11-26 10:00:29,735 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1\n",
            "2020-11-26 10:00:29,735 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2020-11-26 10:00:29,735 - INFO - allennlp.common.params - trainer.model_save_interval = None\n",
            "2020-11-26 10:00:29,736 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2020-11-26 10:00:29,736 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2020-11-26 10:00:29,736 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2020-11-26 10:00:29,736 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False\n",
            "2020-11-26 10:00:29,736 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2020-11-26 10:00:29,757 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2020-11-26 10:00:29,758 - INFO - allennlp.training.trainer - Epoch 0/99\n",
            "2020-11-26 10:00:29,758 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3734.44\n",
            "2020-11-26 10:00:29,847 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 463\n",
            "2020-11-26 10:00:29,848 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8191, precision-overall: 0.6184, recall-overall: 0.1128, f1-measure-overall: 0.1908, loss: 0.8879 ||: 100%|##########| 1363/1363 [01:27<00:00, 15.51it/s]\n",
            "2020-11-26 10:01:57,707 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7717, precision-overall: 0.5196, recall-overall: 0.2908, f1-measure-overall: 0.3729, loss: 0.9375 ||: 100%|##########| 453/453 [00:24<00:00, 18.39it/s]\n",
            "2020-11-26 10:02:22,347 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:02:22,349 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.113  |     0.291\n",
            "2020-11-26 10:02:22,349 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.819  |     0.772\n",
            "2020-11-26 10:02:22,350 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3734.440  |       N/A\n",
            "2020-11-26 10:02:22,351 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.618  |     0.520\n",
            "2020-11-26 10:02:22,357 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   463.000  |       N/A\n",
            "2020-11-26 10:02:22,358 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.191  |     0.373\n",
            "2020-11-26 10:02:22,359 - INFO - allennlp.training.tensorboard_writer - loss               |     0.888  |     0.938\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:02:22,718 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:02:23,141 - INFO - allennlp.training.trainer - Epoch duration: 0:01:53.382682\n",
            "2020-11-26 10:02:23,142 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:07:05\n",
            "2020-11-26 10:02:23,142 - INFO - allennlp.training.trainer - Epoch 1/99\n",
            "2020-11-26 10:02:23,142 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4042.088\n",
            "2020-11-26 10:02:23,240 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:02:23,241 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8330, precision-overall: 0.6338, recall-overall: 0.2570, f1-measure-overall: 0.3657, loss: 0.6282 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.30it/s]\n",
            "2020-11-26 10:03:42,036 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7948, precision-overall: 0.6876, recall-overall: 0.2454, f1-measure-overall: 0.3617, loss: 0.6946 ||: 100%|##########| 453/453 [00:22<00:00, 20.42it/s]\n",
            "2020-11-26 10:04:04,220 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:04:04,221 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.257  |     0.245\n",
            "2020-11-26 10:04:04,221 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.833  |     0.795\n",
            "2020-11-26 10:04:04,222 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4042.088  |       N/A\n",
            "2020-11-26 10:04:04,224 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.634  |     0.688\n",
            "2020-11-26 10:04:04,225 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:04:04,226 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.366  |     0.362\n",
            "2020-11-26 10:04:04,227 - INFO - allennlp.training.tensorboard_writer - loss               |     0.628  |     0.695\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:04:04,616 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.473733\n",
            "2020-11-26 10:04:04,616 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:55:28\n",
            "2020-11-26 10:04:04,617 - INFO - allennlp.training.trainer - Epoch 2/99\n",
            "2020-11-26 10:04:04,617 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.544\n",
            "2020-11-26 10:04:04,719 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:04:04,720 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8429, precision-overall: 0.6494, recall-overall: 0.3239, f1-measure-overall: 0.4322, loss: 0.5420 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.44it/s]\n",
            "2020-11-26 10:05:22,869 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7993, precision-overall: 0.5806, recall-overall: 0.4292, f1-measure-overall: 0.4935, loss: 0.6453 ||: 100%|##########| 453/453 [00:22<00:00, 20.47it/s]\n",
            "2020-11-26 10:05:44,997 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:05:44,998 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.324  |     0.429\n",
            "2020-11-26 10:05:44,999 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.843  |     0.799\n",
            "2020-11-26 10:05:45,000 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.544  |       N/A\n",
            "2020-11-26 10:05:45,001 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.649  |     0.581\n",
            "2020-11-26 10:05:45,002 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:05:45,003 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.432  |     0.494\n",
            "2020-11-26 10:05:45,004 - INFO - allennlp.training.tensorboard_writer - loss               |     0.542  |     0.645\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:05:45,293 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:05:45,735 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.118469\n",
            "2020-11-26 10:05:45,736 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:50:16\n",
            "2020-11-26 10:05:45,737 - INFO - allennlp.training.trainer - Epoch 3/99\n",
            "2020-11-26 10:05:45,737 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.668\n",
            "2020-11-26 10:05:45,840 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:05:45,841 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8484, precision-overall: 0.6566, recall-overall: 0.3693, f1-measure-overall: 0.4727, loss: 0.4977 ||: 100%|##########| 1363/1363 [01:20<00:00, 16.93it/s]\n",
            "2020-11-26 10:07:06,344 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8202, precision-overall: 0.6420, recall-overall: 0.4492, f1-measure-overall: 0.5286, loss: 0.5656 ||: 100%|##########| 453/453 [00:21<00:00, 20.81it/s]\n",
            "2020-11-26 10:07:28,118 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:07:28,119 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.369  |     0.449\n",
            "2020-11-26 10:07:28,120 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.848  |     0.820\n",
            "2020-11-26 10:07:28,121 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.668  |       N/A\n",
            "2020-11-26 10:07:28,122 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.657  |     0.642\n",
            "2020-11-26 10:07:28,122 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:07:28,124 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.473  |     0.529\n",
            "2020-11-26 10:07:28,125 - INFO - allennlp.training.tensorboard_writer - loss               |     0.498  |     0.566\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:07:28,401 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:07:28,817 - INFO - allennlp.training.trainer - Epoch duration: 0:01:43.080020\n",
            "2020-11-26 10:07:28,817 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:47:37\n",
            "2020-11-26 10:07:28,818 - INFO - allennlp.training.trainer - Epoch 4/99\n",
            "2020-11-26 10:07:28,818 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.668\n",
            "2020-11-26 10:07:28,922 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:07:28,923 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8530, precision-overall: 0.6709, recall-overall: 0.3944, f1-measure-overall: 0.4968, loss: 0.4746 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.49it/s]\n",
            "2020-11-26 10:08:46,863 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8227, precision-overall: 0.7126, recall-overall: 0.3931, f1-measure-overall: 0.5067, loss: 0.5506 ||: 100%|##########| 453/453 [00:21<00:00, 20.85it/s]\n",
            "2020-11-26 10:09:08,592 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:09:08,593 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.394  |     0.393\n",
            "2020-11-26 10:09:08,594 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.853  |     0.823\n",
            "2020-11-26 10:09:08,595 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.668  |       N/A\n",
            "2020-11-26 10:09:08,596 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.671  |     0.713\n",
            "2020-11-26 10:09:08,597 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:09:08,598 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.497  |     0.507\n",
            "2020-11-26 10:09:08,599 - INFO - allennlp.training.tensorboard_writer - loss               |     0.475  |     0.551\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:09:08,902 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.084334\n",
            "2020-11-26 10:09:08,903 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:44:23\n",
            "2020-11-26 10:09:08,903 - INFO - allennlp.training.trainer - Epoch 5/99\n",
            "2020-11-26 10:09:08,903 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.668\n",
            "2020-11-26 10:09:08,993 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:09:08,994 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8557, precision-overall: 0.6774, recall-overall: 0.4052, f1-measure-overall: 0.5071, loss: 0.4613 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.58it/s]\n",
            "2020-11-26 10:10:26,538 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8179, precision-overall: 0.5850, recall-overall: 0.5164, f1-measure-overall: 0.5486, loss: 0.5464 ||: 100%|##########| 453/453 [00:21<00:00, 20.86it/s]\n",
            "2020-11-26 10:10:48,252 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:10:48,253 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.405  |     0.516\n",
            "2020-11-26 10:10:48,254 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.856  |     0.818\n",
            "2020-11-26 10:10:48,255 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.668  |       N/A\n",
            "2020-11-26 10:10:48,256 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.677  |     0.585\n",
            "2020-11-26 10:10:48,257 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:10:48,258 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.507  |     0.549\n",
            "2020-11-26 10:10:48,259 - INFO - allennlp.training.tensorboard_writer - loss               |     0.461  |     0.546\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:10:48,536 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:10:48,939 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.035659\n",
            "2020-11-26 10:10:48,939 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:41:40\n",
            "2020-11-26 10:10:48,939 - INFO - allennlp.training.trainer - Epoch 6/99\n",
            "2020-11-26 10:10:48,940 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.668\n",
            "2020-11-26 10:10:49,037 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:10:49,038 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8567, precision-overall: 0.6815, recall-overall: 0.4108, f1-measure-overall: 0.5126, loss: 0.4521 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.48it/s]\n",
            "2020-11-26 10:12:07,033 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8203, precision-overall: 0.6307, recall-overall: 0.4864, f1-measure-overall: 0.5492, loss: 0.5226 ||: 100%|##########| 453/453 [00:22<00:00, 20.57it/s]\n",
            "2020-11-26 10:12:29,063 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:12:29,064 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.411  |     0.486\n",
            "2020-11-26 10:12:29,065 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.857  |     0.820\n",
            "2020-11-26 10:12:29,065 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.668  |       N/A\n",
            "2020-11-26 10:12:29,066 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.681  |     0.631\n",
            "2020-11-26 10:12:29,067 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:12:29,068 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.513  |     0.549\n",
            "2020-11-26 10:12:29,069 - INFO - allennlp.training.tensorboard_writer - loss               |     0.452  |     0.523\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:12:29,324 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:12:29,718 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.778182\n",
            "2020-11-26 10:12:29,719 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:39:25\n",
            "2020-11-26 10:12:29,719 - INFO - allennlp.training.trainer - Epoch 7/99\n",
            "2020-11-26 10:12:29,719 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.684\n",
            "2020-11-26 10:12:29,816 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:12:29,817 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8596, precision-overall: 0.6891, recall-overall: 0.4268, f1-measure-overall: 0.5271, loss: 0.4416 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.45it/s]\n",
            "2020-11-26 10:13:47,932 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8295, precision-overall: 0.6489, recall-overall: 0.5140, f1-measure-overall: 0.5736, loss: 0.5024 ||: 100%|##########| 453/453 [00:22<00:00, 20.19it/s]\n",
            "2020-11-26 10:14:10,368 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:14:10,369 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.427  |     0.514\n",
            "2020-11-26 10:14:10,370 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.860  |     0.829\n",
            "2020-11-26 10:14:10,371 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.684  |       N/A\n",
            "2020-11-26 10:14:10,372 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.689  |     0.649\n",
            "2020-11-26 10:14:10,373 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:14:10,374 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.527  |     0.574\n",
            "2020-11-26 10:14:10,375 - INFO - allennlp.training.tensorboard_writer - loss               |     0.442  |     0.502\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:14:10,650 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:14:11,050 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.331001\n",
            "2020-11-26 10:14:11,051 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:24\n",
            "2020-11-26 10:14:11,051 - INFO - allennlp.training.trainer - Epoch 8/99\n",
            "2020-11-26 10:14:11,051 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.684\n",
            "2020-11-26 10:14:11,153 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:14:11,154 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8604, precision-overall: 0.6893, recall-overall: 0.4338, f1-measure-overall: 0.5325, loss: 0.4337 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.45it/s]\n",
            "2020-11-26 10:15:29,273 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8340, precision-overall: 0.6802, recall-overall: 0.4812, f1-measure-overall: 0.5637, loss: 0.4926 ||: 100%|##########| 453/453 [00:21<00:00, 20.66it/s]\n",
            "2020-11-26 10:15:51,199 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:15:51,200 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.434  |     0.481\n",
            "2020-11-26 10:15:51,201 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.860  |     0.834\n",
            "2020-11-26 10:15:51,202 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.684  |       N/A\n",
            "2020-11-26 10:15:51,203 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.689  |     0.680\n",
            "2020-11-26 10:15:51,204 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:15:51,205 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.532  |     0.564\n",
            "2020-11-26 10:15:51,206 - INFO - allennlp.training.tensorboard_writer - loss               |     0.434  |     0.493\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:15:51,488 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.436916\n",
            "2020-11-26 10:15:51,488 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:35:19\n",
            "2020-11-26 10:15:51,488 - INFO - allennlp.training.trainer - Epoch 9/99\n",
            "2020-11-26 10:15:51,489 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.684\n",
            "2020-11-26 10:15:51,581 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:15:51,582 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8614, precision-overall: 0.6906, recall-overall: 0.4424, f1-measure-overall: 0.5393, loss: 0.4300 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.52it/s]\n",
            "2020-11-26 10:17:09,376 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8289, precision-overall: 0.6776, recall-overall: 0.4656, f1-measure-overall: 0.5520, loss: 0.5041 ||: 100%|##########| 453/453 [00:21<00:00, 20.83it/s]\n",
            "2020-11-26 10:17:31,127 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:17:31,128 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.442  |     0.466\n",
            "2020-11-26 10:17:31,129 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.861  |     0.829\n",
            "2020-11-26 10:17:31,130 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.684  |       N/A\n",
            "2020-11-26 10:17:31,131 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.691  |     0.678\n",
            "2020-11-26 10:17:31,132 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:17:31,133 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.539  |     0.552\n",
            "2020-11-26 10:17:31,134 - INFO - allennlp.training.tensorboard_writer - loss               |     0.430  |     0.504\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:17:31,405 - INFO - allennlp.training.trainer - Epoch duration: 0:01:39.916514\n",
            "2020-11-26 10:17:31,406 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:33:14\n",
            "2020-11-26 10:17:31,406 - INFO - allennlp.training.trainer - Epoch 10/99\n",
            "2020-11-26 10:17:31,406 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.684\n",
            "2020-11-26 10:17:31,498 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:17:31,499 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8632, precision-overall: 0.6935, recall-overall: 0.4510, f1-measure-overall: 0.5465, loss: 0.4193 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.54it/s]\n",
            "2020-11-26 10:18:49,191 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8278, precision-overall: 0.6286, recall-overall: 0.5414, f1-measure-overall: 0.5817, loss: 0.4983 ||: 100%|##########| 453/453 [00:21<00:00, 20.62it/s]\n",
            "2020-11-26 10:19:11,163 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:19:11,164 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.451  |     0.541\n",
            "2020-11-26 10:19:11,165 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.863  |     0.828\n",
            "2020-11-26 10:19:11,166 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.684  |       N/A\n",
            "2020-11-26 10:19:11,167 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.694  |     0.629\n",
            "2020-11-26 10:19:11,168 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:19:11,169 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.547  |     0.582\n",
            "2020-11-26 10:19:11,170 - INFO - allennlp.training.tensorboard_writer - loss               |     0.419  |     0.498\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:19:11,468 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:19:11,861 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.455382\n",
            "2020-11-26 10:19:11,862 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:31:18\n",
            "2020-11-26 10:19:11,863 - INFO - allennlp.training.trainer - Epoch 11/99\n",
            "2020-11-26 10:19:11,864 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:19:11,972 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:19:11,973 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8650, precision-overall: 0.7039, recall-overall: 0.4567, f1-measure-overall: 0.5539, loss: 0.4154 ||: 100%|##########| 1363/1363 [01:20<00:00, 17.01it/s]\n",
            "2020-11-26 10:20:32,113 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8336, precision-overall: 0.6277, recall-overall: 0.5657, f1-measure-overall: 0.5951, loss: 0.4868 ||: 100%|##########| 453/453 [00:22<00:00, 20.56it/s]\n",
            "2020-11-26 10:20:54,150 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:20:54,151 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.457  |     0.566\n",
            "2020-11-26 10:20:54,152 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.865  |     0.834\n",
            "2020-11-26 10:20:54,153 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:20:54,154 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.704  |     0.628\n",
            "2020-11-26 10:20:54,155 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:20:54,155 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.554  |     0.595\n",
            "2020-11-26 10:20:54,156 - INFO - allennlp.training.tensorboard_writer - loss               |     0.415  |     0.487\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:20:54,420 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:20:54,808 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.944180\n",
            "2020-11-26 10:20:54,808 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:29:43\n",
            "2020-11-26 10:20:54,808 - INFO - allennlp.training.trainer - Epoch 12/99\n",
            "2020-11-26 10:20:54,808 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:20:54,909 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:20:54,910 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8663, precision-overall: 0.7046, recall-overall: 0.4659, f1-measure-overall: 0.5609, loss: 0.4106 ||: 100%|##########| 1363/1363 [01:20<00:00, 17.02it/s]\n",
            "2020-11-26 10:22:14,991 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8255, precision-overall: 0.6094, recall-overall: 0.5911, f1-measure-overall: 0.6001, loss: 0.4845 ||: 100%|##########| 453/453 [00:22<00:00, 20.30it/s]\n",
            "2020-11-26 10:22:37,310 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:22:37,310 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.466  |     0.591\n",
            "2020-11-26 10:22:37,312 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.866  |     0.826\n",
            "2020-11-26 10:22:37,313 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:22:37,314 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.705  |     0.609\n",
            "2020-11-26 10:22:37,315 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:22:37,315 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.561  |     0.600\n",
            "2020-11-26 10:22:37,317 - INFO - allennlp.training.tensorboard_writer - loss               |     0.411  |     0.484\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:22:37,582 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:22:37,985 - INFO - allennlp.training.trainer - Epoch duration: 0:01:43.176916\n",
            "2020-11-26 10:22:37,986 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:28:08\n",
            "2020-11-26 10:22:37,986 - INFO - allennlp.training.trainer - Epoch 13/99\n",
            "2020-11-26 10:22:37,986 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:22:38,079 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:22:38,080 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8671, precision-overall: 0.7058, recall-overall: 0.4688, f1-measure-overall: 0.5634, loss: 0.4080 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.14it/s]\n",
            "2020-11-26 10:23:57,606 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8366, precision-overall: 0.7325, recall-overall: 0.4584, f1-measure-overall: 0.5639, loss: 0.4682 ||: 100%|##########| 453/453 [00:22<00:00, 20.06it/s]\n",
            "2020-11-26 10:24:20,192 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:24:20,193 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.469  |     0.458\n",
            "2020-11-26 10:24:20,195 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.867  |     0.837\n",
            "2020-11-26 10:24:20,196 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:24:20,196 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.706  |     0.733\n",
            "2020-11-26 10:24:20,198 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:24:20,198 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.563  |     0.564\n",
            "2020-11-26 10:24:20,200 - INFO - allennlp.training.tensorboard_writer - loss               |     0.408  |     0.468\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:24:20,479 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.493517\n",
            "2020-11-26 10:24:20,480 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:26:28\n",
            "2020-11-26 10:24:20,480 - INFO - allennlp.training.trainer - Epoch 14/99\n",
            "2020-11-26 10:24:20,480 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:24:20,570 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:24:20,572 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8669, precision-overall: 0.7044, recall-overall: 0.4691, f1-measure-overall: 0.5632, loss: 0.4065 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.28it/s]\n",
            "2020-11-26 10:25:39,451 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8361, precision-overall: 0.6379, recall-overall: 0.5879, f1-measure-overall: 0.6119, loss: 0.4678 ||: 100%|##########| 453/453 [00:22<00:00, 20.14it/s]\n",
            "2020-11-26 10:26:01,948 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:26:01,948 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.469  |     0.588\n",
            "2020-11-26 10:26:01,950 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.867  |     0.836\n",
            "2020-11-26 10:26:01,951 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:26:01,951 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.704  |     0.638\n",
            "2020-11-26 10:26:01,952 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:26:01,953 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.563  |     0.612\n",
            "2020-11-26 10:26:01,954 - INFO - allennlp.training.tensorboard_writer - loss               |     0.406  |     0.468\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:26:02,227 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:26:02,622 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.141939\n",
            "2020-11-26 10:26:02,622 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:24:46\n",
            "2020-11-26 10:26:02,623 - INFO - allennlp.training.trainer - Epoch 15/99\n",
            "2020-11-26 10:26:02,623 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:26:02,716 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:26:02,717 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8693, precision-overall: 0.7121, recall-overall: 0.4789, f1-measure-overall: 0.5727, loss: 0.4020 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.23it/s]\n",
            "2020-11-26 10:27:21,848 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8312, precision-overall: 0.6509, recall-overall: 0.5313, f1-measure-overall: 0.5850, loss: 0.4689 ||: 100%|##########| 453/453 [00:22<00:00, 20.51it/s]\n",
            "2020-11-26 10:27:43,933 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:27:43,934 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.479  |     0.531\n",
            "2020-11-26 10:27:43,935 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.869  |     0.831\n",
            "2020-11-26 10:27:43,936 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:27:43,937 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.712  |     0.651\n",
            "2020-11-26 10:27:43,938 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:27:43,939 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.573  |     0.585\n",
            "2020-11-26 10:27:43,940 - INFO - allennlp.training.tensorboard_writer - loss               |     0.402  |     0.469\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:27:44,216 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.593264\n",
            "2020-11-26 10:27:44,216 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:23:00\n",
            "2020-11-26 10:27:44,217 - INFO - allennlp.training.trainer - Epoch 16/99\n",
            "2020-11-26 10:27:44,217 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:27:44,307 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:27:44,308 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8668, precision-overall: 0.7019, recall-overall: 0.4754, f1-measure-overall: 0.5669, loss: 0.4017 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.39it/s]\n",
            "2020-11-26 10:29:02,677 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8419, precision-overall: 0.6736, recall-overall: 0.5688, f1-measure-overall: 0.6168, loss: 0.4501 ||: 100%|##########| 453/453 [00:22<00:00, 20.22it/s]\n",
            "2020-11-26 10:29:25,083 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:29:25,084 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.475  |     0.569\n",
            "2020-11-26 10:29:25,084 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.867  |     0.842\n",
            "2020-11-26 10:29:25,086 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:29:25,087 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.702  |     0.674\n",
            "2020-11-26 10:29:25,088 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:29:25,088 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.567  |     0.617\n",
            "2020-11-26 10:29:25,089 - INFO - allennlp.training.tensorboard_writer - loss               |     0.402  |     0.450\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:29:25,415 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:29:25,950 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.733538\n",
            "2020-11-26 10:29:25,951 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:21:16\n",
            "2020-11-26 10:29:25,951 - INFO - allennlp.training.trainer - Epoch 17/99\n",
            "2020-11-26 10:29:25,951 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:29:26,056 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:29:26,057 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8685, precision-overall: 0.7077, recall-overall: 0.4791, f1-measure-overall: 0.5714, loss: 0.3966 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.42it/s]\n",
            "2020-11-26 10:30:44,318 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8417, precision-overall: 0.6558, recall-overall: 0.5863, f1-measure-overall: 0.6191, loss: 0.4613 ||: 100%|##########| 453/453 [00:22<00:00, 20.56it/s]\n",
            "2020-11-26 10:31:06,352 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:31:06,353 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.479  |     0.586\n",
            "2020-11-26 10:31:06,354 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.869  |     0.842\n",
            "2020-11-26 10:31:06,355 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:31:06,356 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.708  |     0.656\n",
            "2020-11-26 10:31:06,357 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:31:06,357 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.571  |     0.619\n",
            "2020-11-26 10:31:06,358 - INFO - allennlp.training.tensorboard_writer - loss               |     0.397  |     0.461\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:31:06,608 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:31:07,013 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.061890\n",
            "2020-11-26 10:31:07,014 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:19:29\n",
            "2020-11-26 10:31:07,014 - INFO - allennlp.training.trainer - Epoch 18/99\n",
            "2020-11-26 10:31:07,014 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:31:07,125 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:31:07,126 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8685, precision-overall: 0.7049, recall-overall: 0.4809, f1-measure-overall: 0.5718, loss: 0.3959 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.42it/s]\n",
            "2020-11-26 10:32:25,377 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8443, precision-overall: 0.6842, recall-overall: 0.5780, f1-measure-overall: 0.6266, loss: 0.4480 ||: 100%|##########| 453/453 [00:22<00:00, 20.54it/s]\n",
            "2020-11-26 10:32:47,435 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:32:47,436 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.481  |     0.578\n",
            "2020-11-26 10:32:47,437 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.869  |     0.844\n",
            "2020-11-26 10:32:47,438 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:32:47,439 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.705  |     0.684\n",
            "2020-11-26 10:32:47,440 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:32:47,441 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.572  |     0.627\n",
            "2020-11-26 10:32:47,442 - INFO - allennlp.training.tensorboard_writer - loss               |     0.396  |     0.448\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:32:47,726 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:32:48,122 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.108264\n",
            "2020-11-26 10:32:48,122 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:17:43\n",
            "2020-11-26 10:32:48,123 - INFO - allennlp.training.trainer - Epoch 19/99\n",
            "2020-11-26 10:32:48,123 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:32:48,574 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:32:48,575 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8701, precision-overall: 0.7124, recall-overall: 0.4867, f1-measure-overall: 0.5783, loss: 0.3950 ||: 100%|##########| 1363/1363 [01:20<00:00, 17.03it/s]\n",
            "2020-11-26 10:34:08,602 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8419, precision-overall: 0.6695, recall-overall: 0.5809, f1-measure-overall: 0.6221, loss: 0.4416 ||: 100%|##########| 453/453 [00:21<00:00, 20.60it/s]\n",
            "2020-11-26 10:34:30,600 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:34:30,601 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.487  |     0.581\n",
            "2020-11-26 10:34:30,602 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.870  |     0.842\n",
            "2020-11-26 10:34:30,603 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:34:30,604 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.712  |     0.669\n",
            "2020-11-26 10:34:30,605 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:34:30,608 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.578  |     0.622\n",
            "2020-11-26 10:34:30,610 - INFO - allennlp.training.tensorboard_writer - loss               |     0.395  |     0.442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:34:30,948 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.825558\n",
            "2020-11-26 10:34:30,949 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:16:04\n",
            "2020-11-26 10:34:30,949 - INFO - allennlp.training.trainer - Epoch 20/99\n",
            "2020-11-26 10:34:30,949 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:34:31,043 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:34:31,044 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8700, precision-overall: 0.7127, recall-overall: 0.4882, f1-measure-overall: 0.5795, loss: 0.3920 ||: 100%|##########| 1363/1363 [01:18<00:00, 17.42it/s]\n",
            "2020-11-26 10:35:49,289 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8421, precision-overall: 0.6794, recall-overall: 0.5473, f1-measure-overall: 0.6062, loss: 0.4520 ||: 100%|##########| 453/453 [00:21<00:00, 20.78it/s]\n",
            "2020-11-26 10:36:11,097 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:36:11,097 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.488  |     0.547\n",
            "2020-11-26 10:36:11,098 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.870  |     0.842\n",
            "2020-11-26 10:36:11,100 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:36:11,100 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.713  |     0.679\n",
            "2020-11-26 10:36:11,101 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:36:11,102 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.579  |     0.606\n",
            "2020-11-26 10:36:11,102 - INFO - allennlp.training.tensorboard_writer - loss               |     0.392  |     0.452\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:36:11,380 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.431277\n",
            "2020-11-26 10:36:11,381 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:14:16\n",
            "2020-11-26 10:36:11,381 - INFO - allennlp.training.trainer - Epoch 21/99\n",
            "2020-11-26 10:36:11,381 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:36:11,473 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:36:11,474 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8706, precision-overall: 0.7117, recall-overall: 0.4929, f1-measure-overall: 0.5825, loss: 0.3887 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.51it/s]\n",
            "2020-11-26 10:37:29,315 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8415, precision-overall: 0.6627, recall-overall: 0.6043, f1-measure-overall: 0.6322, loss: 0.4477 ||: 100%|##########| 453/453 [00:22<00:00, 20.54it/s]\n",
            "2020-11-26 10:37:51,377 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:37:51,378 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.493  |     0.604\n",
            "2020-11-26 10:37:51,379 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.871  |     0.842\n",
            "2020-11-26 10:37:51,380 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:37:51,381 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.712  |     0.663\n",
            "2020-11-26 10:37:51,382 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:37:51,383 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.582  |     0.632\n",
            "2020-11-26 10:37:51,384 - INFO - allennlp.training.tensorboard_writer - loss               |     0.389  |     0.448\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:37:51,654 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:37:52,053 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.671982\n",
            "2020-11-26 10:37:52,053 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:12:29\n",
            "2020-11-26 10:37:52,054 - INFO - allennlp.training.trainer - Epoch 22/99\n",
            "2020-11-26 10:37:52,054 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:37:52,152 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:37:52,153 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8716, precision-overall: 0.7149, recall-overall: 0.4932, f1-measure-overall: 0.5837, loss: 0.3861 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.52it/s]\n",
            "2020-11-26 10:39:09,966 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8426, precision-overall: 0.6567, recall-overall: 0.6072, f1-measure-overall: 0.6310, loss: 0.4417 ||: 100%|##########| 453/453 [00:22<00:00, 20.59it/s]\n",
            "2020-11-26 10:39:31,976 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:39:31,977 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.493  |     0.607\n",
            "2020-11-26 10:39:31,978 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.872  |     0.843\n",
            "2020-11-26 10:39:31,979 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:39:31,980 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.715  |     0.657\n",
            "2020-11-26 10:39:31,981 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:39:31,982 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.584  |     0.631\n",
            "2020-11-26 10:39:31,983 - INFO - allennlp.training.tensorboard_writer - loss               |     0.386  |     0.442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:39:32,267 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.213020\n",
            "2020-11-26 10:39:32,267 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:10:42\n",
            "2020-11-26 10:39:32,268 - INFO - allennlp.training.trainer - Epoch 23/99\n",
            "2020-11-26 10:39:32,268 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:39:32,359 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:39:32,361 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8717, precision-overall: 0.7144, recall-overall: 0.4942, f1-measure-overall: 0.5842, loss: 0.3839 ||: 100%|##########| 1363/1363 [01:17<00:00, 17.58it/s]\n",
            "2020-11-26 10:40:49,907 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8480, precision-overall: 0.6786, recall-overall: 0.6014, f1-measure-overall: 0.6377, loss: 0.4314 ||: 100%|##########| 453/453 [00:22<00:00, 20.25it/s]\n",
            "2020-11-26 10:41:12,284 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:41:12,285 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.494  |     0.601\n",
            "2020-11-26 10:41:12,286 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.872  |     0.848\n",
            "2020-11-26 10:41:12,287 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:41:12,288 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.714  |     0.679\n",
            "2020-11-26 10:41:12,289 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:41:12,290 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.584  |     0.638\n",
            "2020-11-26 10:41:12,291 - INFO - allennlp.training.tensorboard_writer - loss               |     0.384  |     0.431\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:41:12,554 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/best.th'.\n",
            "2020-11-26 10:41:12,967 - INFO - allennlp.training.trainer - Epoch duration: 0:01:40.699396\n",
            "2020-11-26 10:41:12,967 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:08:56\n",
            "2020-11-26 10:41:12,968 - INFO - allennlp.training.trainer - Epoch 24/99\n",
            "2020-11-26 10:41:12,968 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:41:13,069 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:41:13,076 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8714, precision-overall: 0.7133, recall-overall: 0.4935, f1-measure-overall: 0.5834, loss: 0.3852 ||: 100%|##########| 1363/1363 [01:20<00:00, 16.92it/s]\n",
            "2020-11-26 10:42:33,651 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8450, precision-overall: 0.6837, recall-overall: 0.5892, f1-measure-overall: 0.6329, loss: 0.4426 ||: 100%|##########| 453/453 [00:22<00:00, 20.07it/s]\n",
            "2020-11-26 10:42:56,231 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:42:56,232 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.494  |     0.589\n",
            "2020-11-26 10:42:56,233 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.871  |     0.845\n",
            "2020-11-26 10:42:56,234 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:42:56,235 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.713  |     0.684\n",
            "2020-11-26 10:42:56,236 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:42:56,237 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.583  |     0.633\n",
            "2020-11-26 10:42:56,238 - INFO - allennlp.training.tensorboard_writer - loss               |     0.385  |     0.443\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:42:56,520 - INFO - allennlp.training.trainer - Epoch duration: 0:01:43.552400\n",
            "2020-11-26 10:42:56,520 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:07:20\n",
            "2020-11-26 10:42:56,521 - INFO - allennlp.training.trainer - Epoch 25/99\n",
            "2020-11-26 10:42:56,521 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:42:56,613 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:42:56,614 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8730, precision-overall: 0.7165, recall-overall: 0.5042, f1-measure-overall: 0.5919, loss: 0.3800 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.18it/s]\n",
            "2020-11-26 10:44:15,940 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8379, precision-overall: 0.6665, recall-overall: 0.5684, f1-measure-overall: 0.6136, loss: 0.4553 ||: 100%|##########| 453/453 [00:22<00:00, 20.38it/s]\n",
            "2020-11-26 10:44:38,167 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:44:38,167 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.504  |     0.568\n",
            "2020-11-26 10:44:38,169 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.873  |     0.838\n",
            "2020-11-26 10:44:38,169 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:44:38,171 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.717  |     0.667\n",
            "2020-11-26 10:44:38,172 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:44:38,172 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.592  |     0.614\n",
            "2020-11-26 10:44:38,173 - INFO - allennlp.training.tensorboard_writer - loss               |     0.380  |     0.455\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:44:38,471 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.950514\n",
            "2020-11-26 10:44:38,472 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:05:38\n",
            "2020-11-26 10:44:38,472 - INFO - allennlp.training.trainer - Epoch 26/99\n",
            "2020-11-26 10:44:38,472 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:44:38,566 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:44:38,567 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8736, precision-overall: 0.7188, recall-overall: 0.5058, f1-measure-overall: 0.5938, loss: 0.3778 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.23it/s]\n",
            "2020-11-26 10:45:57,693 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8474, precision-overall: 0.7082, recall-overall: 0.5550, f1-measure-overall: 0.6223, loss: 0.4342 ||: 100%|##########| 453/453 [00:22<00:00, 20.36it/s]\n",
            "2020-11-26 10:46:19,942 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:46:19,942 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.506  |     0.555\n",
            "2020-11-26 10:46:19,943 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.874  |     0.847\n",
            "2020-11-26 10:46:19,944 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:46:19,946 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.719  |     0.708\n",
            "2020-11-26 10:46:19,947 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:46:19,947 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.594  |     0.622\n",
            "2020-11-26 10:46:19,948 - INFO - allennlp.training.tensorboard_writer - loss               |     0.378  |     0.434\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:46:20,249 - INFO - allennlp.training.trainer - Epoch duration: 0:01:41.776900\n",
            "2020-11-26 10:46:20,249 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:03:56\n",
            "2020-11-26 10:46:20,250 - INFO - allennlp.training.trainer - Epoch 27/99\n",
            "2020-11-26 10:46:20,250 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:46:20,341 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:46:20,342 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8740, precision-overall: 0.7201, recall-overall: 0.5061, f1-measure-overall: 0.5944, loss: 0.3759 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.22it/s]\n",
            "2020-11-26 10:47:39,499 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8442, precision-overall: 0.6687, recall-overall: 0.5953, f1-measure-overall: 0.6299, loss: 0.4343 ||: 100%|##########| 453/453 [00:24<00:00, 18.34it/s]\n",
            "2020-11-26 10:48:04,204 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:48:04,205 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.506  |     0.595\n",
            "2020-11-26 10:48:04,206 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.874  |     0.844\n",
            "2020-11-26 10:48:04,207 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:48:04,207 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.720  |     0.669\n",
            "2020-11-26 10:48:04,208 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:48:04,209 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.594  |     0.630\n",
            "2020-11-26 10:48:04,210 - INFO - allennlp.training.tensorboard_writer - loss               |     0.376  |     0.434\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:48:04,494 - INFO - allennlp.training.trainer - Epoch duration: 0:01:44.244064\n",
            "2020-11-26 10:48:04,494 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:02:20\n",
            "2020-11-26 10:48:04,494 - INFO - allennlp.training.trainer - Epoch 28/99\n",
            "2020-11-26 10:48:04,495 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:48:04,588 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:48:04,589 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8735, precision-overall: 0.7160, recall-overall: 0.5082, f1-measure-overall: 0.5945, loss: 0.3755 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.12it/s]\n",
            "2020-11-26 10:49:24,215 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8494, precision-overall: 0.7102, recall-overall: 0.5681, f1-measure-overall: 0.6312, loss: 0.4238 ||: 100%|##########| 453/453 [00:22<00:00, 19.93it/s]\n",
            "2020-11-26 10:49:46,944 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:49:46,945 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.508  |     0.568\n",
            "2020-11-26 10:49:46,946 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.873  |     0.849\n",
            "2020-11-26 10:49:46,947 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:49:46,948 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.716  |     0.710\n",
            "2020-11-26 10:49:46,949 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:49:46,950 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.594  |     0.631\n",
            "2020-11-26 10:49:46,951 - INFO - allennlp.training.tensorboard_writer - loss               |     0.376  |     0.424\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:49:47,252 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.757512\n",
            "2020-11-26 10:49:47,252 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:00:40\n",
            "2020-11-26 10:49:47,253 - INFO - allennlp.training.trainer - Epoch 29/99\n",
            "2020-11-26 10:49:47,253 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:49:47,344 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:49:47,345 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8751, precision-overall: 0.7232, recall-overall: 0.5115, f1-measure-overall: 0.5992, loss: 0.3736 ||: 100%|##########| 1363/1363 [01:20<00:00, 16.96it/s]\n",
            "2020-11-26 10:51:07,719 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8495, precision-overall: 0.7134, recall-overall: 0.5581, f1-measure-overall: 0.6263, loss: 0.4249 ||: 100%|##########| 453/453 [00:22<00:00, 20.03it/s]\n",
            "2020-11-26 10:51:30,343 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:51:30,344 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.512  |     0.558\n",
            "2020-11-26 10:51:30,346 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.875  |     0.850\n",
            "2020-11-26 10:51:30,347 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:51:30,348 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.723  |     0.713\n",
            "2020-11-26 10:51:30,349 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:51:30,350 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.599  |     0.626\n",
            "2020-11-26 10:51:30,351 - INFO - allennlp.training.tensorboard_writer - loss               |     0.374  |     0.425\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:51:30,643 - INFO - allennlp.training.trainer - Epoch duration: 0:01:43.390930\n",
            "2020-11-26 10:51:30,644 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:59:02\n",
            "2020-11-26 10:51:30,644 - INFO - allennlp.training.trainer - Epoch 30/99\n",
            "2020-11-26 10:51:30,644 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:51:30,737 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:51:30,738 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8765, precision-overall: 0.7252, recall-overall: 0.5227, f1-measure-overall: 0.6075, loss: 0.3695 ||: 100%|##########| 1363/1363 [01:19<00:00, 17.10it/s]\n",
            "2020-11-26 10:52:50,471 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8507, precision-overall: 0.7035, recall-overall: 0.5758, f1-measure-overall: 0.6333, loss: 0.4241 ||: 100%|##########| 453/453 [00:22<00:00, 19.91it/s]\n",
            "2020-11-26 10:53:13,229 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:53:13,230 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.523  |     0.576\n",
            "2020-11-26 10:53:13,231 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.877  |     0.851\n",
            "2020-11-26 10:53:13,232 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:53:13,233 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.725  |     0.704\n",
            "2020-11-26 10:53:13,233 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:53:13,234 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.608  |     0.633\n",
            "2020-11-26 10:53:13,236 - INFO - allennlp.training.tensorboard_writer - loss               |     0.370  |     0.424\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:53:13,532 - INFO - allennlp.training.trainer - Epoch duration: 0:01:42.888106\n",
            "2020-11-26 10:53:13,533 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:57:21\n",
            "2020-11-26 10:53:13,533 - INFO - allennlp.training.trainer - Epoch 31/99\n",
            "2020-11-26 10:53:13,533 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:53:13,627 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:53:13,628 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8754, precision-overall: 0.7197, recall-overall: 0.5191, f1-measure-overall: 0.6031, loss: 0.3683 ||: 100%|##########| 1363/1363 [01:20<00:00, 16.97it/s]\n",
            "2020-11-26 10:54:33,968 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8426, precision-overall: 0.6525, recall-overall: 0.6203, f1-measure-overall: 0.6360, loss: 0.4360 ||: 100%|##########| 453/453 [00:22<00:00, 20.16it/s]\n",
            "2020-11-26 10:54:56,444 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:54:56,444 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.519  |     0.620\n",
            "2020-11-26 10:54:56,445 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.875  |     0.843\n",
            "2020-11-26 10:54:56,446 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:54:56,447 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.720  |     0.652\n",
            "2020-11-26 10:54:56,448 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:54:56,449 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.603  |     0.636\n",
            "2020-11-26 10:54:56,450 - INFO - allennlp.training.tensorboard_writer - loss               |     0.368  |     0.436\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:54:56,755 - INFO - allennlp.training.trainer - Epoch duration: 0:01:43.221886\n",
            "2020-11-26 10:54:56,755 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:55:42\n",
            "2020-11-26 10:54:56,756 - INFO - allennlp.training.trainer - Epoch 32/99\n",
            "2020-11-26 10:54:56,756 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:54:56,852 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:54:56,853 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8773, precision-overall: 0.7274, recall-overall: 0.5254, f1-measure-overall: 0.6101, loss: 0.3659 ||: 100%|##########| 1363/1363 [01:21<00:00, 16.73it/s]\n",
            "2020-11-26 10:56:18,307 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8483, precision-overall: 0.7172, recall-overall: 0.5576, f1-measure-overall: 0.6274, loss: 0.4255 ||: 100%|##########| 453/453 [00:23<00:00, 19.59it/s]\n",
            "2020-11-26 10:56:41,435 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 10:56:41,436 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.525  |     0.558\n",
            "2020-11-26 10:56:41,437 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.877  |     0.848\n",
            "2020-11-26 10:56:41,438 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  4050.696  |       N/A\n",
            "2020-11-26 10:56:41,439 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.727  |     0.717\n",
            "2020-11-26 10:56:41,440 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   514.000  |       N/A\n",
            "2020-11-26 10:56:41,441 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.610  |     0.627\n",
            "2020-11-26 10:56:41,442 - INFO - allennlp.training.tensorboard_writer - loss               |     0.366  |     0.425\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 10:56:41,722 - INFO - allennlp.training.trainer - Epoch duration: 0:01:44.966009\n",
            "2020-11-26 10:56:41,722 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:54:06\n",
            "2020-11-26 10:56:41,722 - INFO - allennlp.training.trainer - Epoch 33/99\n",
            "2020-11-26 10:56:41,723 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4050.696\n",
            "2020-11-26 10:56:41,819 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 514\n",
            "2020-11-26 10:56:41,820 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8769, precision-overall: 0.7263, recall-overall: 0.5233, f1-measure-overall: 0.6083, loss: 0.3632 ||: 100%|##########| 1363/1363 [01:21<00:00, 16.74it/s]\n",
            "2020-11-26 10:58:03,241 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8474, precision-overall: 0.6893, recall-overall: 0.5804, f1-measure-overall: 0.6302, loss: 0.4236 ||: 100%|##########| 453/453 [00:23<00:00, 19.65it/s]\n",
            "2020-11-26 10:58:26,296 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.\n",
            "2020-11-26 10:58:26,301 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2020-11-26 10:58:26,441 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/drive/My Drive/685_project/RelEx/models/baseline_gcn_tacred/model.tar.gz\n",
            "2020-11-26 10:58:30,060 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 23,\n",
            "  \"peak_cpu_memory_MB\": 4050.696,\n",
            "  \"peak_gpu_0_memory_MB\": 514,\n",
            "  \"training_duration\": \"0:56:11.685474\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 32,\n",
            "  \"epoch\": 32,\n",
            "  \"training_accuracy\": 0.8773266396570959,\n",
            "  \"training_precision-overall\": 0.7274178104053622,\n",
            "  \"training_recall-overall\": 0.5254380571779895,\n",
            "  \"training_f1-measure-overall\": 0.610146802909237,\n",
            "  \"training_loss\": 0.36587335742731264,\n",
            "  \"training_cpu_memory_MB\": 4050.696,\n",
            "  \"training_gpu_0_memory_MB\": 514,\n",
            "  \"validation_accuracy\": 0.8483496089434845,\n",
            "  \"validation_precision-overall\": 0.7172266919072409,\n",
            "  \"validation_recall-overall\": 0.5575791022810891,\n",
            "  \"validation_f1-measure-overall\": 0.6274063340922713,\n",
            "  \"validation_loss\": 0.4254843509914333,\n",
            "  \"best_validation_accuracy\": 0.8479519243515532,\n",
            "  \"best_validation_precision-overall\": 0.67863815652896,\n",
            "  \"best_validation_recall-overall\": 0.6013612950699043,\n",
            "  \"best_validation_f1-measure-overall\": 0.6376670242855252,\n",
            "  \"best_validation_loss\": 0.4313611082946491\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA8ABm2F_w-K",
        "outputId": "f89ea1ca-4df6-424f-d59f-9220935b273c"
      },
      "source": [
        "# Train a baseline LSTM model for relation extraction (TACRED)  \n",
        "!allennlp train \\\n",
        "  ./configs/relation_classification/tacred/baseline_lstm_tacred.jsonnet \\\n",
        "  -s '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred' \\\n",
        "  --include-package relex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-11-26 11:01:36,121 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 11:01:36,828 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 11:01:36,832 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-26 11:01:37,311 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 11:01:37,312 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 11:01:37,313 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 11:01:37,313 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-26 11:01:38,126 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2020-11-26 11:01:38,127 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2020-11-26 11:01:38,127 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2020-11-26 11:01:38,133 - INFO - allennlp.common.checks - Pytorch version: 1.7.0+cu101\n",
            "2020-11-26 11:01:38,140 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2020-11-26 11:01:38,141 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2020-11-26 11:01:38,141 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'tacred'} and extras set()\n",
            "2020-11-26 11:01:38,141 - INFO - allennlp.common.params - dataset_reader.type = tacred\n",
            "2020-11-26 11:01:38,142 - INFO - allennlp.common.from_params - instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2020-11-26 11:01:38,142 - INFO - allennlp.common.params - dataset_reader.max_len = 100\n",
            "2020-11-26 11:01:38,142 - INFO - allennlp.common.params - dataset_reader.masking_mode = NER+Grammar\n",
            "2020-11-26 11:01:38,143 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-11-26 11:01:38,143 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2020-11-26 11:01:38,143 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-11-26 11:01:38,144 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
            "2020-11-26 11:01:38,144 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-11-26 11:01:38,144 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2020-11-26 11:01:38,144 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-11-26 11:01:38,145 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-11-26 11:01:38,145 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-11-26 11:01:38,145 - INFO - allennlp.common.params - dataset_reader.dep_pruning = 1\n",
            "2020-11-26 11:01:38,145 - INFO - allennlp.common.params - train_data_path = ../relex-data/tacred/train.json\n",
            "2020-11-26 11:01:38,146 - INFO - allennlp.training.util - Reading training data from ../relex-data/tacred/train.json\n",
            "0it [00:00, ?it/s]2020-11-26 11:01:38,150 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/train.json\n",
            "68124it [00:29, 2284.34it/s]\n",
            "2020-11-26 11:02:07,969 - INFO - allennlp.common.params - validation_data_path = ../relex-data/tacred/dev.json\n",
            "2020-11-26 11:02:07,970 - INFO - allennlp.training.util - Reading validation data from ../relex-data/tacred/dev.json\n",
            "0it [00:00, ?it/s]2020-11-26 11:02:07,972 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/dev.json\n",
            "22631it [00:08, 2643.58it/s]\n",
            "2020-11-26 11:02:16,532 - INFO - allennlp.common.params - test_data_path = None\n",
            "2020-11-26 11:02:16,710 - INFO - allennlp.training.trainer_pieces - From dataset instances, validation, train will be considered for vocabulary creation.\n",
            "2020-11-26 11:02:16,712 - INFO - allennlp.common.params - vocabulary.type = None\n",
            "2020-11-26 11:02:16,712 - INFO - allennlp.common.params - vocabulary.extend = False\n",
            "2020-11-26 11:02:16,712 - INFO - allennlp.common.params - vocabulary.directory_path = None\n",
            "2020-11-26 11:02:16,713 - INFO - allennlp.common.params - vocabulary.min_count = {'tokens': 2}\n",
            "2020-11-26 11:02:16,713 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2020-11-26 11:02:16,713 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2020-11-26 11:02:16,713 - INFO - allennlp.common.params - vocabulary.pretrained_files = {}\n",
            "2020-11-26 11:02:16,714 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2020-11-26 11:02:16,714 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2020-11-26 11:02:16,714 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2020-11-26 11:02:16,714 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "90755it [00:03, 26588.34it/s]\n",
            "2020-11-26 11:02:20,246 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'type': 'basic_relation_classifier', 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-11-26 11:02:20,246 - INFO - allennlp.common.params - model.type = basic_relation_classifier\n",
            "2020-11-26 11:02:20,246 - INFO - allennlp.common.from_params - instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-11-26 11:02:20,247 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\n",
            "2020-11-26 11:02:20,247 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-11-26 11:02:20,248 - INFO - allennlp.common.params - model.text_field_embedder.embedder_to_indexer_map = None\n",
            "2020-11-26 11:02:20,248 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = False\n",
            "2020-11-26 11:02:20,248 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders = None\n",
            "2020-11-26 11:02:20,248 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-11-26 11:02:20,249 - INFO - allennlp.common.params - model.text_field_embedder.tokens.type = embedding\n",
            "2020-11-26 11:02:20,249 - INFO - allennlp.common.params - model.text_field_embedder.tokens.num_embeddings = None\n",
            "2020-11-26 11:02:20,249 - INFO - allennlp.common.params - model.text_field_embedder.tokens.vocab_namespace = tokens\n",
            "2020-11-26 11:02:20,249 - INFO - allennlp.common.params - model.text_field_embedder.tokens.embedding_dim = 300\n",
            "2020-11-26 11:02:20,250 - INFO - allennlp.common.params - model.text_field_embedder.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz\n",
            "2020-11-26 11:02:20,250 - INFO - allennlp.common.params - model.text_field_embedder.tokens.projection_dim = None\n",
            "2020-11-26 11:02:20,250 - INFO - allennlp.common.params - model.text_field_embedder.tokens.trainable = False\n",
            "2020-11-26 11:02:20,250 - INFO - allennlp.common.params - model.text_field_embedder.tokens.padding_index = None\n",
            "2020-11-26 11:02:20,250 - INFO - allennlp.common.params - model.text_field_embedder.tokens.max_norm = None\n",
            "2020-11-26 11:02:20,251 - INFO - allennlp.common.params - model.text_field_embedder.tokens.norm_type = 2.0\n",
            "2020-11-26 11:02:20,251 - INFO - allennlp.common.params - model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
            "2020-11-26 11:02:20,251 - INFO - allennlp.common.params - model.text_field_embedder.tokens.sparse = False\n",
            "2020-11-26 11:02:20,257 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n",
            "2196017it [00:57, 37923.94it/s]\n",
            "2020-11-26 11:03:18,801 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n",
            "2020-11-26 11:03:19,519 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 34922 out of 41038 tokens\n",
            "2020-11-26 11:03:19,541 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,541 - INFO - allennlp.common.params - model.text_encoder.type = seq2seq_pool\n",
            "2020-11-26 11:03:19,542 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.seq2vec_encoders.seq2seq_pool_encoder.Seq2SeqPoolEncoder'> from params {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final'} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,542 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,543 - INFO - allennlp.common.params - model.text_encoder.encoder.type = lstm\n",
            "2020-11-26 11:03:19,543 - INFO - allennlp.common.params - model.text_encoder.encoder.batch_first = True\n",
            "2020-11-26 11:03:19,543 - INFO - allennlp.common.params - model.text_encoder.encoder.stateful = False\n",
            "2020-11-26 11:03:19,543 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-26 11:03:19,544 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-26 11:03:19,544 - INFO - allennlp.common.params - model.text_encoder.encoder.bidirectional = False\n",
            "2020-11-26 11:03:19,544 - INFO - allennlp.common.params - model.text_encoder.encoder.dropout = 0.5\n",
            "2020-11-26 11:03:19,544 - INFO - allennlp.common.params - model.text_encoder.encoder.hidden_size = 500\n",
            "2020-11-26 11:03:19,545 - INFO - allennlp.common.params - model.text_encoder.encoder.input_size = 360\n",
            "2020-11-26 11:03:19,545 - INFO - allennlp.common.params - model.text_encoder.encoder.num_layers = 2\n",
            "2020-11-26 11:03:19,545 - INFO - allennlp.common.params - model.text_encoder.encoder.batch_first = True\n",
            "2020-11-26 11:03:19,575 - INFO - allennlp.common.params - model.text_encoder.pooling = final\n",
            "2020-11-26 11:03:19,576 - INFO - allennlp.common.params - model.text_encoder.pooling_scope = None\n",
            "2020-11-26 11:03:19,576 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,577 - INFO - allennlp.common.params - model.classifier_feedforward.input_dim = 500\n",
            "2020-11-26 11:03:19,577 - INFO - allennlp.common.params - model.classifier_feedforward.num_layers = 1\n",
            "2020-11-26 11:03:19,577 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-11-26 11:03:19,577 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-11-26 11:03:19,578 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['linear']\n",
            "2020-11-26 11:03:19,578 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-11-26 11:03:19,578 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['linear']\n",
            "2020-11-26 11:03:19,579 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-11-26 11:03:19,579 - INFO - allennlp.common.params - type = linear\n",
            "2020-11-26 11:03:19,579 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0]\n",
            "2020-11-26 11:03:19,580 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0]\n",
            "2020-11-26 11:03:19,580 - INFO - allennlp.common.params - model.word_dropout = 0.04\n",
            "2020-11-26 11:03:19,581 - INFO - allennlp.common.params - model.embedding_dropout = 0\n",
            "2020-11-26 11:03:19,581 - INFO - allennlp.common.params - model.encoding_dropout = 0\n",
            "2020-11-26 11:03:19,581 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,582 - INFO - allennlp.common.params - model.offset_embedder_head.type = relative\n",
            "2020-11-26 11:03:19,582 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,582 - INFO - allennlp.common.params - model.offset_embedder_head.n_position = 100\n",
            "2020-11-26 11:03:19,583 - INFO - allennlp.common.params - model.offset_embedder_head.embedding_dim = 30\n",
            "2020-11-26 11:03:19,583 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,584 - INFO - allennlp.common.params - model.offset_embedder_tail.type = relative\n",
            "2020-11-26 11:03:19,584 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-11-26 11:03:19,584 - INFO - allennlp.common.params - model.offset_embedder_tail.n_position = 100\n",
            "2020-11-26 11:03:19,585 - INFO - allennlp.common.params - model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-11-26 11:03:19,585 - INFO - allennlp.common.params - model.verbose_metrics = False\n",
            "2020-11-26 11:03:19,585 - INFO - allennlp.common.params - model.ignore_label = no_relation\n",
            "2020-11-26 11:03:19,586 - INFO - allennlp.common.params - model.f1_average = micro\n",
            "2020-11-26 11:03:19,586 - INFO - allennlp.common.params - model.use_adjacency = False\n",
            "2020-11-26 11:03:19,586 - INFO - allennlp.common.params - model.use_entity_offsets = False\n",
            "2020-11-26 11:03:19,587 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-11-26 11:03:19,587 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-11-26 11:03:19,587 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-26 11:03:19,588 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-26 11:03:19,588 - INFO - allennlp.nn.initializers -    offset_embedder_head._embedding.weight\n",
            "2020-11-26 11:03:19,588 - INFO - allennlp.nn.initializers -    offset_embedder_tail._embedding.weight\n",
            "2020-11-26 11:03:19,588 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.bias_hh_l0\n",
            "2020-11-26 11:03:19,588 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.bias_hh_l1\n",
            "2020-11-26 11:03:19,589 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.bias_ih_l0\n",
            "2020-11-26 11:03:19,589 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.bias_ih_l1\n",
            "2020-11-26 11:03:19,589 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.weight_hh_l0\n",
            "2020-11-26 11:03:19,589 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.weight_hh_l1\n",
            "2020-11-26 11:03:19,590 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.weight_ih_l0\n",
            "2020-11-26 11:03:19,590 - INFO - allennlp.nn.initializers -    text_encoder._encoder._module.weight_ih_l1\n",
            "2020-11-26 11:03:19,590 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-26 11:03:19,721 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']], 'type': 'bucket'} and extras set()\n",
            "2020-11-26 11:03:19,722 - INFO - allennlp.common.params - iterator.type = bucket\n",
            "2020-11-26 11:03:19,722 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']]} and extras set()\n",
            "2020-11-26 11:03:19,723 - INFO - allennlp.common.params - iterator.sorting_keys = [['text', 'num_tokens']]\n",
            "2020-11-26 11:03:19,723 - INFO - allennlp.common.params - iterator.padding_noise = 0.1\n",
            "2020-11-26 11:03:19,723 - INFO - allennlp.common.params - iterator.biggest_batch_first = False\n",
            "2020-11-26 11:03:19,723 - INFO - allennlp.common.params - iterator.batch_size = 50\n",
            "2020-11-26 11:03:19,724 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2020-11-26 11:03:19,724 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2020-11-26 11:03:19,724 - INFO - allennlp.common.params - iterator.cache_instances = False\n",
            "2020-11-26 11:03:19,724 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2020-11-26 11:03:19,725 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2020-11-26 11:03:19,725 - INFO - allennlp.common.params - iterator.skip_smaller_batches = False\n",
            "2020-11-26 11:03:19,725 - INFO - allennlp.common.params - validation_iterator = None\n",
            "2020-11-26 11:03:19,725 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2020-11-26 11:03:19,726 - INFO - allennlp.training.trainer_pieces - Following parameters are Frozen  (without gradient):\n",
            "2020-11-26 11:03:19,726 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-26 11:03:19,726 - INFO - allennlp.training.trainer_pieces - Following parameters are Tunable (with gradient):\n",
            "2020-11-26 11:03:19,727 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.weight_ih_l0\n",
            "2020-11-26 11:03:19,727 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.weight_hh_l0\n",
            "2020-11-26 11:03:19,727 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.bias_ih_l0\n",
            "2020-11-26 11:03:19,727 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.bias_hh_l0\n",
            "2020-11-26 11:03:19,727 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.weight_ih_l1\n",
            "2020-11-26 11:03:19,728 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.weight_hh_l1\n",
            "2020-11-26 11:03:19,728 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.bias_ih_l1\n",
            "2020-11-26 11:03:19,728 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder._module.bias_hh_l1\n",
            "2020-11-26 11:03:19,728 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-26 11:03:19,728 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-26 11:03:19,729 - INFO - allennlp.training.trainer_pieces - offset_embedder_head._embedding.weight\n",
            "2020-11-26 11:03:19,729 - INFO - allennlp.training.trainer_pieces - offset_embedder_tail._embedding.weight\n",
            "2020-11-26 11:03:19,729 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2020-11-26 11:03:19,729 - INFO - allennlp.common.params - trainer.validation_metric = +f1-measure-overall\n",
            "2020-11-26 11:03:19,729 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2020-11-26 11:03:19,730 - INFO - allennlp.common.params - trainer.num_epochs = 30\n",
            "2020-11-26 11:03:19,730 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2020-11-26 11:03:19,730 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2020-11-26 11:03:19,730 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2020-11-26 11:03:19,730 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2020-11-26 11:03:22,227 - INFO - allennlp.common.params - trainer.optimizer.type = adagrad\n",
            "2020-11-26 11:03:22,228 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2020-11-26 11:03:22,228 - INFO - allennlp.training.optimizers - Number of trainable parameters: 3761102\n",
            "2020-11-26 11:03:22,229 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2020-11-26 11:03:22,229 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-26 11:03:22,230 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-26 11:03:22,230 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.01\n",
            "2020-11-26 11:03:22,230 - INFO - allennlp.common.registrable - instantiating registered subclass adagrad of <class 'allennlp.training.optimizers.Optimizer'>\n",
            "2020-11-26 11:03:22,231 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = multi_step\n",
            "2020-11-26 11:03:22,231 - INFO - allennlp.common.registrable - instantiating registered subclass multi_step of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
            "2020-11-26 11:03:22,232 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-26 11:03:22,232 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-26 11:03:22,232 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gamma = 0.9\n",
            "2020-11-26 11:03:22,232 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.milestones = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "2020-11-26 11:03:22,233 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1\n",
            "2020-11-26 11:03:22,233 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2020-11-26 11:03:22,233 - INFO - allennlp.common.params - trainer.model_save_interval = None\n",
            "2020-11-26 11:03:22,233 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2020-11-26 11:03:22,234 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2020-11-26 11:03:22,234 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2020-11-26 11:03:22,234 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False\n",
            "2020-11-26 11:03:22,234 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2020-11-26 11:03:22,255 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2020-11-26 11:03:22,255 - INFO - allennlp.training.trainer - Epoch 0/29\n",
            "2020-11-26 11:03:22,256 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3738.728\n",
            "2020-11-26 11:03:22,350 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 482\n",
            "2020-11-26 11:03:22,351 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8156, precision-overall: 0.5990, recall-overall: 0.0853, f1-measure-overall: 0.1493, loss: 0.8606 ||: 100%|##########| 1363/1363 [02:22<00:00,  9.59it/s]\n",
            "2020-11-26 11:05:44,527 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7925, precision-overall: 0.6526, recall-overall: 0.2494, f1-measure-overall: 0.3609, loss: 0.8142 ||: 100%|##########| 453/453 [00:31<00:00, 14.50it/s]\n",
            "2020-11-26 11:06:15,774 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:06:15,775 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3738.728  |       N/A\n",
            "2020-11-26 11:06:15,776 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.085  |     0.249\n",
            "2020-11-26 11:06:15,777 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.149  |     0.361\n",
            "2020-11-26 11:06:15,777 - INFO - allennlp.training.tensorboard_writer - loss               |     0.861  |     0.814\n",
            "2020-11-26 11:06:15,778 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.816  |     0.793\n",
            "2020-11-26 11:06:15,780 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   482.000  |       N/A\n",
            "2020-11-26 11:06:15,781 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.599  |     0.653\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:06:16,209 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:06:17,095 - INFO - allennlp.training.trainer - Epoch duration: 0:02:54.839470\n",
            "2020-11-26 11:06:17,097 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:24:30\n",
            "2020-11-26 11:06:17,097 - INFO - allennlp.training.trainer - Epoch 1/29\n",
            "2020-11-26 11:06:17,097 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3968.828\n",
            "2020-11-26 11:06:17,195 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:06:17,196 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8488, precision-overall: 0.6834, recall-overall: 0.3352, f1-measure-overall: 0.4497, loss: 0.5330 ||: 100%|##########| 1363/1363 [02:16<00:00, 10.00it/s]\n",
            "2020-11-26 11:08:33,442 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8158, precision-overall: 0.6791, recall-overall: 0.3355, f1-measure-overall: 0.4492, loss: 0.6165 ||: 100%|##########| 453/453 [00:27<00:00, 16.39it/s]\n",
            "2020-11-26 11:09:01,079 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:09:01,080 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3968.828  |       N/A\n",
            "2020-11-26 11:09:01,081 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.335  |     0.336\n",
            "2020-11-26 11:09:01,082 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.450  |     0.449\n",
            "2020-11-26 11:09:01,083 - INFO - allennlp.training.tensorboard_writer - loss               |     0.533  |     0.616\n",
            "2020-11-26 11:09:01,084 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.849  |     0.816\n",
            "2020-11-26 11:09:01,085 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:09:01,086 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.683  |     0.679\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:09:01,459 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:09:01,945 - INFO - allennlp.training.trainer - Epoch duration: 0:02:44.847711\n",
            "2020-11-26 11:09:01,945 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:19:15\n",
            "2020-11-26 11:09:01,945 - INFO - allennlp.training.trainer - Epoch 2/29\n",
            "2020-11-26 11:09:01,945 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.644\n",
            "2020-11-26 11:09:02,039 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:09:02,040 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8620, precision-overall: 0.6929, recall-overall: 0.4154, f1-measure-overall: 0.5194, loss: 0.4572 ||: 100%|##########| 1363/1363 [02:16<00:00,  9.99it/s]\n",
            "2020-11-26 11:11:18,495 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8240, precision-overall: 0.7050, recall-overall: 0.3631, f1-measure-overall: 0.4794, loss: 0.5660 ||: 100%|##########| 453/453 [00:28<00:00, 15.96it/s]\n",
            "2020-11-26 11:11:46,874 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:11:46,875 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.644  |       N/A\n",
            "2020-11-26 11:11:46,876 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.415  |     0.363\n",
            "2020-11-26 11:11:46,877 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.519  |     0.479\n",
            "2020-11-26 11:11:46,878 - INFO - allennlp.training.tensorboard_writer - loss               |     0.457  |     0.566\n",
            "2020-11-26 11:11:46,879 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.862  |     0.824\n",
            "2020-11-26 11:11:46,881 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:11:46,882 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.693  |     0.705\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:11:47,242 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:11:48,077 - INFO - allennlp.training.trainer - Epoch duration: 0:02:46.131537\n",
            "2020-11-26 11:11:48,077 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:15:52\n",
            "2020-11-26 11:11:48,078 - INFO - allennlp.training.trainer - Epoch 3/29\n",
            "2020-11-26 11:11:48,078 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.652\n",
            "2020-11-26 11:11:48,172 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:11:48,173 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8699, precision-overall: 0.7130, recall-overall: 0.4573, f1-measure-overall: 0.5573, loss: 0.4171 ||: 100%|##########| 1363/1363 [02:14<00:00, 10.14it/s]\n",
            "2020-11-26 11:14:02,558 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8301, precision-overall: 0.6581, recall-overall: 0.4426, f1-measure-overall: 0.5293, loss: 0.5322 ||: 100%|##########| 453/453 [00:28<00:00, 16.13it/s]\n",
            "2020-11-26 11:14:30,644 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:14:30,645 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.652  |       N/A\n",
            "2020-11-26 11:14:30,646 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.457  |     0.443\n",
            "2020-11-26 11:14:30,647 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.557  |     0.529\n",
            "2020-11-26 11:14:30,648 - INFO - allennlp.training.tensorboard_writer - loss               |     0.417  |     0.532\n",
            "2020-11-26 11:14:30,649 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.870  |     0.830\n",
            "2020-11-26 11:14:30,650 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:14:30,651 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.713  |     0.658\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:14:31,001 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:14:31,484 - INFO - allennlp.training.trainer - Epoch duration: 0:02:43.406042\n",
            "2020-11-26 11:14:31,484 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:12:29\n",
            "2020-11-26 11:14:31,484 - INFO - allennlp.training.trainer - Epoch 4/29\n",
            "2020-11-26 11:14:31,484 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.66\n",
            "2020-11-26 11:14:31,578 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:14:31,579 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8765, precision-overall: 0.7301, recall-overall: 0.4931, f1-measure-overall: 0.5886, loss: 0.3932 ||: 100%|##########| 1363/1363 [02:16<00:00, 10.02it/s]\n",
            "2020-11-26 11:16:47,660 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8343, precision-overall: 0.6653, recall-overall: 0.4857, f1-measure-overall: 0.5615, loss: 0.5146 ||: 100%|##########| 453/453 [00:29<00:00, 15.33it/s]\n",
            "2020-11-26 11:17:17,210 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:17:17,210 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.660  |       N/A\n",
            "2020-11-26 11:17:17,211 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.493  |     0.486\n",
            "2020-11-26 11:17:17,213 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.589  |     0.561\n",
            "2020-11-26 11:17:17,213 - INFO - allennlp.training.tensorboard_writer - loss               |     0.393  |     0.515\n",
            "2020-11-26 11:17:17,214 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.876  |     0.834\n",
            "2020-11-26 11:17:17,216 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:17:17,216 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.730  |     0.665\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:17:17,544 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:17:18,427 - INFO - allennlp.training.trainer - Epoch duration: 0:02:46.942316\n",
            "2020-11-26 11:17:18,427 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:09:40\n",
            "2020-11-26 11:17:18,427 - INFO - allennlp.training.trainer - Epoch 5/29\n",
            "2020-11-26 11:17:18,428 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.86\n",
            "2020-11-26 11:17:18,522 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:17:18,523 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8817, precision-overall: 0.7394, recall-overall: 0.5224, f1-measure-overall: 0.6122, loss: 0.3741 ||: 100%|##########| 1363/1363 [02:16<00:00,  9.96it/s]\n",
            "2020-11-26 11:19:35,353 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8416, precision-overall: 0.6781, recall-overall: 0.5193, f1-measure-overall: 0.5882, loss: 0.4857 ||: 100%|##########| 453/453 [00:29<00:00, 15.29it/s]\n",
            "2020-11-26 11:20:04,978 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:20:04,979 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.860  |       N/A\n",
            "2020-11-26 11:20:04,980 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.522  |     0.519\n",
            "2020-11-26 11:20:04,981 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.612  |     0.588\n",
            "2020-11-26 11:20:04,982 - INFO - allennlp.training.tensorboard_writer - loss               |     0.374  |     0.486\n",
            "2020-11-26 11:20:04,983 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.882  |     0.842\n",
            "2020-11-26 11:20:04,984 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:20:04,985 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.739  |     0.678\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:20:05,319 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:20:05,774 - INFO - allennlp.training.trainer - Epoch duration: 0:02:47.346176\n",
            "2020-11-26 11:20:05,774 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:06:54\n",
            "2020-11-26 11:20:05,774 - INFO - allennlp.training.trainer - Epoch 6/29\n",
            "2020-11-26 11:20:05,775 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.86\n",
            "2020-11-26 11:20:05,870 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:20:05,871 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8858, precision-overall: 0.7496, recall-overall: 0.5430, f1-measure-overall: 0.6298, loss: 0.3557 ||: 100%|##########| 1363/1363 [02:14<00:00, 10.15it/s]\n",
            "2020-11-26 11:22:20,172 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8427, precision-overall: 0.6887, recall-overall: 0.5224, f1-measure-overall: 0.5941, loss: 0.4818 ||: 100%|##########| 453/453 [00:26<00:00, 16.84it/s]\n",
            "2020-11-26 11:22:47,075 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:22:47,076 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.860  |       N/A\n",
            "2020-11-26 11:22:47,077 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.543  |     0.522\n",
            "2020-11-26 11:22:47,078 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.630  |     0.594\n",
            "2020-11-26 11:22:47,079 - INFO - allennlp.training.tensorboard_writer - loss               |     0.356  |     0.482\n",
            "2020-11-26 11:22:47,080 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.886  |     0.843\n",
            "2020-11-26 11:22:47,081 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:22:47,082 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.750  |     0.689\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:22:47,434 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:22:47,908 - INFO - allennlp.training.trainer - Epoch duration: 0:02:42.133110\n",
            "2020-11-26 11:22:47,908 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:03:50\n",
            "2020-11-26 11:22:47,908 - INFO - allennlp.training.trainer - Epoch 7/29\n",
            "2020-11-26 11:22:47,908 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.86\n",
            "2020-11-26 11:22:48,005 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:22:48,006 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8899, precision-overall: 0.7553, recall-overall: 0.5666, f1-measure-overall: 0.6475, loss: 0.3404 ||: 100%|##########| 1363/1363 [02:14<00:00, 10.14it/s]\n",
            "2020-11-26 11:25:02,409 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8394, precision-overall: 0.6556, recall-overall: 0.5364, f1-measure-overall: 0.5900, loss: 0.4864 ||: 100%|##########| 453/453 [00:28<00:00, 16.16it/s]\n",
            "2020-11-26 11:25:30,444 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:25:30,445 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.860  |       N/A\n",
            "2020-11-26 11:25:30,446 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.567  |     0.536\n",
            "2020-11-26 11:25:30,447 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.647  |     0.590\n",
            "2020-11-26 11:25:30,448 - INFO - allennlp.training.tensorboard_writer - loss               |     0.340  |     0.486\n",
            "2020-11-26 11:25:30,449 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.890  |     0.839\n",
            "2020-11-26 11:25:30,451 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:25:30,452 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.755  |     0.656\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:25:30,818 - INFO - allennlp.training.trainer - Epoch duration: 0:02:42.909343\n",
            "2020-11-26 11:25:30,818 - INFO - allennlp.training.trainer - Estimated training time remaining: 1:00:53\n",
            "2020-11-26 11:25:30,818 - INFO - allennlp.training.trainer - Epoch 8/29\n",
            "2020-11-26 11:25:30,819 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.86\n",
            "2020-11-26 11:25:30,920 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:25:30,921 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8947, precision-overall: 0.7600, recall-overall: 0.5933, f1-measure-overall: 0.6664, loss: 0.3260 ||: 100%|##########| 1363/1363 [02:15<00:00, 10.04it/s]\n",
            "2020-11-26 11:27:46,720 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8378, precision-overall: 0.6600, recall-overall: 0.5445, f1-measure-overall: 0.5967, loss: 0.4733 ||: 100%|##########| 453/453 [00:29<00:00, 15.14it/s]\n",
            "2020-11-26 11:28:16,648 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:28:16,649 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.860  |       N/A\n",
            "2020-11-26 11:28:16,650 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.593  |     0.545\n",
            "2020-11-26 11:28:16,651 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.666  |     0.597\n",
            "2020-11-26 11:28:16,652 - INFO - allennlp.training.tensorboard_writer - loss               |     0.326  |     0.473\n",
            "2020-11-26 11:28:16,653 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.895  |     0.838\n",
            "2020-11-26 11:28:16,654 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:28:16,654 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.760  |     0.660\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:28:16,991 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:28:17,442 - INFO - allennlp.training.trainer - Epoch duration: 0:02:46.623255\n",
            "2020-11-26 11:28:17,442 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:58:08\n",
            "2020-11-26 11:28:17,442 - INFO - allennlp.training.trainer - Epoch 9/29\n",
            "2020-11-26 11:28:17,442 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.86\n",
            "2020-11-26 11:28:17,537 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:28:17,538 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.8983, precision-overall: 0.7714, recall-overall: 0.6081, f1-measure-overall: 0.6801, loss: 0.3132 ||: 100%|##########| 1363/1363 [02:14<00:00, 10.10it/s]\n",
            "2020-11-26 11:30:32,540 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8445, precision-overall: 0.6901, recall-overall: 0.5362, f1-measure-overall: 0.6035, loss: 0.4747 ||: 100%|##########| 453/453 [00:27<00:00, 16.62it/s]\n",
            "2020-11-26 11:30:59,800 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:30:59,801 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.860  |       N/A\n",
            "2020-11-26 11:30:59,802 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.608  |     0.536\n",
            "2020-11-26 11:30:59,803 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.680  |     0.604\n",
            "2020-11-26 11:30:59,804 - INFO - allennlp.training.tensorboard_writer - loss               |     0.313  |     0.475\n",
            "2020-11-26 11:30:59,805 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.898  |     0.845\n",
            "2020-11-26 11:30:59,806 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:30:59,807 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.771  |     0.690\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:31:00,141 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:31:00,593 - INFO - allennlp.training.trainer - Epoch duration: 0:02:43.150869\n",
            "2020-11-26 11:31:00,594 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:55:16\n",
            "2020-11-26 11:31:00,594 - INFO - allennlp.training.trainer - Epoch 10/29\n",
            "2020-11-26 11:31:00,594 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.876\n",
            "2020-11-26 11:31:00,689 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:31:00,690 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9017, precision-overall: 0.7748, recall-overall: 0.6250, f1-measure-overall: 0.6919, loss: 0.3026 ||: 100%|##########| 1363/1363 [02:14<00:00, 10.14it/s]\n",
            "2020-11-26 11:33:15,073 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8444, precision-overall: 0.6685, recall-overall: 0.5646, f1-measure-overall: 0.6121, loss: 0.4827 ||: 100%|##########| 453/453 [00:30<00:00, 15.03it/s]\n",
            "2020-11-26 11:33:45,210 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:33:45,211 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.876  |       N/A\n",
            "2020-11-26 11:33:45,212 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.625  |     0.565\n",
            "2020-11-26 11:33:45,213 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.692  |     0.612\n",
            "2020-11-26 11:33:45,214 - INFO - allennlp.training.tensorboard_writer - loss               |     0.303  |     0.483\n",
            "2020-11-26 11:33:45,216 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.902  |     0.844\n",
            "2020-11-26 11:33:45,217 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:33:45,218 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.775  |     0.668\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:33:45,570 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:33:46,032 - INFO - allennlp.training.trainer - Epoch duration: 0:02:45.437895\n",
            "2020-11-26 11:33:46,032 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:52:30\n",
            "2020-11-26 11:33:46,032 - INFO - allennlp.training.trainer - Epoch 11/29\n",
            "2020-11-26 11:33:46,033 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.876\n",
            "2020-11-26 11:33:46,132 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:33:46,133 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9058, precision-overall: 0.7818, recall-overall: 0.6453, f1-measure-overall: 0.7070, loss: 0.2895 ||: 100%|##########| 1363/1363 [02:17<00:00,  9.92it/s]\n",
            "2020-11-26 11:36:03,474 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8432, precision-overall: 0.6750, recall-overall: 0.5556, f1-measure-overall: 0.6095, loss: 0.4751 ||: 100%|##########| 453/453 [00:28<00:00, 15.69it/s]\n",
            "2020-11-26 11:36:32,342 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:36:32,343 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.876  |       N/A\n",
            "2020-11-26 11:36:32,344 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.645  |     0.556\n",
            "2020-11-26 11:36:32,345 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.707  |     0.609\n",
            "2020-11-26 11:36:32,346 - INFO - allennlp.training.tensorboard_writer - loss               |     0.289  |     0.475\n",
            "2020-11-26 11:36:32,347 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.906  |     0.843\n",
            "2020-11-26 11:36:32,348 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:36:32,349 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.782  |     0.675\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:36:32,759 - INFO - allennlp.training.trainer - Epoch duration: 0:02:46.726857\n",
            "2020-11-26 11:36:32,760 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:49:45\n",
            "2020-11-26 11:36:32,760 - INFO - allennlp.training.trainer - Epoch 12/29\n",
            "2020-11-26 11:36:32,760 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.876\n",
            "2020-11-26 11:36:32,857 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:36:32,860 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9084, precision-overall: 0.7864, recall-overall: 0.6592, f1-measure-overall: 0.7172, loss: 0.2776 ||: 100%|##########| 1363/1363 [02:19<00:00,  9.75it/s]\n",
            "2020-11-26 11:38:52,668 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8425, precision-overall: 0.6740, recall-overall: 0.5440, f1-measure-overall: 0.6021, loss: 0.4885 ||: 100%|##########| 453/453 [00:29<00:00, 15.50it/s]\n",
            "2020-11-26 11:39:21,903 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:39:21,904 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.876  |       N/A\n",
            "2020-11-26 11:39:21,905 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.659  |     0.544\n",
            "2020-11-26 11:39:21,906 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.717  |     0.602\n",
            "2020-11-26 11:39:21,907 - INFO - allennlp.training.tensorboard_writer - loss               |     0.278  |     0.489\n",
            "2020-11-26 11:39:21,908 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.908  |     0.843\n",
            "2020-11-26 11:39:21,909 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:39:21,910 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.786  |     0.674\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:39:22,280 - INFO - allennlp.training.trainer - Epoch duration: 0:02:49.520235\n",
            "2020-11-26 11:39:22,281 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:47:04\n",
            "2020-11-26 11:39:22,281 - INFO - allennlp.training.trainer - Epoch 13/29\n",
            "2020-11-26 11:39:22,281 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:39:22,375 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:39:22,376 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9141, precision-overall: 0.7982, recall-overall: 0.6827, f1-measure-overall: 0.7359, loss: 0.2619 ||: 100%|##########| 1363/1363 [02:21<00:00,  9.63it/s]\n",
            "2020-11-26 11:41:43,860 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8394, precision-overall: 0.6631, recall-overall: 0.5425, f1-measure-overall: 0.5968, loss: 0.5035 ||: 100%|##########| 453/453 [00:31<00:00, 14.58it/s]\n",
            "2020-11-26 11:42:14,934 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:42:14,935 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:42:14,936 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.683  |     0.542\n",
            "2020-11-26 11:42:14,937 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.736  |     0.597\n",
            "2020-11-26 11:42:14,938 - INFO - allennlp.training.tensorboard_writer - loss               |     0.262  |     0.503\n",
            "2020-11-26 11:42:14,940 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.914  |     0.839\n",
            "2020-11-26 11:42:14,941 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:42:14,941 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.798  |     0.663\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:42:15,331 - INFO - allennlp.training.trainer - Epoch duration: 0:02:53.050203\n",
            "2020-11-26 11:42:15,332 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:44:26\n",
            "2020-11-26 11:42:15,332 - INFO - allennlp.training.trainer - Epoch 14/29\n",
            "2020-11-26 11:42:15,332 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:42:15,429 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:42:15,430 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9180, precision-overall: 0.8066, recall-overall: 0.6991, f1-measure-overall: 0.7490, loss: 0.2514 ||: 100%|##########| 1363/1363 [02:22<00:00,  9.57it/s]\n",
            "2020-11-26 11:44:37,919 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8438, precision-overall: 0.6768, recall-overall: 0.5596, f1-measure-overall: 0.6126, loss: 0.5367 ||: 100%|##########| 453/453 [00:28<00:00, 16.01it/s]\n",
            "2020-11-26 11:45:06,216 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:45:06,217 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:45:06,218 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.699  |     0.560\n",
            "2020-11-26 11:45:06,219 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.749  |     0.613\n",
            "2020-11-26 11:45:06,220 - INFO - allennlp.training.tensorboard_writer - loss               |     0.251  |     0.537\n",
            "2020-11-26 11:45:06,221 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.918  |     0.844\n",
            "2020-11-26 11:45:06,222 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:45:06,222 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.807  |     0.677\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:45:06,565 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/best.th'.\n",
            "2020-11-26 11:45:07,044 - INFO - allennlp.training.trainer - Epoch duration: 0:02:51.711948\n",
            "2020-11-26 11:45:07,045 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:41:44\n",
            "2020-11-26 11:45:07,045 - INFO - allennlp.training.trainer - Epoch 15/29\n",
            "2020-11-26 11:45:07,045 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:45:07,142 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:45:07,143 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9221, precision-overall: 0.8099, recall-overall: 0.7187, f1-measure-overall: 0.7616, loss: 0.2378 ||: 100%|##########| 1363/1363 [02:18<00:00,  9.82it/s]\n",
            "2020-11-26 11:47:25,926 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8385, precision-overall: 0.6716, recall-overall: 0.5191, f1-measure-overall: 0.5856, loss: 0.5493 ||: 100%|##########| 453/453 [00:29<00:00, 15.12it/s]\n",
            "2020-11-26 11:47:55,898 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:47:55,898 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:47:55,899 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.719  |     0.519\n",
            "2020-11-26 11:47:55,900 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.762  |     0.586\n",
            "2020-11-26 11:47:55,901 - INFO - allennlp.training.tensorboard_writer - loss               |     0.238  |     0.549\n",
            "2020-11-26 11:47:55,902 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.922  |     0.838\n",
            "2020-11-26 11:47:55,903 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:47:55,904 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.810  |     0.672\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:47:56,256 - INFO - allennlp.training.trainer - Epoch duration: 0:02:49.211423\n",
            "2020-11-26 11:47:56,257 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:38:59\n",
            "2020-11-26 11:47:56,257 - INFO - allennlp.training.trainer - Epoch 16/29\n",
            "2020-11-26 11:47:56,257 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:47:56,357 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:47:56,358 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9279, precision-overall: 0.8255, recall-overall: 0.7402, f1-measure-overall: 0.7805, loss: 0.2243 ||: 100%|##########| 1363/1363 [02:19<00:00,  9.76it/s]\n",
            "2020-11-26 11:50:16,026 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8400, precision-overall: 0.6604, recall-overall: 0.5495, f1-measure-overall: 0.5999, loss: 0.5427 ||: 100%|##########| 453/453 [00:31<00:00, 14.54it/s]\n",
            "2020-11-26 11:50:47,184 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:50:47,185 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:50:47,185 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.740  |     0.549\n",
            "2020-11-26 11:50:47,187 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.781  |     0.600\n",
            "2020-11-26 11:50:47,188 - INFO - allennlp.training.tensorboard_writer - loss               |     0.224  |     0.543\n",
            "2020-11-26 11:50:47,189 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.928  |     0.840\n",
            "2020-11-26 11:50:47,191 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:50:47,191 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.825  |     0.660\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:50:47,566 - INFO - allennlp.training.trainer - Epoch duration: 0:02:51.308771\n",
            "2020-11-26 11:50:47,566 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:36:15\n",
            "2020-11-26 11:50:47,566 - INFO - allennlp.training.trainer - Epoch 17/29\n",
            "2020-11-26 11:50:47,567 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:50:47,663 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:50:47,664 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9331, precision-overall: 0.8349, recall-overall: 0.7611, f1-measure-overall: 0.7963, loss: 0.2079 ||: 100%|##########| 1363/1363 [02:21<00:00,  9.66it/s]\n",
            "2020-11-26 11:53:08,799 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8357, precision-overall: 0.6521, recall-overall: 0.5447, f1-measure-overall: 0.5936, loss: 0.5701 ||: 100%|##########| 453/453 [00:31<00:00, 14.56it/s]\n",
            "2020-11-26 11:53:39,908 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:53:39,908 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:53:39,909 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.761  |     0.545\n",
            "2020-11-26 11:53:39,910 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.796  |     0.594\n",
            "2020-11-26 11:53:39,911 - INFO - allennlp.training.tensorboard_writer - loss               |     0.208  |     0.570\n",
            "2020-11-26 11:53:39,912 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.933  |     0.836\n",
            "2020-11-26 11:53:39,913 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:53:39,914 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.835  |     0.652\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:53:40,306 - INFO - allennlp.training.trainer - Epoch duration: 0:02:52.739699\n",
            "2020-11-26 11:53:40,307 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:33:32\n",
            "2020-11-26 11:53:40,307 - INFO - allennlp.training.trainer - Epoch 18/29\n",
            "2020-11-26 11:53:40,307 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:53:40,404 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:53:40,405 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9370, precision-overall: 0.8412, recall-overall: 0.7790, f1-measure-overall: 0.8089, loss: 0.1928 ||: 100%|##########| 1363/1363 [02:21<00:00,  9.65it/s]\n",
            "2020-11-26 11:56:01,611 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8403, precision-overall: 0.6574, recall-overall: 0.5655, f1-measure-overall: 0.6080, loss: 0.5691 ||: 100%|##########| 453/453 [00:30<00:00, 14.62it/s]\n",
            "2020-11-26 11:56:32,608 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:56:32,609 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:56:32,610 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.779  |     0.565\n",
            "2020-11-26 11:56:32,611 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.809  |     0.608\n",
            "2020-11-26 11:56:32,612 - INFO - allennlp.training.tensorboard_writer - loss               |     0.193  |     0.569\n",
            "2020-11-26 11:56:32,614 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.937  |     0.840\n",
            "2020-11-26 11:56:32,615 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:56:32,616 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.841  |     0.657\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:56:32,990 - INFO - allennlp.training.trainer - Epoch duration: 0:02:52.682647\n",
            "2020-11-26 11:56:32,990 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:30:47\n",
            "2020-11-26 11:56:32,991 - INFO - allennlp.training.trainer - Epoch 19/29\n",
            "2020-11-26 11:56:32,991 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:56:33,088 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:56:33,089 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9419, precision-overall: 0.8509, recall-overall: 0.7958, f1-measure-overall: 0.8224, loss: 0.1787 ||: 100%|##########| 1363/1363 [02:21<00:00,  9.66it/s]\n",
            "2020-11-26 11:58:54,220 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8370, precision-overall: 0.6513, recall-overall: 0.5499, f1-measure-overall: 0.5963, loss: 0.6038 ||: 100%|##########| 453/453 [00:28<00:00, 16.15it/s]\n",
            "2020-11-26 11:59:22,265 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 11:59:22,265 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 11:59:22,266 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.796  |     0.550\n",
            "2020-11-26 11:59:22,268 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.822  |     0.596\n",
            "2020-11-26 11:59:22,269 - INFO - allennlp.training.tensorboard_writer - loss               |     0.179  |     0.604\n",
            "2020-11-26 11:59:22,270 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.942  |     0.837\n",
            "2020-11-26 11:59:22,271 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 11:59:22,272 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.851  |     0.651\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 11:59:22,660 - INFO - allennlp.training.trainer - Epoch duration: 0:02:49.669461\n",
            "2020-11-26 11:59:22,661 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:28:00\n",
            "2020-11-26 11:59:22,661 - INFO - allennlp.training.trainer - Epoch 20/29\n",
            "2020-11-26 11:59:22,661 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 11:59:22,758 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 11:59:22,759 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9474, precision-overall: 0.8635, recall-overall: 0.8156, f1-measure-overall: 0.8389, loss: 0.1663 ||: 100%|##########| 1363/1363 [02:20<00:00,  9.73it/s]\n",
            "2020-11-26 12:01:42,778 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8311, precision-overall: 0.6279, recall-overall: 0.5559, f1-measure-overall: 0.5897, loss: 0.6355 ||: 100%|##########| 453/453 [00:33<00:00, 13.68it/s]\n",
            "2020-11-26 12:02:15,898 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 12:02:15,898 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 12:02:15,899 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.816  |     0.556\n",
            "2020-11-26 12:02:15,900 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.839  |     0.590\n",
            "2020-11-26 12:02:15,901 - INFO - allennlp.training.tensorboard_writer - loss               |     0.166  |     0.636\n",
            "2020-11-26 12:02:15,902 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.947  |     0.831\n",
            "2020-11-26 12:02:15,903 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 12:02:15,904 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.863  |     0.628\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 12:02:16,258 - INFO - allennlp.training.trainer - Epoch duration: 0:02:53.596897\n",
            "2020-11-26 12:02:16,258 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:25:14\n",
            "2020-11-26 12:02:16,258 - INFO - allennlp.training.trainer - Epoch 21/29\n",
            "2020-11-26 12:02:16,259 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 12:02:16,351 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 12:02:16,352 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9498, precision-overall: 0.8645, recall-overall: 0.8260, f1-measure-overall: 0.8448, loss: 0.1566 ||: 100%|##########| 1363/1363 [02:19<00:00,  9.74it/s]\n",
            "2020-11-26 12:04:36,335 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8298, precision-overall: 0.6246, recall-overall: 0.5543, f1-measure-overall: 0.5873, loss: 0.6512 ||: 100%|##########| 453/453 [00:31<00:00, 14.52it/s]\n",
            "2020-11-26 12:05:07,531 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 12:05:07,532 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 12:05:07,533 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.826  |     0.554\n",
            "2020-11-26 12:05:07,534 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.845  |     0.587\n",
            "2020-11-26 12:05:07,535 - INFO - allennlp.training.tensorboard_writer - loss               |     0.157  |     0.651\n",
            "2020-11-26 12:05:07,536 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.950  |     0.830\n",
            "2020-11-26 12:05:07,537 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 12:05:07,538 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.865  |     0.625\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 12:05:07,913 - INFO - allennlp.training.trainer - Epoch duration: 0:02:51.654785\n",
            "2020-11-26 12:05:07,914 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:22:27\n",
            "2020-11-26 12:05:07,914 - INFO - allennlp.training.trainer - Epoch 22/29\n",
            "2020-11-26 12:05:07,914 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 12:05:08,023 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 12:05:08,024 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9544, precision-overall: 0.8777, recall-overall: 0.8415, f1-measure-overall: 0.8592, loss: 0.1449 ||: 100%|##########| 1363/1363 [02:19<00:00,  9.74it/s]\n",
            "2020-11-26 12:07:27,935 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8301, precision-overall: 0.6291, recall-overall: 0.5495, f1-measure-overall: 0.5866, loss: 0.6728 ||: 100%|##########| 453/453 [00:30<00:00, 14.64it/s]\n",
            "2020-11-26 12:07:58,871 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 12:07:58,872 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 12:07:58,873 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.841  |     0.549\n",
            "2020-11-26 12:07:58,874 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.859  |     0.587\n",
            "2020-11-26 12:07:58,875 - INFO - allennlp.training.tensorboard_writer - loss               |     0.145  |     0.673\n",
            "2020-11-26 12:07:58,876 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.954  |     0.830\n",
            "2020-11-26 12:07:58,877 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 12:07:58,878 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.878  |     0.629\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 12:07:59,250 - INFO - allennlp.training.trainer - Epoch duration: 0:02:51.336192\n",
            "2020-11-26 12:07:59,251 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:19:39\n",
            "2020-11-26 12:07:59,251 - INFO - allennlp.training.trainer - Epoch 23/29\n",
            "2020-11-26 12:07:59,251 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 12:07:59,349 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 12:07:59,350 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9581, precision-overall: 0.8858, recall-overall: 0.8571, f1-measure-overall: 0.8712, loss: 0.1337 ||: 100%|##########| 1363/1363 [02:21<00:00,  9.60it/s]\n",
            "2020-11-26 12:10:21,269 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8332, precision-overall: 0.6371, recall-overall: 0.5480, f1-measure-overall: 0.5892, loss: 0.7002 ||: 100%|##########| 453/453 [00:28<00:00, 16.15it/s]\n",
            "2020-11-26 12:10:49,319 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-26 12:10:49,320 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  3973.888  |       N/A\n",
            "2020-11-26 12:10:49,321 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.857  |     0.548\n",
            "2020-11-26 12:10:49,322 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.871  |     0.589\n",
            "2020-11-26 12:10:49,323 - INFO - allennlp.training.tensorboard_writer - loss               |     0.134  |     0.700\n",
            "2020-11-26 12:10:49,324 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.958  |     0.833\n",
            "2020-11-26 12:10:49,325 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1236.000  |       N/A\n",
            "2020-11-26 12:10:49,325 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.886  |     0.637\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-26 12:10:49,677 - INFO - allennlp.training.trainer - Epoch duration: 0:02:50.425405\n",
            "2020-11-26 12:10:49,677 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:16:51\n",
            "2020-11-26 12:10:49,677 - INFO - allennlp.training.trainer - Epoch 24/29\n",
            "2020-11-26 12:10:49,678 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 3973.888\n",
            "2020-11-26 12:10:49,772 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1236\n",
            "2020-11-26 12:10:49,773 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1363 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/relex/modules/nn.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
            "  return inputs.masked_fill_(dropout_mask.byte(), self.fill_idx)\n",
            "accuracy: 0.9602, precision-overall: 0.8886, recall-overall: 0.8630, f1-measure-overall: 0.8756, loss: 0.1289 ||: 100%|##########| 1363/1363 [02:18<00:00,  9.83it/s]\n",
            "2020-11-26 12:13:08,405 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8297, precision-overall: 0.6219, recall-overall: 0.5636, f1-measure-overall: 0.5913, loss: 0.7097 ||: 100%|##########| 453/453 [00:29<00:00, 15.19it/s]\n",
            "2020-11-26 12:13:38,228 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.\n",
            "2020-11-26 12:13:38,233 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2020-11-26 12:13:38,364 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/model.tar.gz\n",
            "2020-11-26 12:13:42,354 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 14,\n",
            "  \"peak_cpu_memory_MB\": 3973.888,\n",
            "  \"peak_gpu_0_memory_MB\": 1236,\n",
            "  \"training_duration\": \"1:07:27.070759\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 23,\n",
            "  \"epoch\": 23,\n",
            "  \"training_accuracy\": 0.9581498444013857,\n",
            "  \"training_precision-overall\": 0.8857914383289651,\n",
            "  \"training_recall-overall\": 0.8571318782662158,\n",
            "  \"training_f1-measure-overall\": 0.8712260281997704,\n",
            "  \"training_loss\": 0.1336628996909911,\n",
            "  \"training_cpu_memory_MB\": 3973.888,\n",
            "  \"training_gpu_0_memory_MB\": 1236,\n",
            "  \"validation_accuracy\": 0.8332375944500906,\n",
            "  \"validation_precision-overall\": 0.6370829769033362,\n",
            "  \"validation_recall-overall\": 0.5480132450331126,\n",
            "  \"validation_f1-measure-overall\": 0.5892009493670389,\n",
            "  \"validation_loss\": 0.7001646369629875,\n",
            "  \"best_validation_accuracy\": 0.8437541425478327,\n",
            "  \"best_validation_precision-overall\": 0.6767519466073415,\n",
            "  \"best_validation_recall-overall\": 0.5596026490066225,\n",
            "  \"best_validation_f1-measure-overall\": 0.6126271271774755,\n",
            "  \"best_validation_loss\": 0.5366530279088231\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4OEx2RVzarB",
        "outputId": "9da63a0e-f2bc-46d7-9554-104d96d59515"
      },
      "source": [
        "# Train a baseline self-attention model for relation extraction (TACRED)  \n",
        "!allennlp train \\\n",
        "  ./configs/relation_classification/tacred/baseline_self_attention_tacred.jsonnet \\\n",
        "  -s '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred' \\\n",
        "  --include-package relex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-28 00:46:29,408 - INFO - pytorch_pretrained_bert.modeling - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-28 00:46:29,840 - INFO - pytorch_transformers.modeling_bert - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-28 00:46:29,843 - INFO - pytorch_transformers.modeling_xlnet - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "2020-11-28 00:46:30,219 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:46:30,220 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:46:30,221 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:46:30,221 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:46:30,351 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2020-11-28 00:46:30,352 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2020-11-28 00:46:30,352 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2020-11-28 00:46:30,362 - INFO - allennlp.common.checks - Pytorch version: 1.7.0+cu101\n",
            "2020-11-28 00:46:30,383 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2020-11-28 00:46:30,384 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2020-11-28 00:46:30,384 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'tacred'} and extras set()\n",
            "2020-11-28 00:46:30,384 - INFO - allennlp.common.params - dataset_reader.type = tacred\n",
            "2020-11-28 00:46:30,385 - INFO - allennlp.common.from_params - instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2020-11-28 00:46:30,385 - INFO - allennlp.common.params - dataset_reader.max_len = 100\n",
            "2020-11-28 00:46:30,385 - INFO - allennlp.common.params - dataset_reader.masking_mode = NER+Grammar\n",
            "2020-11-28 00:46:30,385 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2020-11-28 00:46:30,386 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2020-11-28 00:46:30,386 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-11-28 00:46:30,386 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
            "2020-11-28 00:46:30,386 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-11-28 00:46:30,387 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2020-11-28 00:46:30,387 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-11-28 00:46:30,387 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-11-28 00:46:30,387 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-11-28 00:46:30,387 - INFO - allennlp.common.params - dataset_reader.dep_pruning = 1\n",
            "2020-11-28 00:46:30,388 - INFO - allennlp.common.params - train_data_path = ../relex-data/tacred/train.json\n",
            "2020-11-28 00:46:30,388 - INFO - allennlp.training.util - Reading training data from ../relex-data/tacred/train.json\n",
            "0it [00:00, ?it/s]2020-11-28 00:46:30,659 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/train.json\n",
            "68124it [00:24, 2808.50it/s]\n",
            "2020-11-28 00:46:54,646 - INFO - allennlp.common.params - validation_data_path = ../relex-data/tacred/dev.json\n",
            "2020-11-28 00:46:54,646 - INFO - allennlp.training.util - Reading validation data from ../relex-data/tacred/dev.json\n",
            "0it [00:00, ?it/s]2020-11-28 00:46:54,648 - INFO - relex.dataset_readers.tacred - Reading TACRED instances from json dataset at: ../relex-data/tacred/dev.json\n",
            "22631it [00:07, 3088.08it/s]\n",
            "2020-11-28 00:47:01,975 - INFO - allennlp.common.params - test_data_path = None\n",
            "2020-11-28 00:47:02,124 - INFO - allennlp.training.trainer_pieces - From dataset instances, train, validation will be considered for vocabulary creation.\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.type = None\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.extend = False\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.directory_path = None\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.min_count = {'tokens': 2}\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n",
            "2020-11-28 00:47:02,125 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n",
            "2020-11-28 00:47:02,126 - INFO - allennlp.common.params - vocabulary.pretrained_files = {}\n",
            "2020-11-28 00:47:02,126 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n",
            "2020-11-28 00:47:02,126 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n",
            "2020-11-28 00:47:02,126 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n",
            "2020-11-28 00:47:02,126 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "90755it [00:02, 32804.67it/s]\n",
            "2020-11-28 00:47:05,000 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'type': 'basic_relation_classifier', 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-11-28 00:47:05,001 - INFO - allennlp.common.params - model.type = basic_relation_classifier\n",
            "2020-11-28 00:47:05,001 - INFO - allennlp.common.from_params - instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}}, 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-11-28 00:47:05,001 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\n",
            "2020-11-28 00:47:05,002 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2020-11-28 00:47:05,002 - INFO - allennlp.common.params - model.text_field_embedder.embedder_to_indexer_map = None\n",
            "2020-11-28 00:47:05,002 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = False\n",
            "2020-11-28 00:47:05,002 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders = None\n",
            "2020-11-28 00:47:05,002 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-11-28 00:47:05,003 - INFO - allennlp.common.params - model.text_field_embedder.tokens.type = embedding\n",
            "2020-11-28 00:47:05,003 - INFO - allennlp.common.params - model.text_field_embedder.tokens.num_embeddings = None\n",
            "2020-11-28 00:47:05,003 - INFO - allennlp.common.params - model.text_field_embedder.tokens.vocab_namespace = tokens\n",
            "2020-11-28 00:47:05,003 - INFO - allennlp.common.params - model.text_field_embedder.tokens.embedding_dim = 300\n",
            "2020-11-28 00:47:05,003 - INFO - allennlp.common.params - model.text_field_embedder.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.projection_dim = None\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.trainable = False\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.padding_index = None\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.max_norm = None\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.norm_type = 2.0\n",
            "2020-11-28 00:47:05,004 - INFO - allennlp.common.params - model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
            "2020-11-28 00:47:05,005 - INFO - allennlp.common.params - model.text_field_embedder.tokens.sparse = False\n",
            "2020-11-28 00:47:05,011 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n",
            "2020-11-28 00:47:05,117 - INFO - allennlp.common.file_utils - https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.txt.gz not found in cache, downloading to /tmp/tmp49xecuj3\n",
            "100%|##########| 2176768669/2176768669 [00:51<00:00, 42457130.52B/s]\n",
            "2020-11-28 00:47:56,613 - INFO - allennlp.common.file_utils - copying /tmp/tmp49xecuj3 to cache at /root/.allennlp/cache/0b88846fe4d9635db0e2cc8f37422337080361ccc153c21b68e874ec49ec0a59.8de7c14eda15038006ebf553b708bcae9c046b2ecf71dbbbedd8b0b9d0d0a621\n",
            "2020-11-28 00:48:25,780 - INFO - allennlp.common.file_utils - creating metadata file for /root/.allennlp/cache/0b88846fe4d9635db0e2cc8f37422337080361ccc153c21b68e874ec49ec0a59.8de7c14eda15038006ebf553b708bcae9c046b2ecf71dbbbedd8b0b9d0d0a621\n",
            "2020-11-28 00:48:25,781 - INFO - allennlp.common.file_utils - removing temp file /tmp/tmp49xecuj3\n",
            "2196017it [00:53, 41106.52it/s]\n",
            "2020-11-28 00:49:19,476 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n",
            "2020-11-28 00:49:20,043 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 34922 out of 41038 tokens\n",
            "2020-11-28 00:49:20,069 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,069 - INFO - allennlp.common.params - model.text_encoder.type = seq2seq_pool\n",
            "2020-11-28 00:49:20,069 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.seq2vec_encoders.seq2seq_pool_encoder.Seq2SeqPoolEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final'} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,070 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,070 - INFO - allennlp.common.params - model.text_encoder.encoder.type = stacked_self_attention\n",
            "2020-11-28 00:49:20,070 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 360, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.input_dim = 360\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.hidden_dim = 256\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.projection_dim = 256\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.feedforward_hidden_dim = 512\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.num_layers = 8\n",
            "2020-11-28 00:49:20,071 - INFO - allennlp.common.params - model.text_encoder.encoder.num_attention_heads = 8\n",
            "2020-11-28 00:49:20,072 - INFO - allennlp.common.params - model.text_encoder.encoder.use_positional_encoding = False\n",
            "2020-11-28 00:49:20,072 - INFO - allennlp.common.params - model.text_encoder.encoder.dropout_prob = 0.1\n",
            "2020-11-28 00:49:20,072 - INFO - allennlp.common.params - model.text_encoder.encoder.residual_dropout_prob = 0.2\n",
            "2020-11-28 00:49:20,072 - INFO - allennlp.common.params - model.text_encoder.encoder.attention_dropout_prob = 0.1\n",
            "2020-11-28 00:49:20,072 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,073 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,084 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,084 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,089 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,089 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,093 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,093 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,098 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,098 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,103 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,103 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,107 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,107 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,112 - INFO - allennlp.common.registrable - instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,112 - INFO - allennlp.common.registrable - instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-11-28 00:49:20,116 - INFO - allennlp.common.params - model.text_encoder.pooling = final\n",
            "2020-11-28 00:49:20,117 - INFO - allennlp.common.params - model.text_encoder.pooling_scope = None\n",
            "2020-11-28 00:49:20,117 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,117 - INFO - allennlp.common.params - model.classifier_feedforward.input_dim = 256\n",
            "2020-11-28 00:49:20,118 - INFO - allennlp.common.params - model.classifier_feedforward.num_layers = 1\n",
            "2020-11-28 00:49:20,119 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-11-28 00:49:20,119 - INFO - allennlp.common.params - model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-11-28 00:49:20,119 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['linear']\n",
            "2020-11-28 00:49:20,119 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-11-28 00:49:20,119 - INFO - allennlp.common.params - model.classifier_feedforward.activations = ['linear']\n",
            "2020-11-28 00:49:20,120 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-11-28 00:49:20,120 - INFO - allennlp.common.params - type = linear\n",
            "2020-11-28 00:49:20,120 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0]\n",
            "2020-11-28 00:49:20,120 - INFO - allennlp.common.params - model.classifier_feedforward.dropout = [0]\n",
            "2020-11-28 00:49:20,121 - INFO - allennlp.common.params - model.word_dropout = 0\n",
            "2020-11-28 00:49:20,121 - INFO - allennlp.common.params - model.embedding_dropout = 0.5\n",
            "2020-11-28 00:49:20,121 - INFO - allennlp.common.params - model.encoding_dropout = 0.5\n",
            "2020-11-28 00:49:20,122 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,122 - INFO - allennlp.common.params - model.offset_embedder_head.type = relative\n",
            "2020-11-28 00:49:20,122 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,122 - INFO - allennlp.common.params - model.offset_embedder_head.n_position = 100\n",
            "2020-11-28 00:49:20,123 - INFO - allennlp.common.params - model.offset_embedder_head.embedding_dim = 30\n",
            "2020-11-28 00:49:20,127 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,127 - INFO - allennlp.common.params - model.offset_embedder_tail.type = relative\n",
            "2020-11-28 00:49:20,127 - INFO - allennlp.common.from_params - instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-11-28 00:49:20,127 - INFO - allennlp.common.params - model.offset_embedder_tail.n_position = 100\n",
            "2020-11-28 00:49:20,128 - INFO - allennlp.common.params - model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-11-28 00:49:20,128 - INFO - allennlp.common.params - model.verbose_metrics = False\n",
            "2020-11-28 00:49:20,128 - INFO - allennlp.common.params - model.ignore_label = no_relation\n",
            "2020-11-28 00:49:20,129 - INFO - allennlp.common.params - model.f1_average = micro\n",
            "2020-11-28 00:49:20,129 - INFO - allennlp.common.params - model.use_adjacency = False\n",
            "2020-11-28 00:49:20,129 - INFO - allennlp.common.params - model.use_entity_offsets = False\n",
            "2020-11-28 00:49:20,129 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2020-11-28 00:49:20,130 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    offset_embedder_head._embedding.weight\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    offset_embedder_tail._embedding.weight\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_0._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_0._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_0._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,131 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_0._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_1._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_1._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_1._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_1._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_2._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,132 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_2._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_2._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_2._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_3._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_3._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_3._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_3._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_4._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,133 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_4._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_4._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_4._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_5._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_5._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_5._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_5._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_6._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,134 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_6._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_6._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_6._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_7._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_7._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_7._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_7._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_0.beta\n",
            "2020-11-28 00:49:20,135 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_0.gamma\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_1.beta\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_1.gamma\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_2.beta\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_2.gamma\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_3.beta\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_3.gamma\n",
            "2020-11-28 00:49:20,136 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_4.beta\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_4.gamma\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_5.beta\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_5.gamma\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_6.beta\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_6.gamma\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_7.beta\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.feedforward_layer_norm_7.gamma\n",
            "2020-11-28 00:49:20,137 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_0.beta\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_0.gamma\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_1.beta\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_1.gamma\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_2.beta\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_2.gamma\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_3.beta\n",
            "2020-11-28 00:49:20,138 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_3.gamma\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_4.beta\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_4.gamma\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_5.beta\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_5.gamma\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_6.beta\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_6.gamma\n",
            "2020-11-28 00:49:20,139 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_7.beta\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.layer_norm_7.gamma\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_0._combined_projection.bias\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_0._combined_projection.weight\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_0._output_projection.bias\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_0._output_projection.weight\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_1._combined_projection.bias\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_1._combined_projection.weight\n",
            "2020-11-28 00:49:20,140 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_1._output_projection.bias\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_1._output_projection.weight\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_2._combined_projection.bias\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_2._combined_projection.weight\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_2._output_projection.bias\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_2._output_projection.weight\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_3._combined_projection.bias\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_3._combined_projection.weight\n",
            "2020-11-28 00:49:20,141 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_3._output_projection.bias\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_3._output_projection.weight\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_4._combined_projection.bias\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_4._combined_projection.weight\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_4._output_projection.bias\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_4._output_projection.weight\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_5._combined_projection.bias\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_5._combined_projection.weight\n",
            "2020-11-28 00:49:20,142 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_5._output_projection.bias\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_5._output_projection.weight\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_6._combined_projection.bias\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_6._combined_projection.weight\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_6._output_projection.bias\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_6._output_projection.weight\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_7._combined_projection.bias\n",
            "2020-11-28 00:49:20,143 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_7._combined_projection.weight\n",
            "2020-11-28 00:49:20,144 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_7._output_projection.bias\n",
            "2020-11-28 00:49:20,144 - INFO - allennlp.nn.initializers -    text_encoder._encoder.self_attention_7._output_projection.weight\n",
            "2020-11-28 00:49:20,144 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-28 00:49:20,262 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']], 'type': 'bucket'} and extras set()\n",
            "2020-11-28 00:49:20,263 - INFO - allennlp.common.params - iterator.type = bucket\n",
            "2020-11-28 00:49:20,263 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 50, 'sorting_keys': [['text', 'num_tokens']]} and extras set()\n",
            "2020-11-28 00:49:20,264 - INFO - allennlp.common.params - iterator.sorting_keys = [['text', 'num_tokens']]\n",
            "2020-11-28 00:49:20,264 - INFO - allennlp.common.params - iterator.padding_noise = 0.1\n",
            "2020-11-28 00:49:20,264 - INFO - allennlp.common.params - iterator.biggest_batch_first = False\n",
            "2020-11-28 00:49:20,264 - INFO - allennlp.common.params - iterator.batch_size = 50\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.cache_instances = False\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.track_epoch = False\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\n",
            "2020-11-28 00:49:20,265 - INFO - allennlp.common.params - iterator.skip_smaller_batches = False\n",
            "2020-11-28 00:49:20,266 - INFO - allennlp.common.params - validation_iterator = None\n",
            "2020-11-28 00:49:20,266 - INFO - allennlp.common.params - trainer.no_grad = ()\n",
            "2020-11-28 00:49:20,267 - INFO - allennlp.training.trainer_pieces - Following parameters are Frozen  (without gradient):\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_field_embedder.token_embedder_tokens.weight\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - Following parameters are Tunable (with gradient):\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_0._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_0._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_0._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_0._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,268 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_0.gamma\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_0.beta\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_0._combined_projection.weight\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_0._combined_projection.bias\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_0._output_projection.weight\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_0._output_projection.bias\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_0.gamma\n",
            "2020-11-28 00:49:20,269 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_0.beta\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_1._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_1._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_1._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_1._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_1.gamma\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_1.beta\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_1._combined_projection.weight\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_1._combined_projection.bias\n",
            "2020-11-28 00:49:20,270 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_1._output_projection.weight\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_1._output_projection.bias\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_1.gamma\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_1.beta\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_2._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_2._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_2._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_2._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,271 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_2.gamma\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_2.beta\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_2._combined_projection.weight\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_2._combined_projection.bias\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_2._output_projection.weight\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_2._output_projection.bias\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_2.gamma\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_2.beta\n",
            "2020-11-28 00:49:20,272 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_3._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_3._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_3._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_3._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_3.gamma\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_3.beta\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_3._combined_projection.weight\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_3._combined_projection.bias\n",
            "2020-11-28 00:49:20,273 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_3._output_projection.weight\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_3._output_projection.bias\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_3.gamma\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_3.beta\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_4._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_4._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_4._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_4._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,274 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_4.gamma\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_4.beta\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_4._combined_projection.weight\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_4._combined_projection.bias\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_4._output_projection.weight\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_4._output_projection.bias\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_4.gamma\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_4.beta\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_5._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,275 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_5._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_5._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_5._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_5.gamma\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_5.beta\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_5._combined_projection.weight\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_5._combined_projection.bias\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_5._output_projection.weight\n",
            "2020-11-28 00:49:20,276 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_5._output_projection.bias\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_5.gamma\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_5.beta\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_6._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_6._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_6._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_6._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_6.gamma\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_6.beta\n",
            "2020-11-28 00:49:20,277 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_6._combined_projection.weight\n",
            "2020-11-28 00:49:20,278 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_6._combined_projection.bias\n",
            "2020-11-28 00:49:20,278 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_6._output_projection.weight\n",
            "2020-11-28 00:49:20,279 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_6._output_projection.bias\n",
            "2020-11-28 00:49:20,279 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_6.gamma\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_6.beta\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_7._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_7._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_7._linear_layers.1.weight\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_7._linear_layers.1.bias\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_7.gamma\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.feedforward_layer_norm_7.beta\n",
            "2020-11-28 00:49:20,280 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_7._combined_projection.weight\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_7._combined_projection.bias\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_7._output_projection.weight\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.self_attention_7._output_projection.bias\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_7.gamma\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - text_encoder._encoder.layer_norm_7.beta\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.weight\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - classifier_feedforward._linear_layers.0.bias\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - offset_embedder_head._embedding.weight\n",
            "2020-11-28 00:49:20,281 - INFO - allennlp.training.trainer_pieces - offset_embedder_tail._embedding.weight\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.validation_metric = +f1-measure-overall\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.shuffle = True\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.num_epochs = 100\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.cuda_device = 0\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.grad_norm = None\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.grad_clipping = 5\n",
            "2020-11-28 00:49:20,282 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2020-11-28 00:49:30,716 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
            "2020-11-28 00:49:30,717 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2020-11-28 00:49:30,717 - INFO - allennlp.training.optimizers - Number of trainable parameters: 4292934\n",
            "2020-11-28 00:49:30,718 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\n",
            "2020-11-28 00:49:30,719 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-28 00:49:30,719 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-28 00:49:30,719 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.0001\n",
            "2020-11-28 00:49:30,719 - INFO - allennlp.common.registrable - instantiating registered subclass adam of <class 'allennlp.training.optimizers.Optimizer'>\n",
            "2020-11-28 00:49:30,720 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
            "2020-11-28 00:49:30,720 - INFO - allennlp.common.registrable - instantiating registered subclass reduce_on_plateau of <class 'allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler'>\n",
            "2020-11-28 00:49:30,720 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-11-28 00:49:30,720 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \n",
            "2020-11-28 00:49:30,720 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.factor = 0.9\n",
            "2020-11-28 00:49:30,721 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.mode = max\n",
            "2020-11-28 00:49:30,721 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.patience = 1\n",
            "2020-11-28 00:49:30,721 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1\n",
            "2020-11-28 00:49:30,721 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\n",
            "2020-11-28 00:49:30,721 - INFO - allennlp.common.params - trainer.model_save_interval = None\n",
            "2020-11-28 00:49:30,722 - INFO - allennlp.common.params - trainer.summary_interval = 100\n",
            "2020-11-28 00:49:30,722 - INFO - allennlp.common.params - trainer.histogram_interval = None\n",
            "2020-11-28 00:49:30,722 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\n",
            "2020-11-28 00:49:30,722 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False\n",
            "2020-11-28 00:49:30,722 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\n",
            "2020-11-28 00:49:30,755 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2020-11-28 00:49:30,756 - INFO - allennlp.training.trainer - Epoch 0/99\n",
            "2020-11-28 00:49:30,756 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5404.208\n",
            "2020-11-28 00:49:30,901 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1015\n",
            "2020-11-28 00:49:30,903 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8082, precision-overall: 0.4687, recall-overall: 0.0868, f1-measure-overall: 0.1464, loss: 0.9000 ||: 100%|##########| 1363/1363 [02:04<00:00, 10.95it/s]\n",
            "2020-11-28 00:51:35,392 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7479, precision-overall: 0.4382, recall-overall: 0.2995, f1-measure-overall: 0.3558, loss: 0.8335 ||: 100%|##########| 453/453 [00:24<00:00, 18.73it/s]\n",
            "2020-11-28 00:51:59,577 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 00:51:59,578 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.087  |     0.299\n",
            "2020-11-28 00:51:59,579 - INFO - allennlp.training.tensorboard_writer - loss               |     0.900  |     0.833\n",
            "2020-11-28 00:51:59,580 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.808  |     0.748\n",
            "2020-11-28 00:51:59,580 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.146  |     0.356\n",
            "2020-11-28 00:51:59,581 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5404.208  |       N/A\n",
            "2020-11-28 00:51:59,582 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.469  |     0.438\n",
            "2020-11-28 00:51:59,589 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  1015.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 00:52:00,169 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 00:52:00,493 - INFO - allennlp.training.trainer - Epoch duration: 0:02:29.737105\n",
            "2020-11-28 00:52:00,494 - INFO - allennlp.training.trainer - Estimated training time remaining: 4:07:04\n",
            "2020-11-28 00:52:00,494 - INFO - allennlp.training.trainer - Epoch 1/99\n",
            "2020-11-28 00:52:00,495 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5634.608\n",
            "2020-11-28 00:52:00,611 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2415\n",
            "2020-11-28 00:52:00,613 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8178, precision-overall: 0.5235, recall-overall: 0.2350, f1-measure-overall: 0.3244, loss: 0.5843 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.31it/s]\n",
            "2020-11-28 00:54:01,083 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7894, precision-overall: 0.5689, recall-overall: 0.3295, f1-measure-overall: 0.4173, loss: 0.6244 ||: 100%|##########| 453/453 [00:23<00:00, 19.08it/s]\n",
            "2020-11-28 00:54:24,831 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 00:54:24,832 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.235  |     0.329\n",
            "2020-11-28 00:54:24,833 - INFO - allennlp.training.tensorboard_writer - loss               |     0.584  |     0.624\n",
            "2020-11-28 00:54:24,834 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.818  |     0.789\n",
            "2020-11-28 00:54:24,834 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.324  |     0.417\n",
            "2020-11-28 00:54:24,835 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5634.608  |       N/A\n",
            "2020-11-28 00:54:24,836 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.524  |     0.569\n",
            "2020-11-28 00:54:24,837 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2415.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 00:54:25,382 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 00:54:25,788 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.293375\n",
            "2020-11-28 00:54:25,788 - INFO - allennlp.training.trainer - Estimated training time remaining: 4:00:56\n",
            "2020-11-28 00:54:25,788 - INFO - allennlp.training.trainer - Epoch 2/99\n",
            "2020-11-28 00:54:25,789 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.436\n",
            "2020-11-28 00:54:25,914 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 00:54:25,916 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8368, precision-overall: 0.5982, recall-overall: 0.3389, f1-measure-overall: 0.4327, loss: 0.5046 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.28it/s]\n",
            "2020-11-28 00:56:26,711 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8127, precision-overall: 0.6178, recall-overall: 0.3898, f1-measure-overall: 0.4780, loss: 0.5739 ||: 100%|##########| 453/453 [00:23<00:00, 19.38it/s]\n",
            "2020-11-28 00:56:50,089 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 00:56:50,090 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.339  |     0.390\n",
            "2020-11-28 00:56:50,091 - INFO - allennlp.training.tensorboard_writer - loss               |     0.505  |     0.574\n",
            "2020-11-28 00:56:50,091 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.837  |     0.813\n",
            "2020-11-28 00:56:50,091 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.433  |     0.478\n",
            "2020-11-28 00:56:50,092 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.436  |       N/A\n",
            "2020-11-28 00:56:50,093 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.598  |     0.618\n",
            "2020-11-28 00:56:50,094 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 00:56:50,621 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 00:56:50,967 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.178017\n",
            "2020-11-28 00:56:50,967 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:57:13\n",
            "2020-11-28 00:56:50,967 - INFO - allennlp.training.trainer - Epoch 3/99\n",
            "2020-11-28 00:56:50,967 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.472\n",
            "2020-11-28 00:56:51,085 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 00:56:51,087 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8461, precision-overall: 0.6212, recall-overall: 0.3913, f1-measure-overall: 0.4801, loss: 0.4683 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.44it/s]\n",
            "2020-11-28 00:58:50,217 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.7995, precision-overall: 0.5177, recall-overall: 0.5510, f1-measure-overall: 0.5338, loss: 0.5638 ||: 100%|##########| 453/453 [00:23<00:00, 19.35it/s]\n",
            "2020-11-28 00:59:13,630 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 00:59:13,631 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.391  |     0.551\n",
            "2020-11-28 00:59:13,632 - INFO - allennlp.training.tensorboard_writer - loss               |     0.468  |     0.564\n",
            "2020-11-28 00:59:13,633 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.846  |     0.800\n",
            "2020-11-28 00:59:13,634 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.480  |     0.534\n",
            "2020-11-28 00:59:13,634 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.472  |       N/A\n",
            "2020-11-28 00:59:13,635 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.621  |     0.518\n",
            "2020-11-28 00:59:13,636 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 00:59:14,109 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 00:59:14,494 - INFO - allennlp.training.trainer - Epoch duration: 0:02:23.527371\n",
            "2020-11-28 00:59:14,495 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:53:29\n",
            "2020-11-28 00:59:14,495 - INFO - allennlp.training.trainer - Epoch 4/99\n",
            "2020-11-28 00:59:14,495 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.472\n",
            "2020-11-28 00:59:15,800 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 00:59:15,802 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8518, precision-overall: 0.6380, recall-overall: 0.4159, f1-measure-overall: 0.5036, loss: 0.4483 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.33it/s]\n",
            "2020-11-28 01:01:16,074 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8210, precision-overall: 0.5918, recall-overall: 0.5493, f1-measure-overall: 0.5697, loss: 0.5266 ||: 100%|##########| 453/453 [00:23<00:00, 19.23it/s]\n",
            "2020-11-28 01:01:39,636 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:01:39,637 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.416  |     0.549\n",
            "2020-11-28 01:01:39,638 - INFO - allennlp.training.tensorboard_writer - loss               |     0.448  |     0.527\n",
            "2020-11-28 01:01:39,638 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.852  |     0.821\n",
            "2020-11-28 01:01:39,639 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.504  |     0.570\n",
            "2020-11-28 01:01:39,640 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.472  |       N/A\n",
            "2020-11-28 01:01:39,641 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.638  |     0.592\n",
            "2020-11-28 01:01:39,641 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:01:40,116 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:01:40,490 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.994928\n",
            "2020-11-28 01:01:40,491 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:51:04\n",
            "2020-11-28 01:01:40,491 - INFO - allennlp.training.trainer - Epoch 5/99\n",
            "2020-11-28 01:01:40,491 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.472\n",
            "2020-11-28 01:01:40,615 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:01:40,617 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8571, precision-overall: 0.6551, recall-overall: 0.4411, f1-measure-overall: 0.5272, loss: 0.4314 ||: 100%|##########| 1363/1363 [02:02<00:00, 11.09it/s]\n",
            "2020-11-28 01:03:43,559 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8244, precision-overall: 0.5995, recall-overall: 0.5695, f1-measure-overall: 0.5842, loss: 0.5144 ||: 100%|##########| 453/453 [00:23<00:00, 19.22it/s]\n",
            "2020-11-28 01:04:07,133 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:04:07,133 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.441  |     0.570\n",
            "2020-11-28 01:04:07,134 - INFO - allennlp.training.tensorboard_writer - loss               |     0.431  |     0.514\n",
            "2020-11-28 01:04:07,135 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.857  |     0.824\n",
            "2020-11-28 01:04:07,136 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.527  |     0.584\n",
            "2020-11-28 01:04:07,137 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.472  |       N/A\n",
            "2020-11-28 01:04:07,137 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.655  |     0.600\n",
            "2020-11-28 01:04:07,138 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:04:07,639 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:04:07,982 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.491247\n",
            "2020-11-28 01:04:07,983 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:49:03\n",
            "2020-11-28 01:04:07,983 - INFO - allennlp.training.trainer - Epoch 6/99\n",
            "2020-11-28 01:04:07,983 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.496\n",
            "2020-11-28 01:04:08,103 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:04:08,105 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8615, precision-overall: 0.6735, recall-overall: 0.4597, f1-measure-overall: 0.5464, loss: 0.4169 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.26it/s]\n",
            "2020-11-28 01:06:09,163 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8183, precision-overall: 0.5696, recall-overall: 0.6383, f1-measure-overall: 0.6020, loss: 0.5106 ||: 100%|##########| 453/453 [00:23<00:00, 19.28it/s]\n",
            "2020-11-28 01:06:32,664 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:06:32,665 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.460  |     0.638\n",
            "2020-11-28 01:06:32,666 - INFO - allennlp.training.tensorboard_writer - loss               |     0.417  |     0.511\n",
            "2020-11-28 01:06:32,667 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.862  |     0.818\n",
            "2020-11-28 01:06:32,667 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.546  |     0.602\n",
            "2020-11-28 01:06:32,668 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.496  |       N/A\n",
            "2020-11-28 01:06:32,669 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.674  |     0.570\n",
            "2020-11-28 01:06:32,670 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:06:33,157 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:06:33,518 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.534944\n",
            "2020-11-28 01:06:33,518 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:46:28\n",
            "2020-11-28 01:06:33,518 - INFO - allennlp.training.trainer - Epoch 7/99\n",
            "2020-11-28 01:06:33,518 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.496\n",
            "2020-11-28 01:06:33,635 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:06:33,637 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8648, precision-overall: 0.6826, recall-overall: 0.4789, f1-measure-overall: 0.5629, loss: 0.4067 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.31it/s]\n",
            "2020-11-28 01:08:34,169 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8274, precision-overall: 0.6011, recall-overall: 0.5993, f1-measure-overall: 0.6002, loss: 0.4964 ||: 100%|##########| 453/453 [00:23<00:00, 19.36it/s]\n",
            "2020-11-28 01:08:57,567 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:08:57,568 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.479  |     0.599\n",
            "2020-11-28 01:08:57,569 - INFO - allennlp.training.tensorboard_writer - loss               |     0.407  |     0.496\n",
            "2020-11-28 01:08:57,570 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.865  |     0.827\n",
            "2020-11-28 01:08:57,571 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.563  |     0.600\n",
            "2020-11-28 01:08:57,571 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.496  |       N/A\n",
            "2020-11-28 01:08:57,572 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.683  |     0.601\n",
            "2020-11-28 01:08:57,573 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:08:58,070 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.551866\n",
            "2020-11-28 01:08:58,071 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:43:44\n",
            "2020-11-28 01:08:58,071 - INFO - allennlp.training.trainer - Epoch 8/99\n",
            "2020-11-28 01:08:58,071 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.508\n",
            "2020-11-28 01:08:58,193 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:08:58,195 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8683, precision-overall: 0.6945, recall-overall: 0.4919, f1-measure-overall: 0.5759, loss: 0.3971 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.40it/s]\n",
            "2020-11-28 01:10:57,752 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8357, precision-overall: 0.6367, recall-overall: 0.5920, f1-measure-overall: 0.6135, loss: 0.4691 ||: 100%|##########| 453/453 [00:23<00:00, 19.13it/s]\n",
            "2020-11-28 01:11:21,433 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:11:21,434 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.492  |     0.592\n",
            "2020-11-28 01:11:21,435 - INFO - allennlp.training.tensorboard_writer - loss               |     0.397  |     0.469\n",
            "2020-11-28 01:11:21,435 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.868  |     0.836\n",
            "2020-11-28 01:11:21,436 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.576  |     0.614\n",
            "2020-11-28 01:11:21,437 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.508  |       N/A\n",
            "2020-11-28 01:11:21,438 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.694  |     0.637\n",
            "2020-11-28 01:11:21,439 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:11:21,920 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:11:22,284 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.212669\n",
            "2020-11-28 01:11:22,284 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:41:01\n",
            "2020-11-28 01:11:22,284 - INFO - allennlp.training.trainer - Epoch 9/99\n",
            "2020-11-28 01:11:22,284 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.508\n",
            "2020-11-28 01:11:22,396 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:11:22,398 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8705, precision-overall: 0.7006, recall-overall: 0.5035, f1-measure-overall: 0.5859, loss: 0.3897 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.34it/s]\n",
            "2020-11-28 01:13:22,625 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8282, precision-overall: 0.5945, recall-overall: 0.6409, f1-measure-overall: 0.6169, loss: 0.4930 ||: 100%|##########| 453/453 [00:23<00:00, 19.66it/s]\n",
            "2020-11-28 01:13:45,670 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:13:45,670 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.503  |     0.641\n",
            "2020-11-28 01:13:45,671 - INFO - allennlp.training.tensorboard_writer - loss               |     0.390  |     0.493\n",
            "2020-11-28 01:13:45,672 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.871  |     0.828\n",
            "2020-11-28 01:13:45,673 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.586  |     0.617\n",
            "2020-11-28 01:13:45,673 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.508  |       N/A\n",
            "2020-11-28 01:13:45,674 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.701  |     0.595\n",
            "2020-11-28 01:13:45,675 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:13:46,160 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:13:46,517 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.232988\n",
            "2020-11-28 01:13:46,517 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:38:21\n",
            "2020-11-28 01:13:46,518 - INFO - allennlp.training.trainer - Epoch 10/99\n",
            "2020-11-28 01:13:46,518 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.508\n",
            "2020-11-28 01:13:46,635 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:13:46,637 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8737, precision-overall: 0.7122, recall-overall: 0.5136, f1-measure-overall: 0.5968, loss: 0.3818 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.37it/s]\n",
            "2020-11-28 01:15:46,481 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8243, precision-overall: 0.5828, recall-overall: 0.6646, f1-measure-overall: 0.6211, loss: 0.5081 ||: 100%|##########| 453/453 [00:23<00:00, 19.26it/s]\n",
            "2020-11-28 01:16:10,008 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:16:10,009 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.514  |     0.665\n",
            "2020-11-28 01:16:10,010 - INFO - allennlp.training.tensorboard_writer - loss               |     0.382  |     0.508\n",
            "2020-11-28 01:16:10,011 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.874  |     0.824\n",
            "2020-11-28 01:16:10,012 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.597  |     0.621\n",
            "2020-11-28 01:16:10,013 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.508  |       N/A\n",
            "2020-11-28 01:16:10,014 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.712  |     0.583\n",
            "2020-11-28 01:16:10,015 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:16:10,492 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:16:10,851 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.333826\n",
            "2020-11-28 01:16:10,852 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:35:46\n",
            "2020-11-28 01:16:10,852 - INFO - allennlp.training.trainer - Epoch 11/99\n",
            "2020-11-28 01:16:10,852 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.508\n",
            "2020-11-28 01:16:10,986 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:16:10,988 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8743, precision-overall: 0.7090, recall-overall: 0.5238, f1-measure-overall: 0.6025, loss: 0.3758 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.40it/s]\n",
            "2020-11-28 01:18:10,551 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8353, precision-overall: 0.6189, recall-overall: 0.6628, f1-measure-overall: 0.6401, loss: 0.4740 ||: 100%|##########| 453/453 [00:23<00:00, 19.07it/s]\n",
            "2020-11-28 01:18:34,315 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:18:34,316 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.524  |     0.663\n",
            "2020-11-28 01:18:34,317 - INFO - allennlp.training.tensorboard_writer - loss               |     0.376  |     0.474\n",
            "2020-11-28 01:18:34,318 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.874  |     0.835\n",
            "2020-11-28 01:18:34,318 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.602  |     0.640\n",
            "2020-11-28 01:18:34,319 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.508  |       N/A\n",
            "2020-11-28 01:18:34,320 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.709  |     0.619\n",
            "2020-11-28 01:18:34,321 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:18:34,832 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:18:35,161 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.309225\n",
            "2020-11-28 01:18:35,162 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:33:12\n",
            "2020-11-28 01:18:35,162 - INFO - allennlp.training.trainer - Epoch 12/99\n",
            "2020-11-28 01:18:35,162 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.508\n",
            "2020-11-28 01:18:35,282 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:18:35,284 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8765, precision-overall: 0.7157, recall-overall: 0.5342, f1-measure-overall: 0.6118, loss: 0.3697 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.28it/s]\n",
            "2020-11-28 01:20:36,165 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8457, precision-overall: 0.6712, recall-overall: 0.5936, f1-measure-overall: 0.6300, loss: 0.4439 ||: 100%|##########| 453/453 [00:25<00:00, 17.59it/s]\n",
            "2020-11-28 01:21:01,917 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:21:01,917 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.534  |     0.594\n",
            "2020-11-28 01:21:01,918 - INFO - allennlp.training.tensorboard_writer - loss               |     0.370  |     0.444\n",
            "2020-11-28 01:21:01,919 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.876  |     0.846\n",
            "2020-11-28 01:21:01,920 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.612  |     0.630\n",
            "2020-11-28 01:21:01,920 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.508  |       N/A\n",
            "2020-11-28 01:21:01,921 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.716  |     0.671\n",
            "2020-11-28 01:21:01,922 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:21:02,442 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.280295\n",
            "2020-11-28 01:21:02,443 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:30:59\n",
            "2020-11-28 01:21:02,443 - INFO - allennlp.training.trainer - Epoch 13/99\n",
            "2020-11-28 01:21:02,443 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:21:02,564 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:21:02,566 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8776, precision-overall: 0.7169, recall-overall: 0.5396, f1-measure-overall: 0.6157, loss: 0.3639 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.27it/s]\n",
            "2020-11-28 01:23:03,552 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8484, precision-overall: 0.6755, recall-overall: 0.6067, f1-measure-overall: 0.6393, loss: 0.4395 ||: 100%|##########| 453/453 [00:23<00:00, 19.09it/s]\n",
            "2020-11-28 01:23:27,282 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:23:27,283 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.540  |     0.607\n",
            "2020-11-28 01:23:27,284 - INFO - allennlp.training.tensorboard_writer - loss               |     0.364  |     0.439\n",
            "2020-11-28 01:23:27,285 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.878  |     0.848\n",
            "2020-11-28 01:23:27,286 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.616  |     0.639\n",
            "2020-11-28 01:23:27,286 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:23:27,287 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.717  |     0.676\n",
            "2020-11-28 01:23:27,288 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:23:27,807 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.364100\n",
            "2020-11-28 01:23:27,809 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:28:33\n",
            "2020-11-28 01:23:27,809 - INFO - allennlp.training.trainer - Epoch 14/99\n",
            "2020-11-28 01:23:27,809 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:23:27,935 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:23:27,937 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8806, precision-overall: 0.7236, recall-overall: 0.5523, f1-measure-overall: 0.6265, loss: 0.3570 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.42it/s]\n",
            "2020-11-28 01:25:27,310 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8389, precision-overall: 0.6322, recall-overall: 0.6475, f1-measure-overall: 0.6398, loss: 0.4560 ||: 100%|##########| 453/453 [00:23<00:00, 18.91it/s]\n",
            "2020-11-28 01:25:51,268 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:25:51,269 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.552  |     0.648\n",
            "2020-11-28 01:25:51,270 - INFO - allennlp.training.tensorboard_writer - loss               |     0.357  |     0.456\n",
            "2020-11-28 01:25:51,271 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.881  |     0.839\n",
            "2020-11-28 01:25:51,272 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.626  |     0.640\n",
            "2020-11-28 01:25:51,272 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:25:51,273 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.724  |     0.632\n",
            "2020-11-28 01:25:51,274 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:25:52,121 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.311675\n",
            "2020-11-28 01:25:52,121 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:26:01\n",
            "2020-11-28 01:25:52,121 - INFO - allennlp.training.trainer - Epoch 15/99\n",
            "2020-11-28 01:25:52,122 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:25:52,239 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:25:52,241 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8819, precision-overall: 0.7258, recall-overall: 0.5587, f1-measure-overall: 0.6314, loss: 0.3517 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.32it/s]\n",
            "2020-11-28 01:27:52,689 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8441, precision-overall: 0.6545, recall-overall: 0.6047, f1-measure-overall: 0.6286, loss: 0.4512 ||: 100%|##########| 453/453 [00:23<00:00, 19.07it/s]\n",
            "2020-11-28 01:28:16,450 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:28:16,450 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.559  |     0.605\n",
            "2020-11-28 01:28:16,452 - INFO - allennlp.training.tensorboard_writer - loss               |     0.352  |     0.451\n",
            "2020-11-28 01:28:16,452 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.882  |     0.844\n",
            "2020-11-28 01:28:16,453 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.631  |     0.629\n",
            "2020-11-28 01:28:16,454 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:28:16,455 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.726  |     0.655\n",
            "2020-11-28 01:28:16,456 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:28:16,956 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.834087\n",
            "2020-11-28 01:28:16,956 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:23:32\n",
            "2020-11-28 01:28:16,956 - INFO - allennlp.training.trainer - Epoch 16/99\n",
            "2020-11-28 01:28:16,956 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:28:17,071 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:28:17,073 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8844, precision-overall: 0.7341, recall-overall: 0.5667, f1-measure-overall: 0.6396, loss: 0.3466 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.33it/s]\n",
            "2020-11-28 01:30:17,325 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8329, precision-overall: 0.6113, recall-overall: 0.6529, f1-measure-overall: 0.6314, loss: 0.4728 ||: 100%|##########| 453/453 [00:23<00:00, 19.26it/s]\n",
            "2020-11-28 01:30:40,856 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:30:40,857 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.567  |     0.653\n",
            "2020-11-28 01:30:40,857 - INFO - allennlp.training.tensorboard_writer - loss               |     0.347  |     0.473\n",
            "2020-11-28 01:30:40,858 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.884  |     0.833\n",
            "2020-11-28 01:30:40,858 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.640  |     0.631\n",
            "2020-11-28 01:30:40,859 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:30:40,859 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.734  |     0.611\n",
            "2020-11-28 01:30:40,859 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:30:41,341 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.384379\n",
            "2020-11-28 01:30:41,341 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:21:02\n",
            "2020-11-28 01:30:41,341 - INFO - allennlp.training.trainer - Epoch 17/99\n",
            "2020-11-28 01:30:41,342 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:30:41,456 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:30:41,458 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8849, precision-overall: 0.7317, recall-overall: 0.5757, f1-measure-overall: 0.6444, loss: 0.3408 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.26it/s]\n",
            "2020-11-28 01:32:42,464 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8339, precision-overall: 0.6124, recall-overall: 0.6637, f1-measure-overall: 0.6370, loss: 0.4585 ||: 100%|##########| 453/453 [00:23<00:00, 18.95it/s]\n",
            "2020-11-28 01:33:06,371 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:33:06,371 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.576  |     0.664\n",
            "2020-11-28 01:33:06,372 - INFO - allennlp.training.tensorboard_writer - loss               |     0.341  |     0.459\n",
            "2020-11-28 01:33:06,373 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.885  |     0.834\n",
            "2020-11-28 01:33:06,374 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.644  |     0.637\n",
            "2020-11-28 01:33:06,375 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:33:06,375 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.732  |     0.612\n",
            "2020-11-28 01:33:06,376 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:33:07,245 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.903339\n",
            "2020-11-28 01:33:07,245 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:18:39\n",
            "2020-11-28 01:33:07,245 - INFO - allennlp.training.trainer - Epoch 18/99\n",
            "2020-11-28 01:33:07,246 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:33:07,361 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:33:07,363 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8872, precision-overall: 0.7418, recall-overall: 0.5797, f1-measure-overall: 0.6508, loss: 0.3351 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.33it/s]\n",
            "2020-11-28 01:35:07,701 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8455, precision-overall: 0.6541, recall-overall: 0.6310, f1-measure-overall: 0.6423, loss: 0.4436 ||: 100%|##########| 453/453 [00:23<00:00, 19.36it/s]\n",
            "2020-11-28 01:35:31,109 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:35:31,110 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.580  |     0.631\n",
            "2020-11-28 01:35:31,111 - INFO - allennlp.training.tensorboard_writer - loss               |     0.335  |     0.444\n",
            "2020-11-28 01:35:31,112 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.887  |     0.845\n",
            "2020-11-28 01:35:31,113 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.651  |     0.642\n",
            "2020-11-28 01:35:31,113 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:35:31,114 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.742  |     0.654\n",
            "2020-11-28 01:35:31,114 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:35:31,582 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:35:31,944 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.698790\n",
            "2020-11-28 01:35:31,945 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:16:11\n",
            "2020-11-28 01:35:31,945 - INFO - allennlp.training.trainer - Epoch 19/99\n",
            "2020-11-28 01:35:31,945 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:35:32,070 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:35:32,072 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8882, precision-overall: 0.7408, recall-overall: 0.5837, f1-measure-overall: 0.6529, loss: 0.3322 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.22it/s]\n",
            "2020-11-28 01:37:33,521 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8548, precision-overall: 0.7193, recall-overall: 0.5736, f1-measure-overall: 0.6382, loss: 0.4293 ||: 100%|##########| 453/453 [00:23<00:00, 18.99it/s]\n",
            "2020-11-28 01:37:57,374 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:37:57,374 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.584  |     0.574\n",
            "2020-11-28 01:37:57,375 - INFO - allennlp.training.tensorboard_writer - loss               |     0.332  |     0.429\n",
            "2020-11-28 01:37:57,376 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.888  |     0.855\n",
            "2020-11-28 01:37:57,377 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.653  |     0.638\n",
            "2020-11-28 01:37:57,378 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:37:57,379 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.741  |     0.719\n",
            "2020-11-28 01:37:57,379 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:37:57,884 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.938757\n",
            "2020-11-28 01:37:57,884 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:13:48\n",
            "2020-11-28 01:37:57,884 - INFO - allennlp.training.trainer - Epoch 20/99\n",
            "2020-11-28 01:37:57,884 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:37:58,000 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:37:58,002 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8884, precision-overall: 0.7402, recall-overall: 0.5901, f1-measure-overall: 0.6567, loss: 0.3282 ||: 100%|##########| 1363/1363 [02:03<00:00, 11.07it/s]\n",
            "2020-11-28 01:40:01,146 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8389, precision-overall: 0.6233, recall-overall: 0.6536, f1-measure-overall: 0.6381, loss: 0.4490 ||: 100%|##########| 453/453 [00:23<00:00, 19.45it/s]\n",
            "2020-11-28 01:40:24,445 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:40:24,446 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.590  |     0.654\n",
            "2020-11-28 01:40:24,447 - INFO - allennlp.training.tensorboard_writer - loss               |     0.328  |     0.449\n",
            "2020-11-28 01:40:24,448 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.888  |     0.839\n",
            "2020-11-28 01:40:24,449 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.657  |     0.638\n",
            "2020-11-28 01:40:24,449 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:40:24,450 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.740  |     0.623\n",
            "2020-11-28 01:40:24,451 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:40:25,280 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.395507\n",
            "2020-11-28 01:40:25,280 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:11:30\n",
            "2020-11-28 01:40:25,280 - INFO - allennlp.training.trainer - Epoch 21/99\n",
            "2020-11-28 01:40:25,280 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.512\n",
            "2020-11-28 01:40:25,393 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:40:25,395 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8904, precision-overall: 0.7468, recall-overall: 0.5941, f1-measure-overall: 0.6617, loss: 0.3245 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.41it/s]\n",
            "2020-11-28 01:42:24,805 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8423, precision-overall: 0.6315, recall-overall: 0.6645, f1-measure-overall: 0.6475, loss: 0.4503 ||: 100%|##########| 453/453 [00:23<00:00, 19.28it/s]\n",
            "2020-11-28 01:42:48,305 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:42:48,306 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.594  |     0.664\n",
            "2020-11-28 01:42:48,307 - INFO - allennlp.training.tensorboard_writer - loss               |     0.325  |     0.450\n",
            "2020-11-28 01:42:48,308 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.890  |     0.842\n",
            "2020-11-28 01:42:48,309 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.662  |     0.648\n",
            "2020-11-28 01:42:48,309 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.512  |       N/A\n",
            "2020-11-28 01:42:48,310 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.747  |     0.631\n",
            "2020-11-28 01:42:48,311 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:42:48,796 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:42:49,150 - INFO - allennlp.training.trainer - Epoch duration: 0:02:23.869853\n",
            "2020-11-28 01:42:49,151 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:08:59\n",
            "2020-11-28 01:42:49,151 - INFO - allennlp.training.trainer - Epoch 22/99\n",
            "2020-11-28 01:42:49,151 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:42:49,275 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:42:49,277 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8928, precision-overall: 0.7560, recall-overall: 0.6018, f1-measure-overall: 0.6702, loss: 0.3178 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.43it/s]\n",
            "2020-11-28 01:44:48,500 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8403, precision-overall: 0.6263, recall-overall: 0.6696, f1-measure-overall: 0.6472, loss: 0.4461 ||: 100%|##########| 453/453 [00:23<00:00, 19.21it/s]\n",
            "2020-11-28 01:45:12,087 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:45:12,088 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.602  |     0.670\n",
            "2020-11-28 01:45:12,089 - INFO - allennlp.training.tensorboard_writer - loss               |     0.318  |     0.446\n",
            "2020-11-28 01:45:12,090 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.893  |     0.840\n",
            "2020-11-28 01:45:12,090 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.670  |     0.647\n",
            "2020-11-28 01:45:12,091 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:45:12,092 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.756  |     0.626\n",
            "2020-11-28 01:45:12,092 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:45:12,596 - INFO - allennlp.training.trainer - Epoch duration: 0:02:23.445127\n",
            "2020-11-28 01:45:12,597 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:06:27\n",
            "2020-11-28 01:45:12,597 - INFO - allennlp.training.trainer - Epoch 23/99\n",
            "2020-11-28 01:45:12,597 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:45:12,729 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:45:12,732 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8925, precision-overall: 0.7504, recall-overall: 0.6064, f1-measure-overall: 0.6707, loss: 0.3186 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.22it/s]\n",
            "2020-11-28 01:47:14,254 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8339, precision-overall: 0.6053, recall-overall: 0.6886, f1-measure-overall: 0.6442, loss: 0.4722 ||: 100%|##########| 453/453 [00:23<00:00, 19.05it/s]\n",
            "2020-11-28 01:47:38,042 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:47:38,043 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.606  |     0.689\n",
            "2020-11-28 01:47:38,044 - INFO - allennlp.training.tensorboard_writer - loss               |     0.319  |     0.472\n",
            "2020-11-28 01:47:38,044 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.892  |     0.834\n",
            "2020-11-28 01:47:38,045 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.671  |     0.644\n",
            "2020-11-28 01:47:38,046 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:47:38,047 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.750  |     0.605\n",
            "2020-11-28 01:47:38,048 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:47:38,557 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.960084\n",
            "2020-11-28 01:47:38,558 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:04:04\n",
            "2020-11-28 01:47:38,558 - INFO - allennlp.training.trainer - Epoch 24/99\n",
            "2020-11-28 01:47:38,558 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:47:38,673 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:47:38,674 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8940, precision-overall: 0.7558, recall-overall: 0.6106, f1-measure-overall: 0.6755, loss: 0.3118 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.20it/s]\n",
            "2020-11-28 01:49:40,346 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8359, precision-overall: 0.6098, recall-overall: 0.6714, f1-measure-overall: 0.6391, loss: 0.4704 ||: 100%|##########| 453/453 [00:24<00:00, 18.83it/s]\n",
            "2020-11-28 01:50:04,411 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:50:04,411 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.611  |     0.671\n",
            "2020-11-28 01:50:04,413 - INFO - allennlp.training.tensorboard_writer - loss               |     0.312  |     0.470\n",
            "2020-11-28 01:50:04,413 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.894  |     0.836\n",
            "2020-11-28 01:50:04,414 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.675  |     0.639\n",
            "2020-11-28 01:50:04,415 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:50:04,416 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.756  |     0.610\n",
            "2020-11-28 01:50:04,416 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:50:04,928 - INFO - allennlp.training.trainer - Epoch duration: 0:02:26.370003\n",
            "2020-11-28 01:50:04,928 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:01:42\n",
            "2020-11-28 01:50:04,928 - INFO - allennlp.training.trainer - Epoch 25/99\n",
            "2020-11-28 01:50:04,929 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:50:05,061 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:50:05,063 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8941, precision-overall: 0.7535, recall-overall: 0.6133, f1-measure-overall: 0.6762, loss: 0.3087 ||: 100%|##########| 1363/1363 [02:03<00:00, 11.08it/s]\n",
            "2020-11-28 01:52:08,067 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8437, precision-overall: 0.6385, recall-overall: 0.6518, f1-measure-overall: 0.6451, loss: 0.4444 ||: 100%|##########| 453/453 [00:23<00:00, 18.96it/s]\n",
            "2020-11-28 01:52:31,969 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:52:31,969 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.613  |     0.652\n",
            "2020-11-28 01:52:31,970 - INFO - allennlp.training.tensorboard_writer - loss               |     0.309  |     0.444\n",
            "2020-11-28 01:52:31,971 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.894  |     0.844\n",
            "2020-11-28 01:52:31,972 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.676  |     0.645\n",
            "2020-11-28 01:52:31,972 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:52:31,973 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.754  |     0.638\n",
            "2020-11-28 01:52:31,974 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:52:32,482 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.553972\n",
            "2020-11-28 01:52:32,483 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:59:23\n",
            "2020-11-28 01:52:32,483 - INFO - allennlp.training.trainer - Epoch 26/99\n",
            "2020-11-28 01:52:32,483 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:52:32,601 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:52:32,603 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8952, precision-overall: 0.7605, recall-overall: 0.6151, f1-measure-overall: 0.6801, loss: 0.3070 ||: 100%|##########| 1363/1363 [02:02<00:00, 11.14it/s]\n",
            "2020-11-28 01:54:34,922 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8432, precision-overall: 0.6339, recall-overall: 0.6628, f1-measure-overall: 0.6480, loss: 0.4434 ||: 100%|##########| 453/453 [00:23<00:00, 19.02it/s]\n",
            "2020-11-28 01:54:58,743 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:54:58,744 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.615  |     0.663\n",
            "2020-11-28 01:54:58,745 - INFO - allennlp.training.tensorboard_writer - loss               |     0.307  |     0.443\n",
            "2020-11-28 01:54:58,746 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.895  |     0.843\n",
            "2020-11-28 01:54:58,747 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.680  |     0.648\n",
            "2020-11-28 01:54:58,747 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:54:58,748 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.761  |     0.634\n",
            "2020-11-28 01:54:58,749 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:54:59,231 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:54:59,878 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.395324\n",
            "2020-11-28 01:54:59,879 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:57:03\n",
            "2020-11-28 01:54:59,879 - INFO - allennlp.training.trainer - Epoch 27/99\n",
            "2020-11-28 01:54:59,879 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:54:59,994 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:54:59,997 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8983, precision-overall: 0.7665, recall-overall: 0.6277, f1-measure-overall: 0.6902, loss: 0.2998 ||: 100%|##########| 1363/1363 [02:02<00:00, 11.12it/s]\n",
            "2020-11-28 01:57:02,519 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8442, precision-overall: 0.6381, recall-overall: 0.6626, f1-measure-overall: 0.6501, loss: 0.4442 ||: 100%|##########| 453/453 [00:23<00:00, 18.99it/s]\n",
            "2020-11-28 01:57:26,377 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:57:26,378 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.628  |     0.663\n",
            "2020-11-28 01:57:26,379 - INFO - allennlp.training.tensorboard_writer - loss               |     0.300  |     0.444\n",
            "2020-11-28 01:57:26,380 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.898  |     0.844\n",
            "2020-11-28 01:57:26,380 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.690  |     0.650\n",
            "2020-11-28 01:57:26,381 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:57:26,385 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.767  |     0.638\n",
            "2020-11-28 01:57:26,385 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:57:26,881 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:57:27,239 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.360281\n",
            "2020-11-28 01:57:27,240 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:54:42\n",
            "2020-11-28 01:57:27,241 - INFO - allennlp.training.trainer - Epoch 28/99\n",
            "2020-11-28 01:57:27,241 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:57:27,383 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:57:27,385 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8985, precision-overall: 0.7656, recall-overall: 0.6307, f1-measure-overall: 0.6916, loss: 0.2978 ||: 100%|##########| 1363/1363 [02:02<00:00, 11.08it/s]\n",
            "2020-11-28 01:59:30,387 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8435, precision-overall: 0.6392, recall-overall: 0.6648, f1-measure-overall: 0.6518, loss: 0.4438 ||: 100%|##########| 453/453 [00:23<00:00, 18.89it/s]\n",
            "2020-11-28 01:59:54,366 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 01:59:54,366 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.631  |     0.665\n",
            "2020-11-28 01:59:54,367 - INFO - allennlp.training.tensorboard_writer - loss               |     0.298  |     0.444\n",
            "2020-11-28 01:59:54,368 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.898  |     0.843\n",
            "2020-11-28 01:59:54,369 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.692  |     0.652\n",
            "2020-11-28 01:59:54,369 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 01:59:54,370 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.766  |     0.639\n",
            "2020-11-28 01:59:54,371 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 01:59:54,848 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/best.th'.\n",
            "2020-11-28 01:59:55,218 - INFO - allennlp.training.trainer - Epoch duration: 0:02:27.977822\n",
            "2020-11-28 01:59:55,219 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:52:22\n",
            "2020-11-28 01:59:55,219 - INFO - allennlp.training.trainer - Epoch 29/99\n",
            "2020-11-28 01:59:55,219 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 01:59:55,345 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 01:59:55,346 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8981, precision-overall: 0.7641, recall-overall: 0.6297, f1-measure-overall: 0.6904, loss: 0.2967 ||: 100%|##########| 1363/1363 [02:01<00:00, 11.19it/s]\n",
            "2020-11-28 02:01:57,156 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8432, precision-overall: 0.6451, recall-overall: 0.6391, f1-measure-overall: 0.6421, loss: 0.4547 ||: 100%|##########| 453/453 [00:23<00:00, 19.12it/s]\n",
            "2020-11-28 02:02:20,852 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:02:20,853 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.630  |     0.639\n",
            "2020-11-28 02:02:20,854 - INFO - allennlp.training.tensorboard_writer - loss               |     0.297  |     0.455\n",
            "2020-11-28 02:02:20,855 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.898  |     0.843\n",
            "2020-11-28 02:02:20,856 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.690  |     0.642\n",
            "2020-11-28 02:02:20,856 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:02:20,857 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.764  |     0.645\n",
            "2020-11-28 02:02:20,858 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:02:21,364 - INFO - allennlp.training.trainer - Epoch duration: 0:02:26.145034\n",
            "2020-11-28 02:02:21,364 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:58\n",
            "2020-11-28 02:02:21,365 - INFO - allennlp.training.trainer - Epoch 30/99\n",
            "2020-11-28 02:02:21,365 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:02:21,485 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:02:21,487 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9000, precision-overall: 0.7690, recall-overall: 0.6359, f1-measure-overall: 0.6961, loss: 0.2944 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.37it/s]\n",
            "2020-11-28 02:04:21,402 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8393, precision-overall: 0.6277, recall-overall: 0.6542, f1-measure-overall: 0.6407, loss: 0.4578 ||: 100%|##########| 453/453 [00:23<00:00, 19.14it/s]\n",
            "2020-11-28 02:04:45,071 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:04:45,071 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.636  |     0.654\n",
            "2020-11-28 02:04:45,072 - INFO - allennlp.training.tensorboard_writer - loss               |     0.294  |     0.458\n",
            "2020-11-28 02:04:45,073 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.900  |     0.839\n",
            "2020-11-28 02:04:45,074 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.696  |     0.641\n",
            "2020-11-28 02:04:45,075 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:04:45,076 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.769  |     0.628\n",
            "2020-11-28 02:04:45,076 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:04:45,576 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.211478\n",
            "2020-11-28 02:04:45,577 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:47:29\n",
            "2020-11-28 02:04:45,577 - INFO - allennlp.training.trainer - Epoch 31/99\n",
            "2020-11-28 02:04:45,577 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:04:45,698 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:04:45,700 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.8993, precision-overall: 0.7650, recall-overall: 0.6379, f1-measure-overall: 0.6957, loss: 0.2909 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.28it/s]\n",
            "2020-11-28 02:06:46,579 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8359, precision-overall: 0.6154, recall-overall: 0.6703, f1-measure-overall: 0.6417, loss: 0.4690 ||: 100%|##########| 453/453 [00:24<00:00, 18.87it/s]\n",
            "2020-11-28 02:07:10,587 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:07:10,587 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.638  |     0.670\n",
            "2020-11-28 02:07:10,589 - INFO - allennlp.training.tensorboard_writer - loss               |     0.291  |     0.469\n",
            "2020-11-28 02:07:10,590 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.899  |     0.836\n",
            "2020-11-28 02:07:10,591 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.696  |     0.642\n",
            "2020-11-28 02:07:10,591 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:07:10,592 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.765  |     0.615\n",
            "2020-11-28 02:07:10,593 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:07:11,151 - INFO - allennlp.training.trainer - Epoch duration: 0:02:25.573647\n",
            "2020-11-28 02:07:11,151 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:45:03\n",
            "2020-11-28 02:07:11,151 - INFO - allennlp.training.trainer - Epoch 32/99\n",
            "2020-11-28 02:07:11,151 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:07:11,274 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:07:11,276 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9009, precision-overall: 0.7704, recall-overall: 0.6414, f1-measure-overall: 0.7000, loss: 0.2888 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.32it/s]\n",
            "2020-11-28 02:09:11,674 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8367, precision-overall: 0.6160, recall-overall: 0.6759, f1-measure-overall: 0.6446, loss: 0.4691 ||: 100%|##########| 453/453 [00:23<00:00, 19.22it/s]\n",
            "2020-11-28 02:09:35,241 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:09:35,241 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.641  |     0.676\n",
            "2020-11-28 02:09:35,242 - INFO - allennlp.training.tensorboard_writer - loss               |     0.289  |     0.469\n",
            "2020-11-28 02:09:35,243 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.901  |     0.837\n",
            "2020-11-28 02:09:35,244 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.700  |     0.645\n",
            "2020-11-28 02:09:35,245 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:09:35,245 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.770  |     0.616\n",
            "2020-11-28 02:09:35,246 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:09:35,741 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.589557\n",
            "2020-11-28 02:09:35,741 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:42:35\n",
            "2020-11-28 02:09:35,741 - INFO - allennlp.training.trainer - Epoch 33/99\n",
            "2020-11-28 02:09:35,741 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:09:35,855 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:09:35,857 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9016, precision-overall: 0.7732, recall-overall: 0.6436, f1-measure-overall: 0.7025, loss: 0.2835 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.39it/s]\n",
            "2020-11-28 02:11:35,539 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8435, precision-overall: 0.6451, recall-overall: 0.6394, f1-measure-overall: 0.6423, loss: 0.4603 ||: 100%|##########| 453/453 [00:23<00:00, 19.39it/s]\n",
            "2020-11-28 02:11:58,905 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:11:58,906 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.644  |     0.639\n",
            "2020-11-28 02:11:58,907 - INFO - allennlp.training.tensorboard_writer - loss               |     0.283  |     0.460\n",
            "2020-11-28 02:11:58,908 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.902  |     0.843\n",
            "2020-11-28 02:11:58,908 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.702  |     0.642\n",
            "2020-11-28 02:11:58,909 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:11:58,910 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.773  |     0.645\n",
            "2020-11-28 02:11:58,911 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:11:59,388 - INFO - allennlp.training.trainer - Epoch duration: 0:02:23.646810\n",
            "2020-11-28 02:11:59,388 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:40:06\n",
            "2020-11-28 02:11:59,389 - INFO - allennlp.training.trainer - Epoch 34/99\n",
            "2020-11-28 02:11:59,389 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:11:59,502 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:11:59,504 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9020, precision-overall: 0.7705, recall-overall: 0.6472, f1-measure-overall: 0.7035, loss: 0.2835 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.32it/s]\n",
            "2020-11-28 02:13:59,944 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8455, precision-overall: 0.6526, recall-overall: 0.6321, f1-measure-overall: 0.6422, loss: 0.4517 ||: 100%|##########| 453/453 [00:23<00:00, 19.52it/s]\n",
            "2020-11-28 02:14:23,160 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:14:23,161 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.647  |     0.632\n",
            "2020-11-28 02:14:23,162 - INFO - allennlp.training.tensorboard_writer - loss               |     0.283  |     0.452\n",
            "2020-11-28 02:14:23,163 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.902  |     0.845\n",
            "2020-11-28 02:14:23,164 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.704  |     0.642\n",
            "2020-11-28 02:14:23,165 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:14:23,165 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.770  |     0.653\n",
            "2020-11-28 02:14:23,166 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:14:23,663 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.274737\n",
            "2020-11-28 02:14:23,664 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:37:38\n",
            "2020-11-28 02:14:23,664 - INFO - allennlp.training.trainer - Epoch 35/99\n",
            "2020-11-28 02:14:23,664 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:14:23,777 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:14:23,779 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9040, precision-overall: 0.7759, recall-overall: 0.6571, f1-measure-overall: 0.7116, loss: 0.2784 ||: 100%|##########| 1363/1363 [01:59<00:00, 11.36it/s]\n",
            "2020-11-28 02:16:23,736 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8469, precision-overall: 0.6499, recall-overall: 0.6536, f1-measure-overall: 0.6517, loss: 0.4512 ||: 100%|##########| 453/453 [00:23<00:00, 18.98it/s]\n",
            "2020-11-28 02:16:47,607 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:16:47,608 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.657  |     0.654\n",
            "2020-11-28 02:16:47,609 - INFO - allennlp.training.tensorboard_writer - loss               |     0.278  |     0.451\n",
            "2020-11-28 02:16:47,610 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.904  |     0.847\n",
            "2020-11-28 02:16:47,611 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.712  |     0.652\n",
            "2020-11-28 02:16:47,612 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:16:47,612 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.776  |     0.650\n",
            "2020-11-28 02:16:47,613 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:16:48,129 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.465212\n",
            "2020-11-28 02:16:48,130 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:35:10\n",
            "2020-11-28 02:16:48,130 - INFO - allennlp.training.trainer - Epoch 36/99\n",
            "2020-11-28 02:16:48,130 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:16:48,247 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:16:48,249 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9052, precision-overall: 0.7819, recall-overall: 0.6576, f1-measure-overall: 0.7144, loss: 0.2753 ||: 100%|##########| 1363/1363 [02:02<00:00, 11.11it/s]\n",
            "2020-11-28 02:18:50,971 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8457, precision-overall: 0.6553, recall-overall: 0.6396, f1-measure-overall: 0.6474, loss: 0.4469 ||: 100%|##########| 453/453 [00:23<00:00, 19.22it/s]\n",
            "2020-11-28 02:19:14,537 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:19:14,538 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.658  |     0.640\n",
            "2020-11-28 02:19:14,539 - INFO - allennlp.training.tensorboard_writer - loss               |     0.275  |     0.447\n",
            "2020-11-28 02:19:14,540 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.905  |     0.846\n",
            "2020-11-28 02:19:14,540 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.714  |     0.647\n",
            "2020-11-28 02:19:14,541 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:19:14,542 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.782  |     0.655\n",
            "2020-11-28 02:19:14,542 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:19:15,035 - INFO - allennlp.training.trainer - Epoch duration: 0:02:26.904758\n",
            "2020-11-28 02:19:15,035 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:32:47\n",
            "2020-11-28 02:19:15,035 - INFO - allennlp.training.trainer - Epoch 37/99\n",
            "2020-11-28 02:19:15,036 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:19:15,151 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:19:15,153 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9056, precision-overall: 0.7796, recall-overall: 0.6642, f1-measure-overall: 0.7173, loss: 0.2717 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.31it/s]\n",
            "2020-11-28 02:21:15,635 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8453, precision-overall: 0.6455, recall-overall: 0.6474, f1-measure-overall: 0.6464, loss: 0.4624 ||: 100%|##########| 453/453 [00:23<00:00, 19.15it/s]\n",
            "2020-11-28 02:21:39,290 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2020-11-28 02:21:39,290 - INFO - allennlp.training.tensorboard_writer - recall-overall     |     0.664  |     0.647\n",
            "2020-11-28 02:21:39,291 - INFO - allennlp.training.tensorboard_writer - loss               |     0.272  |     0.462\n",
            "2020-11-28 02:21:39,292 - INFO - allennlp.training.tensorboard_writer - accuracy           |     0.906  |     0.845\n",
            "2020-11-28 02:21:39,293 - INFO - allennlp.training.tensorboard_writer - f1-measure-overall |     0.717  |     0.646\n",
            "2020-11-28 02:21:39,294 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB      |  5639.524  |       N/A\n",
            "2020-11-28 02:21:39,294 - INFO - allennlp.training.tensorboard_writer - precision-overall  |     0.780  |     0.645\n",
            "2020-11-28 02:21:39,295 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |  2431.000  |       N/A\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:628: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "2020-11-28 02:21:39,773 - INFO - allennlp.training.trainer - Epoch duration: 0:02:24.737956\n",
            "2020-11-28 02:21:39,774 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:30:21\n",
            "2020-11-28 02:21:39,774 - INFO - allennlp.training.trainer - Epoch 38/99\n",
            "2020-11-28 02:21:39,774 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5639.524\n",
            "2020-11-28 02:21:39,891 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 2431\n",
            "2020-11-28 02:21:39,893 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9056, precision-overall: 0.7827, recall-overall: 0.6595, f1-measure-overall: 0.7158, loss: 0.2704 ||: 100%|##########| 1363/1363 [02:00<00:00, 11.29it/s]\n",
            "2020-11-28 02:23:40,603 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.8369, precision-overall: 0.6142, recall-overall: 0.6827, f1-measure-overall: 0.6466, loss: 0.4774 ||: 100%|##########| 453/453 [00:23<00:00, 19.03it/s]\n",
            "2020-11-28 02:24:04,413 - INFO - allennlp.training.trainer - Ran out of patience.  Stopping training.\n",
            "2020-11-28 02:24:04,417 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2020-11-28 02:24:04,578 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred/model.tar.gz\n",
            "2020-11-28 02:24:08,374 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 28,\n",
            "  \"peak_cpu_memory_MB\": 5639.524,\n",
            "  \"peak_gpu_0_memory_MB\": 2431,\n",
            "  \"training_duration\": \"1:32:08.539778\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 37,\n",
            "  \"epoch\": 37,\n",
            "  \"training_accuracy\": 0.9055692560624743,\n",
            "  \"training_precision-overall\": 0.7796120884077582,\n",
            "  \"training_recall-overall\": 0.6641561635413464,\n",
            "  \"training_f1-measure-overall\": 0.7172677096733536,\n",
            "  \"training_loss\": 0.2716577035222566,\n",
            "  \"training_cpu_memory_MB\": 5639.524,\n",
            "  \"training_gpu_0_memory_MB\": 2431,\n",
            "  \"validation_accuracy\": 0.845300693738677,\n",
            "  \"validation_precision-overall\": 0.6454512105649303,\n",
            "  \"validation_recall-overall\": 0.6473509933774835,\n",
            "  \"validation_f1-measure-overall\": 0.6463997060984071,\n",
            "  \"validation_loss\": 0.4623932216077977,\n",
            "  \"best_validation_accuracy\": 0.843489019486545,\n",
            "  \"best_validation_precision-overall\": 0.6391934913335692,\n",
            "  \"best_validation_recall-overall\": 0.6648270787343635,\n",
            "  \"best_validation_f1-measure-overall\": 0.6517583408475605,\n",
            "  \"best_validation_loss\": 0.4437773477853529\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0w-qoCJ-VH"
      },
      "source": [
        "# Part 3: Run Probing Evaluation on TACRED Models (trained in part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrYiH2C8KEL9"
      },
      "source": [
        "# Probing Task Evaluation on TACRED \n",
        "!python '/content/drive/My Drive/685_project/REval/probing_task_evaluation.py' \\\n",
        "  --model-dir '/content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred_bert/' \\\n",
        "  --data-dir '/content/drive/My Drive/685_project/REval/data/TACRED/' \\\n",
        "  --dataset tacred --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0XFPHoTrmZN",
        "outputId": "ebb6166a-d162-4677-d44c-b6c167aa7f2f"
      },
      "source": [
        "# New Probing Task Evaluation on TACRED \n",
        "!python '/content/drive/My Drive/685_project/REval/REval_modified/KLnew_probing_task_evaluation.py' \\\n",
        "  --model-dir '/content/drive/My Drive/685_project/RelEx/models/baseline_self_/' \\\n",
        "  --data-dir '/content/drive/My Drive/685_project/REval/data/TACRED/' \\\n",
        "  --dataset tacred --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-12-03 13:23:55,317 INFO     loading archive file /content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/model.tar.gz\n",
            "2020-12-03 13:23:55,318 INFO     extracting archive file /content/drive/My Drive/685_project/RelEx/models/baseline_lstm_tacred/model.tar.gz to temp dir /tmp/tmp1iqljg4e\n",
            "2020-12-03 13:23:56,588 INFO     instantiating registered subclass basic_relation_classifier of <class 'allennlp.models.model.Model'>\n",
            "2020-12-03 13:23:56,589 INFO     vocabulary.type = default\n",
            "2020-12-03 13:23:56,589 INFO     instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
            "2020-12-03 13:23:56,589 INFO     Loading token dictionary from /tmp/tmp1iqljg4e/vocabulary.\n",
            "2020-12-03 13:23:56,624 INFO     instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'type': 'basic_relation_classifier', 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,624 INFO     model.type = basic_relation_classifier\n",
            "2020-12-03 13:23:56,624 INFO     instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'verbose_metrics': False, 'word_dropout': 0.04} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,624 INFO     instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,624 INFO     model.text_field_embedder.type = basic\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.embedder_to_indexer_map = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.allow_unmatched_keys = False\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.token_embedders = None\n",
            "2020-12-03 13:23:56,625 INFO     instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.type = embedding\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.num_embeddings = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.vocab_namespace = tokens\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.embedding_dim = 300\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.pretrained_file = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.projection_dim = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.trainable = False\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.padding_index = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.max_norm = None\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.norm_type = 2.0\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
            "2020-12-03 13:23:56,625 INFO     model.text_field_embedder.tokens.sparse = False\n",
            "2020-12-03 13:23:56,752 INFO     instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final', 'type': 'seq2seq_pool'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,752 INFO     model.text_encoder.type = seq2seq_pool\n",
            "2020-12-03 13:23:56,753 INFO     instantiating class <class 'relex.modules.seq2vec_encoders.seq2seq_pool_encoder.Seq2SeqPoolEncoder'> from params {'encoder': {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'}, 'pooling': 'final'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,753 INFO     instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': False, 'dropout': 0.5, 'hidden_size': 500, 'input_size': 360, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.type = lstm\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.batch_first = True\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.stateful = False\n",
            "2020-12-03 13:23:56,753 INFO     Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
            "2020-12-03 13:23:56,753 INFO     CURRENTLY DEFINED PARAMETERS: \n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.bidirectional = False\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.dropout = 0.5\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.hidden_size = 500\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.input_size = 360\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.num_layers = 2\n",
            "2020-12-03 13:23:56,753 INFO     model.text_encoder.encoder.batch_first = True\n",
            "2020-12-03 13:23:56,782 INFO     model.text_encoder.pooling = final\n",
            "2020-12-03 13:23:56,782 INFO     model.text_encoder.pooling_scope = None\n",
            "2020-12-03 13:23:56,782 INFO     instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 500, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,782 INFO     model.classifier_feedforward.input_dim = 500\n",
            "2020-12-03 13:23:56,782 INFO     model.classifier_feedforward.num_layers = 1\n",
            "2020-12-03 13:23:56,782 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-03 13:23:56,783 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-03 13:23:56,783 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-03 13:23:56,783 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-12-03 13:23:56,783 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-03 13:23:56,783 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-12-03 13:23:56,783 INFO     type = linear\n",
            "2020-12-03 13:23:56,783 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-03 13:23:56,783 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-03 13:23:56,784 INFO     model.word_dropout = 0.04\n",
            "2020-12-03 13:23:56,784 INFO     model.embedding_dropout = 0\n",
            "2020-12-03 13:23:56,784 INFO     model.encoding_dropout = 0\n",
            "2020-12-03 13:23:56,784 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,784 INFO     model.offset_embedder_head.type = relative\n",
            "2020-12-03 13:23:56,784 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,785 INFO     model.offset_embedder_head.n_position = 100\n",
            "2020-12-03 13:23:56,785 INFO     model.offset_embedder_head.embedding_dim = 30\n",
            "2020-12-03 13:23:56,800 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,800 INFO     model.offset_embedder_tail.type = relative\n",
            "2020-12-03 13:23:56,800 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-03 13:23:56,801 INFO     model.offset_embedder_tail.n_position = 100\n",
            "2020-12-03 13:23:56,801 INFO     model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-12-03 13:23:56,801 INFO     model.verbose_metrics = False\n",
            "2020-12-03 13:23:56,801 INFO     model.ignore_label = no_relation\n",
            "2020-12-03 13:23:56,801 INFO     model.f1_average = micro\n",
            "2020-12-03 13:23:56,801 INFO     model.use_adjacency = False\n",
            "2020-12-03 13:23:56,801 INFO     model.use_entity_offsets = False\n",
            "2020-12-03 13:23:56,802 INFO     Initializing parameters\n",
            "2020-12-03 13:23:56,802 INFO     Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-12-03 13:23:56,802 INFO        classifier_feedforward._linear_layers.0.bias\n",
            "2020-12-03 13:23:56,802 INFO        classifier_feedforward._linear_layers.0.weight\n",
            "2020-12-03 13:23:56,802 INFO        offset_embedder_head._embedding.weight\n",
            "2020-12-03 13:23:56,802 INFO        offset_embedder_tail._embedding.weight\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.bias_hh_l0\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.bias_hh_l1\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.bias_ih_l0\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.bias_ih_l1\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.weight_hh_l0\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.weight_hh_l1\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.weight_ih_l0\n",
            "2020-12-03 13:23:56,802 INFO        text_encoder._encoder._module.weight_ih_l1\n",
            "2020-12-03 13:23:56,803 INFO        text_field_embedder.token_embedder_tokens.weight\n",
            "2020-12-03 13:24:07,224 INFO     instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'tacred'} and extras set()\n",
            "2020-12-03 13:24:07,224 INFO     dataset_reader.type = tacred\n",
            "2020-12-03 13:24:07,224 INFO     instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2020-12-03 13:24:07,224 INFO     dataset_reader.max_len = 100\n",
            "2020-12-03 13:24:07,224 INFO     dataset_reader.masking_mode = NER+Grammar\n",
            "2020-12-03 13:24:07,224 INFO     dataset_reader.lazy = False\n",
            "2020-12-03 13:24:07,225 INFO     instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-12-03 13:24:07,225 INFO     instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-12-03 13:24:07,225 INFO     dataset_reader.dep_pruning = 1\n",
            "2020-12-03 13:24:07,225 INFO     instantiating registered subclass relation_classifier of <class 'allennlp.predictors.predictor.Predictor'>\n",
            "2020-12-03 13:24:07,226 INFO     Parameters: {\n",
            "    \"batch_size\": 64,\n",
            "    \"classifier\": {\n",
            "        \"batch_size\": 64,\n",
            "        \"dropout\": 0,\n",
            "        \"epoch_size\": 12,\n",
            "        \"nhid\": 256,\n",
            "        \"optim\": \"adam\",\n",
            "        \"tenacity\": 5\n",
            "    },\n",
            "    \"kfold\": 10,\n",
            "    \"task_path\": \"/content/drive/My Drive/685_project/REval/data/TACRED/\",\n",
            "    \"usepytorch\": true\n",
            "}\n",
            "2020-12-03 13:24:07,226 INFO     Tasks: ['ArgumentAddGrammarRole_Head', 'ArgumentAddGrammarRole_Tail', 'ArgumentGrammarRole_ControlHead', 'ArgumentGrammarRole_ControlTail']\n",
            "2020-12-03 13:24:08,199 INFO     Loaded 1400 train - 298 dev - 300 test for ArgumentHeadAddGrammaticalRole\n",
            "2020-12-03 13:24:08,199 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-03 13:24:10,371 INFO     Computed embeddings\n",
            "2020-12-03 13:24:10,371 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-03 13:24:22,135 INFO     [('reg:1e-05', 74.16), ('reg:0.0001', 74.16), ('reg:0.001', 74.83), ('reg:0.01', 74.83)]\n",
            "2020-12-03 13:24:22,135 INFO     Validation : best param found is reg = 0.001 with score             74.83\n",
            "2020-12-03 13:24:22,136 INFO     Evaluating...\n",
            "2020-12-03 13:24:25,241 INFO     Loaded 1401 train - 291 dev - 299 test for ArgumentTailAddGrammaticalRole\n",
            "2020-12-03 13:24:25,249 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-03 13:24:25,308 INFO     Computed embeddings\n",
            "2020-12-03 13:24:25,309 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-03 13:24:38,039 INFO     [('reg:1e-05', 80.41), ('reg:0.0001', 80.41), ('reg:0.001', 80.76), ('reg:0.01', 80.76)]\n",
            "2020-12-03 13:24:38,039 INFO     Validation : best param found is reg = 0.001 with score             80.76\n",
            "2020-12-03 13:24:38,039 INFO     Evaluating...\n",
            "2020-12-03 13:24:41,319 INFO     Loaded 1400 train - 298 dev - 300 test for ArgumentHeadAddGrammaticalRoleControl\n",
            "2020-12-03 13:24:41,328 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-03 13:24:41,375 INFO     Computed embeddings\n",
            "2020-12-03 13:24:41,376 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-03 13:24:56,234 INFO     [('reg:1e-05', 34.56), ('reg:0.0001', 36.91), ('reg:0.001', 35.23), ('reg:0.01', 34.23)]\n",
            "2020-12-03 13:24:56,235 INFO     Validation : best param found is reg = 0.0001 with score             36.91\n",
            "2020-12-03 13:24:56,235 INFO     Evaluating...\n",
            "2020-12-03 13:25:00,215 INFO     Loaded 1401 train - 291 dev - 299 test for ArgumentTailAddGrammaticalRoleControl\n",
            "2020-12-03 13:25:00,224 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-03 13:25:00,269 INFO     Computed embeddings\n",
            "2020-12-03 13:25:00,269 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-03 13:25:14,799 INFO     [('reg:1e-05', 35.05), ('reg:0.0001', 35.74), ('reg:0.001', 34.02), ('reg:0.01', 35.4)]\n",
            "2020-12-03 13:25:14,799 INFO     Validation : best param found is reg = 0.0001 with score             35.74\n",
            "2020-12-03 13:25:14,799 INFO     Evaluating...\n",
            "Probing Task Results: \n",
            "    ArgumentAddGrammarRole_Head\n",
            "        devacc: 74.83\n",
            "        testacc: 75.0\n",
            "        testF1: 28.57\n",
            "        ndev: 298\n",
            "        ntest: 300\n",
            "    ArgumentAddGrammarRole_Tail\n",
            "        devacc: 80.76\n",
            "        testacc: 76.92\n",
            "        testF1: 28.99\n",
            "        ndev: 291\n",
            "        ntest: 299\n",
            "    ArgumentGrammarRole_ControlHead\n",
            "        devacc: 36.91\n",
            "        testacc: 36.67\n",
            "        testF1: 35.76\n",
            "        ndev: 298\n",
            "        ntest: 300\n",
            "    ArgumentGrammarRole_ControlTail\n",
            "        devacc: 35.74\n",
            "        testacc: 33.44\n",
            "        testF1: 31.32\n",
            "        ndev: 291\n",
            "        ntest: 299\n",
            "2020-12-03 13:25:19,074 INFO     removing temporary unarchived model dir at /tmp/tmp1iqljg4e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KbHwIkYOfq2",
        "outputId": "e3b72d6c-94bb-4844-89ab-64a8c5c40282"
      },
      "source": [
        "# New Probing Task Evaluation on TACRED \n",
        "!python '/content/drive/My Drive/685_project/REval/REval_modified/KLnew_probing_task_evaluation.py' \\\n",
        "  --model-dir '/content/drive/My Drive/685_project/RelEx/models/baseline_cnn_tacred_bert/' \\\n",
        "  --data-dir '/content/drive/My Drive/685_project/REval/data/TACRED/' \\\n",
        "  --dataset tacred --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-12-04 06:35:48,300 INFO     loading archive file /content/drive/My Drive/685_project/RelEx/models/baseline_cnn_tacred_bert/model.tar.gz\n",
            "2020-12-04 06:35:48,302 INFO     extracting archive file /content/drive/My Drive/685_project/RelEx/models/baseline_cnn_tacred_bert/model.tar.gz to temp dir /tmp/tmplrwwlv_j\n",
            "2020-12-04 06:35:55,539 INFO     instantiating registered subclass basic_relation_classifier of <class 'allennlp.models.model.Model'>\n",
            "2020-12-04 06:35:55,539 INFO     vocabulary.type = default\n",
            "2020-12-04 06:35:55,540 INFO     instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
            "2020-12-04 06:35:55,540 INFO     Loading token dictionary from /tmp/tmplrwwlv_j/vocabulary.\n",
            "2020-12-04 06:35:55,541 INFO     instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 800, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'regularizer': [['text_encoder.conv_layer_.*weight', {'alpha': 0.001, 'type': 'l2'}]], 'text_encoder': {'conv_layer_activation': 'tanh', 'embedding_dim': 828, 'ngram_filter_sizes': [2, 3, 4, 5], 'num_filters': 200, 'type': 'cnn'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained'}}, 'type': 'basic_relation_classifier', 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-12-04 06:35:55,541 INFO     model.type = basic_relation_classifier\n",
            "2020-12-04 06:35:55,542 INFO     instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 800, 'num_layers': 1}, 'embedding_dropout': 0, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'regularizer': [['text_encoder.conv_layer_.*weight', {'alpha': 0.001, 'type': 'l2'}]], 'text_encoder': {'conv_layer_activation': 'tanh', 'embedding_dim': 828, 'ngram_filter_sizes': [2, 3, 4, 5], 'num_filters': 200, 'type': 'cnn'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained'}}, 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-12-04 06:35:55,542 INFO     instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained'}} and extras {'vocab'}\n",
            "2020-12-04 06:35:55,542 INFO     model.text_field_embedder.type = basic\n",
            "2020-12-04 06:35:55,542 INFO     model.text_field_embedder.allow_unmatched_keys = True\n",
            "2020-12-04 06:35:55,542 INFO     model.text_field_embedder.token_embedders = None\n",
            "2020-12-04 06:35:55,542 INFO     instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained'} and extras {'vocab'}\n",
            "2020-12-04 06:35:55,542 INFO     model.text_field_embedder.tokens.type = bert-pretrained\n",
            "2020-12-04 06:35:55,542 INFO     instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-uncased'} and extras {'vocab'}\n",
            "2020-12-04 06:35:55,543 INFO     model.text_field_embedder.tokens.pretrained_model = bert-base-uncased\n",
            "2020-12-04 06:35:55,543 INFO     model.text_field_embedder.tokens.requires_grad = False\n",
            "2020-12-04 06:35:55,543 INFO     model.text_field_embedder.tokens.top_layer_only = False\n",
            "2020-12-04 06:35:55,543 INFO     model.text_field_embedder.tokens.scalar_mix_parameters = None\n",
            "2020-12-04 06:35:55,847 INFO     https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpd5fwscds\n",
            "100% 407873900/407873900 [00:10<00:00, 38909313.98B/s]\n",
            "2020-12-04 06:36:06,694 INFO     copying /tmp/tmpd5fwscds to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2020-12-04 06:36:07,765 INFO     creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2020-12-04 06:36:07,765 INFO     removing temp file /tmp/tmpd5fwscds\n",
            "2020-12-04 06:36:07,818 INFO     loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "2020-12-04 06:36:07,819 INFO     extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpt3kdsvo6\n",
            "2020-12-04 06:36:11,603 INFO     Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "2020-12-04 06:36:16,330 INFO     instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'tanh', 'embedding_dim': 828, 'ngram_filter_sizes': [2, 3, 4, 5], 'num_filters': 200, 'type': 'cnn'} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,330 INFO     model.text_encoder.type = cnn\n",
            "2020-12-04 06:36:16,330 INFO     instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'tanh', 'embedding_dim': 828, 'ngram_filter_sizes': [2, 3, 4, 5], 'num_filters': 200} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,331 INFO     model.text_encoder.embedding_dim = 828\n",
            "2020-12-04 06:36:16,331 INFO     model.text_encoder.num_filters = 200\n",
            "2020-12-04 06:36:16,331 INFO     model.text_encoder.ngram_filter_sizes = [2, 3, 4, 5]\n",
            "2020-12-04 06:36:16,331 INFO     model.text_encoder.conv_layer_activation = tanh\n",
            "2020-12-04 06:36:16,331 INFO     instantiating registered subclass tanh of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:36:16,331 INFO     model.text_encoder.output_dim = None\n",
            "2020-12-04 06:36:16,354 INFO     instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 800, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,354 INFO     model.classifier_feedforward.input_dim = 800\n",
            "2020-12-04 06:36:16,354 INFO     model.classifier_feedforward.num_layers = 1\n",
            "2020-12-04 06:36:16,355 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:36:16,355 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:36:16,355 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:36:16,355 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-12-04 06:36:16,355 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:36:16,355 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-12-04 06:36:16,355 INFO     type = linear\n",
            "2020-12-04 06:36:16,355 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:36:16,356 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:36:16,356 INFO     model.word_dropout = 0\n",
            "2020-12-04 06:36:16,356 INFO     model.embedding_dropout = 0\n",
            "2020-12-04 06:36:16,356 INFO     model.encoding_dropout = 0.5\n",
            "2020-12-04 06:36:16,356 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,356 INFO     model.offset_embedder_head.type = relative\n",
            "2020-12-04 06:36:16,357 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,357 INFO     model.offset_embedder_head.n_position = 100\n",
            "2020-12-04 06:36:16,357 INFO     model.offset_embedder_head.embedding_dim = 30\n",
            "2020-12-04 06:36:16,357 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,357 INFO     model.offset_embedder_tail.type = relative\n",
            "2020-12-04 06:36:16,358 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:36:16,358 INFO     model.offset_embedder_tail.n_position = 100\n",
            "2020-12-04 06:36:16,358 INFO     model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-12-04 06:36:16,358 INFO     model.regularizer.0.1.type = l2\n",
            "2020-12-04 06:36:16,358 INFO     instantiating registered subclass l2 of <class 'allennlp.nn.regularizers.regularizer.Regularizer'>\n",
            "2020-12-04 06:36:16,359 INFO     model.verbose_metrics = False\n",
            "2020-12-04 06:36:16,359 INFO     model.ignore_label = no_relation\n",
            "2020-12-04 06:36:16,359 INFO     model.f1_average = micro\n",
            "2020-12-04 06:36:16,359 INFO     model.use_adjacency = False\n",
            "2020-12-04 06:36:16,359 INFO     model.use_entity_offsets = False\n",
            "2020-12-04 06:36:16,359 INFO     Initializing parameters\n",
            "2020-12-04 06:36:16,361 INFO     Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-12-04 06:36:16,361 INFO        classifier_feedforward._linear_layers.0.bias\n",
            "2020-12-04 06:36:16,361 INFO        classifier_feedforward._linear_layers.0.weight\n",
            "2020-12-04 06:36:16,362 INFO        offset_embedder_head._embedding.weight\n",
            "2020-12-04 06:36:16,362 INFO        offset_embedder_tail._embedding.weight\n",
            "2020-12-04 06:36:16,362 INFO        text_encoder.conv_layer_0.bias\n",
            "2020-12-04 06:36:16,362 INFO        text_encoder.conv_layer_0.weight\n",
            "2020-12-04 06:36:16,362 INFO        text_encoder.conv_layer_1.bias\n",
            "2020-12-04 06:36:16,362 INFO        text_encoder.conv_layer_1.weight\n",
            "2020-12-04 06:36:16,362 INFO        text_encoder.conv_layer_2.bias\n",
            "2020-12-04 06:36:16,363 INFO        text_encoder.conv_layer_2.weight\n",
            "2020-12-04 06:36:16,363 INFO        text_encoder.conv_layer_3.bias\n",
            "2020-12-04 06:36:16,363 INFO        text_encoder.conv_layer_3.weight\n",
            "2020-12-04 06:36:16,363 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.gamma\n",
            "2020-12-04 06:36:16,363 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.0\n",
            "2020-12-04 06:36:16,363 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.1\n",
            "2020-12-04 06:36:16,363 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.10\n",
            "2020-12-04 06:36:16,363 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.11\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.2\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.3\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.4\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.5\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.6\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.7\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.8\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.9\n",
            "2020-12-04 06:36:16,364 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.LayerNorm.bias\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.LayerNorm.weight\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.position_embeddings.weight\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.token_type_embeddings.weight\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.word_embeddings.weight\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,365 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.key.bias\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.key.weight\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.query.bias\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.query.weight\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.value.bias\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.value.weight\n",
            "2020-12-04 06:36:16,366 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.dense.bias\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.dense.weight\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,367 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.key.bias\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.key.weight\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.query.bias\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.query.weight\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.value.bias\n",
            "2020-12-04 06:36:16,368 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.value.weight\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.dense.bias\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.dense.weight\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,369 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.key.bias\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.key.weight\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.query.bias\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.query.weight\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.value.bias\n",
            "2020-12-04 06:36:16,370 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.value.weight\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.dense.bias\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.dense.weight\n",
            "2020-12-04 06:36:16,371 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.key.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.key.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.query.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.query.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.value.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.value.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.dense.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.dense.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,372 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.key.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.key.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.query.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.query.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.value.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.value.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.dense.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.dense.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.key.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.key.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.query.bias\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.query.weight\n",
            "2020-12-04 06:36:16,373 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.value.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.value.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.dense.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.dense.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.key.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.key.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.query.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.query.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.value.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.value.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,374 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.dense.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.dense.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.key.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.key.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.query.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.query.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.value.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.value.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.dense.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.dense.weight\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,375 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.key.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.key.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.query.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.query.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.value.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.value.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.dense.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.dense.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.key.bias\n",
            "2020-12-04 06:36:16,376 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.key.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.query.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.query.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.value.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.value.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.dense.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.dense.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.key.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.key.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.query.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.query.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.value.bias\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.value.weight\n",
            "2020-12-04 06:36:16,377 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.dense.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.dense.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.dense.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.dense.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.key.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.key.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.query.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.query.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.value.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.value.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.intermediate.dense.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.intermediate.dense.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.dense.bias\n",
            "2020-12-04 06:36:16,378 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.dense.weight\n",
            "2020-12-04 06:36:16,379 INFO        text_field_embedder.token_embedder_tokens.bert_model.pooler.dense.bias\n",
            "2020-12-04 06:36:16,379 INFO        text_field_embedder.token_embedder_tokens.bert_model.pooler.dense.weight\n",
            "2020-12-04 06:36:27,195 INFO     instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': None, 'max_len': 100, 'token_indexers': {'tokens': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': True}}, 'type': 'tacred'} and extras set()\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.type = tacred\n",
            "2020-12-04 06:36:27,196 INFO     instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': None, 'max_len': 100, 'token_indexers': {'tokens': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': True}}} and extras set()\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.max_len = 100\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.masking_mode = None\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.lazy = False\n",
            "2020-12-04 06:36:27,196 INFO     instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': True} and extras set()\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.token_indexers.tokens.type = bert-pretrained\n",
            "2020-12-04 06:36:27,196 INFO     instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': True} and extras set()\n",
            "2020-12-04 06:36:27,196 INFO     dataset_reader.token_indexers.tokens.pretrained_model = bert-base-uncased\n",
            "2020-12-04 06:36:27,197 INFO     dataset_reader.token_indexers.tokens.use_starting_offsets = True\n",
            "2020-12-04 06:36:27,197 INFO     dataset_reader.token_indexers.tokens.do_lowercase = True\n",
            "2020-12-04 06:36:27,197 INFO     dataset_reader.token_indexers.tokens.never_lowercase = None\n",
            "2020-12-04 06:36:27,197 INFO     dataset_reader.token_indexers.tokens.max_pieces = 512\n",
            "2020-12-04 06:36:27,197 INFO     dataset_reader.token_indexers.tokens.truncate_long_sequences = True\n",
            "2020-12-04 06:36:27,485 INFO     https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpgi32wiem\n",
            "100% 231508/231508 [00:00<00:00, 1154268.98B/s]\n",
            "2020-12-04 06:36:27,985 INFO     copying /tmp/tmpgi32wiem to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-12-04 06:36:27,985 INFO     creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-12-04 06:36:27,985 INFO     removing temp file /tmp/tmpgi32wiem\n",
            "2020-12-04 06:36:27,985 INFO     loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-12-04 06:36:28,137 INFO     dataset_reader.dep_pruning = 1\n",
            "2020-12-04 06:36:28,138 INFO     instantiating registered subclass relation_classifier of <class 'allennlp.predictors.predictor.Predictor'>\n",
            "2020-12-04 06:36:28,138 INFO     Parameters: {\n",
            "    \"batch_size\": 64,\n",
            "    \"classifier\": {\n",
            "        \"batch_size\": 64,\n",
            "        \"dropout\": 0,\n",
            "        \"epoch_size\": 12,\n",
            "        \"nhid\": 256,\n",
            "        \"optim\": \"adam\",\n",
            "        \"tenacity\": 5\n",
            "    },\n",
            "    \"kfold\": 10,\n",
            "    \"task_path\": \"/content/drive/My Drive/685_project/REval/data/TACRED/\",\n",
            "    \"usepytorch\": true\n",
            "}\n",
            "2020-12-04 06:36:28,138 INFO     Tasks: ['ArgumentAddGrammarRole_Head', 'ArgumentAddGrammarRole_Tail', 'ArgumentGrammarRole_ControlHead', 'ArgumentGrammarRole_ControlTail']\n",
            "2020-12-04 06:36:28,194 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRole\n",
            "2020-12-04 06:36:28,194 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:36:41,052 INFO     Computed embeddings\n",
            "2020-12-04 06:36:41,052 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:36:55,242 INFO     [('reg:1e-05', 81.78), ('reg:0.0001', 81.56), ('reg:0.001', 82.0), ('reg:0.01', 80.67)]\n",
            "2020-12-04 06:36:55,242 INFO     Validation : best param found is reg = 0.001 with score             82.0\n",
            "2020-12-04 06:36:55,242 INFO     Evaluating...\n",
            "2020-12-04 06:36:58,855 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRole\n",
            "2020-12-04 06:36:58,865 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:36:59,009 INFO     Computed embeddings\n",
            "2020-12-04 06:36:59,009 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:37:12,873 INFO     [('reg:1e-05', 86.19), ('reg:0.0001', 85.97), ('reg:0.001', 86.19), ('reg:0.01', 85.97)]\n",
            "2020-12-04 06:37:12,873 INFO     Validation : best param found is reg = 1e-05 with score             86.19\n",
            "2020-12-04 06:37:12,873 INFO     Evaluating...\n",
            "2020-12-04 06:37:16,274 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRoleControl\n",
            "2020-12-04 06:37:16,285 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:37:16,360 INFO     Computed embeddings\n",
            "2020-12-04 06:37:16,360 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:37:29,591 INFO     [('reg:1e-05', 37.56), ('reg:0.0001', 37.33), ('reg:0.001', 36.89), ('reg:0.01', 36.44)]\n",
            "2020-12-04 06:37:29,591 INFO     Validation : best param found is reg = 1e-05 with score             37.56\n",
            "2020-12-04 06:37:29,591 INFO     Evaluating...\n",
            "2020-12-04 06:37:32,814 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRoleControl\n",
            "2020-12-04 06:37:32,824 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:37:32,903 INFO     Computed embeddings\n",
            "2020-12-04 06:37:32,903 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:37:46,336 INFO     [('reg:1e-05', 33.63), ('reg:0.0001', 33.85), ('reg:0.001', 33.41), ('reg:0.01', 36.53)]\n",
            "2020-12-04 06:37:46,337 INFO     Validation : best param found is reg = 0.01 with score             36.53\n",
            "2020-12-04 06:37:46,337 INFO     Evaluating...\n",
            "Probing Task Results: \n",
            "    ArgumentAddGrammarRole_Head\n",
            "        devacc: 82.0\n",
            "        testacc: 81.16\n",
            "        testF1: 70.2\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentAddGrammarRole_Tail\n",
            "        devacc: 86.19\n",
            "        testacc: 82.56\n",
            "        testF1: 72.16\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "    ArgumentGrammarRole_ControlHead\n",
            "        devacc: 37.56\n",
            "        testacc: 30.4\n",
            "        testF1: 29.93\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentGrammarRole_ControlTail\n",
            "        devacc: 36.53\n",
            "        testacc: 33.85\n",
            "        testF1: 29.2\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "2020-12-04 06:37:51,452 INFO     removing temporary unarchived model dir at /tmp/tmplrwwlv_j\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mycu4dANfRvi",
        "outputId": "01e5f89f-9c60-4f7a-dea6-b9db9c723860"
      },
      "source": [
        "# New Probing Task Evaluation on TACRED \n",
        "!python '/content/drive/My Drive/685_project/REval/REval_modified/KLnew_probing_task_evaluation.py' \\\n",
        "  --model-dir '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_bert/' \\\n",
        "  --data-dir '/content/drive/My Drive/685_project/REval/data/TACRED/' \\\n",
        "  --dataset tacred --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-12-04 06:54:43,813 INFO     loading archive file /content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_bert/model.tar.gz\n",
            "2020-12-04 06:54:43,814 INFO     extracting archive file /content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_bert/model.tar.gz to temp dir /tmp/tmphvl8acy1\n",
            "2020-12-04 06:54:48,043 INFO     instantiating registered subclass basic_relation_classifier of <class 'allennlp.models.model.Model'>\n",
            "2020-12-04 06:54:48,044 INFO     vocabulary.type = default\n",
            "2020-12-04 06:54:48,044 INFO     instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
            "2020-12-04 06:54:48,044 INFO     Loading token dictionary from /tmp/tmphvl8acy1/vocabulary.\n",
            "2020-12-04 06:54:48,045 INFO     instantiating class <class 'allennlp.models.model.Model'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained'}}, 'type': 'basic_relation_classifier', 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-12-04 06:54:48,045 INFO     model.type = basic_relation_classifier\n",
            "2020-12-04 06:54:48,045 INFO     instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'embedding_dropout': 0.5, 'encoding_dropout': 0.5, 'f1_average': 'micro', 'ignore_label': 'no_relation', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained'}}, 'verbose_metrics': False, 'word_dropout': 0} and extras {'vocab'}\n",
            "2020-12-04 06:54:48,046 INFO     instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'tokens': ['tokens', 'tokens-offsets']}, 'tokens': {'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained'}} and extras {'vocab'}\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.type = basic\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.allow_unmatched_keys = True\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.token_embedders = None\n",
            "2020-12-04 06:54:48,046 INFO     instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained'} and extras {'vocab'}\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.tokens.type = bert-pretrained\n",
            "2020-12-04 06:54:48,046 INFO     instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-cased'} and extras {'vocab'}\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.tokens.pretrained_model = bert-base-cased\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.tokens.requires_grad = False\n",
            "2020-12-04 06:54:48,046 INFO     model.text_field_embedder.tokens.top_layer_only = False\n",
            "2020-12-04 06:54:48,047 INFO     model.text_field_embedder.tokens.scalar_mix_parameters = None\n",
            "2020-12-04 06:54:48,388 INFO     loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "2020-12-04 06:54:48,389 INFO     extracting archive file /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmpytq9t617\n",
            "2020-12-04 06:54:52,045 INFO     Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "2020-12-04 06:54:54,175 INFO     instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,175 INFO     model.text_encoder.type = seq2seq_pool\n",
            "2020-12-04 06:54:54,175 INFO     instantiating class <class 'relex.modules.seq2vec_encoders.seq2seq_pool_encoder.Seq2SeqPoolEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final'} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,176 INFO     instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.type = stacked_self_attention\n",
            "2020-12-04 06:54:54,176 INFO     instantiating class <class 'allennlp.modules.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 828, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.input_dim = 828\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.hidden_dim = 256\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.projection_dim = 256\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.feedforward_hidden_dim = 512\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.num_layers = 8\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.num_attention_heads = 8\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.use_positional_encoding = False\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.dropout_prob = 0.1\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.residual_dropout_prob = 0.2\n",
            "2020-12-04 06:54:54,176 INFO     model.text_encoder.encoder.attention_dropout_prob = 0.1\n",
            "2020-12-04 06:54:54,176 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,177 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,183 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,184 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,188 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,188 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,193 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,193 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,197 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,197 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,202 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,202 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,207 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,207 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,211 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,211 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:54:54,216 INFO     model.text_encoder.pooling = final\n",
            "2020-12-04 06:54:54,216 INFO     model.text_encoder.pooling_scope = None\n",
            "2020-12-04 06:54:54,216 INFO     instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.input_dim = 256\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.num_layers = 1\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:54:54,217 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-12-04 06:54:54,217 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:54:54,217 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-12-04 06:54:54,218 INFO     type = linear\n",
            "2020-12-04 06:54:54,218 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:54:54,218 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:54:54,218 INFO     model.word_dropout = 0\n",
            "2020-12-04 06:54:54,218 INFO     model.embedding_dropout = 0.5\n",
            "2020-12-04 06:54:54,218 INFO     model.encoding_dropout = 0.5\n",
            "2020-12-04 06:54:54,219 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,219 INFO     model.offset_embedder_head.type = relative\n",
            "2020-12-04 06:54:54,219 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,219 INFO     model.offset_embedder_head.n_position = 100\n",
            "2020-12-04 06:54:54,219 INFO     model.offset_embedder_head.embedding_dim = 30\n",
            "2020-12-04 06:54:54,219 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,219 INFO     model.offset_embedder_tail.type = relative\n",
            "2020-12-04 06:54:54,219 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:54:54,220 INFO     model.offset_embedder_tail.n_position = 100\n",
            "2020-12-04 06:54:54,220 INFO     model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-12-04 06:54:54,220 INFO     model.verbose_metrics = False\n",
            "2020-12-04 06:54:54,220 INFO     model.ignore_label = no_relation\n",
            "2020-12-04 06:54:54,220 INFO     model.f1_average = micro\n",
            "2020-12-04 06:54:54,220 INFO     model.use_adjacency = False\n",
            "2020-12-04 06:54:54,220 INFO     model.use_entity_offsets = False\n",
            "2020-12-04 06:54:54,221 INFO     Initializing parameters\n",
            "2020-12-04 06:54:54,223 INFO     Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-12-04 06:54:54,223 INFO        classifier_feedforward._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        classifier_feedforward._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,223 INFO        offset_embedder_head._embedding.weight\n",
            "2020-12-04 06:54:54,223 INFO        offset_embedder_tail._embedding.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_0._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_0._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_0._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_0._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_1._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_1._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_1._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_1._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_2._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_2._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_2._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_2._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_3._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_3._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_3._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_3._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_4._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,223 INFO        text_encoder._encoder.feedforward_4._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_4._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_4._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_5._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_5._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_5._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_5._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_6._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_6._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_6._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_6._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_7._linear_layers.0.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_7._linear_layers.0.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_7._linear_layers.1.bias\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_7._linear_layers.1.weight\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_0.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_0.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_1.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_1.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_2.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_2.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_3.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_3.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_4.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_4.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_5.beta\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_5.gamma\n",
            "2020-12-04 06:54:54,224 INFO        text_encoder._encoder.feedforward_layer_norm_6.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.feedforward_layer_norm_6.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.feedforward_layer_norm_7.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.feedforward_layer_norm_7.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_0.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_0.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_1.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_1.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_2.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_2.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_3.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_3.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_4.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_4.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_5.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_5.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_6.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_6.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_7.beta\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.layer_norm_7.gamma\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_0._combined_projection.bias\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_0._combined_projection.weight\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_0._output_projection.bias\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_0._output_projection.weight\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_1._combined_projection.bias\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_1._combined_projection.weight\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_1._output_projection.bias\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_1._output_projection.weight\n",
            "2020-12-04 06:54:54,225 INFO        text_encoder._encoder.self_attention_2._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_2._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_2._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_2._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_3._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_3._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_3._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_3._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_4._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_4._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_4._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_4._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_5._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_5._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_5._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_5._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_6._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_6._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_6._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_6._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_7._combined_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_7._combined_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_7._output_projection.bias\n",
            "2020-12-04 06:54:54,226 INFO        text_encoder._encoder.self_attention_7._output_projection.weight\n",
            "2020-12-04 06:54:54,226 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.gamma\n",
            "2020-12-04 06:54:54,226 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.0\n",
            "2020-12-04 06:54:54,226 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.1\n",
            "2020-12-04 06:54:54,226 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.10\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.11\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.2\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.3\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.4\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.5\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.6\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.7\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.8\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens._scalar_mix.scalar_parameters.9\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.LayerNorm.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.LayerNorm.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.position_embeddings.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.token_type_embeddings.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.embeddings.word_embeddings.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.key.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.key.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.query.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.query.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.value.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.attention.self.value.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,227 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.dense.bias\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.0.output.dense.weight\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.key.bias\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.key.weight\n",
            "2020-12-04 06:54:54,228 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.query.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.query.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.value.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.attention.self.value.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.dense.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.1.output.dense.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.key.bias\n",
            "2020-12-04 06:54:54,305 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.key.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.query.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.query.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.value.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.attention.self.value.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.dense.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.10.output.dense.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.key.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.key.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.query.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.query.weight\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.value.bias\n",
            "2020-12-04 06:54:54,306 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.attention.self.value.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.dense.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.11.output.dense.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.key.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.key.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.query.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.query.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.value.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.attention.self.value.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,307 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.dense.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.2.output.dense.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.key.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.key.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.query.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.query.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.value.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.attention.self.value.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.dense.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.3.output.dense.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,308 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.key.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.key.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.query.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.query.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.value.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.attention.self.value.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.dense.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.4.output.dense.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.key.bias\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.key.weight\n",
            "2020-12-04 06:54:54,309 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.query.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.query.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.value.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.attention.self.value.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.dense.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.5.output.dense.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.key.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.key.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.query.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.query.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.value.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.attention.self.value.weight\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,310 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.dense.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.6.output.dense.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.key.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.key.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.query.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.query.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.value.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.attention.self.value.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.dense.bias\n",
            "2020-12-04 06:54:54,311 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.7.output.dense.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.key.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.key.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.query.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.query.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.value.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.attention.self.value.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.dense.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.8.output.dense.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.dense.bias\n",
            "2020-12-04 06:54:54,312 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.output.dense.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.key.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.key.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.query.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.query.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.value.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.attention.self.value.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.intermediate.dense.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.intermediate.dense.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.dense.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.encoder.layer.9.output.dense.weight\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.pooler.dense.bias\n",
            "2020-12-04 06:54:54,313 INFO        text_field_embedder.token_embedder_tokens.bert_model.pooler.dense.weight\n",
            "2020-12-04 06:54:59,217 INFO     instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'do_lowercase': False, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': True}}, 'type': 'tacred'} and extras set()\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.type = tacred\n",
            "2020-12-04 06:54:59,218 INFO     instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'tokens': {'do_lowercase': False, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': True}}} and extras set()\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.max_len = 100\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.masking_mode = NER+Grammar\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.lazy = False\n",
            "2020-12-04 06:54:59,218 INFO     instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': False, 'pretrained_model': 'bert-base-cased', 'type': 'bert-pretrained', 'use_starting_offsets': True} and extras set()\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.token_indexers.tokens.type = bert-pretrained\n",
            "2020-12-04 06:54:59,218 INFO     instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': False, 'pretrained_model': 'bert-base-cased', 'use_starting_offsets': True} and extras set()\n",
            "2020-12-04 06:54:59,218 INFO     dataset_reader.token_indexers.tokens.pretrained_model = bert-base-cased\n",
            "2020-12-04 06:54:59,219 INFO     dataset_reader.token_indexers.tokens.use_starting_offsets = True\n",
            "2020-12-04 06:54:59,219 INFO     dataset_reader.token_indexers.tokens.do_lowercase = False\n",
            "2020-12-04 06:54:59,219 INFO     dataset_reader.token_indexers.tokens.never_lowercase = None\n",
            "2020-12-04 06:54:59,219 INFO     dataset_reader.token_indexers.tokens.max_pieces = 512\n",
            "2020-12-04 06:54:59,219 INFO     dataset_reader.token_indexers.tokens.truncate_long_sequences = True\n",
            "2020-12-04 06:54:59,562 INFO     loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "2020-12-04 06:54:59,586 INFO     dataset_reader.dep_pruning = 1\n",
            "2020-12-04 06:54:59,587 INFO     instantiating registered subclass relation_classifier of <class 'allennlp.predictors.predictor.Predictor'>\n",
            "2020-12-04 06:54:59,587 INFO     Parameters: {\n",
            "    \"batch_size\": 64,\n",
            "    \"classifier\": {\n",
            "        \"batch_size\": 64,\n",
            "        \"dropout\": 0,\n",
            "        \"epoch_size\": 15,\n",
            "        \"nhid\": 256,\n",
            "        \"optim\": \"adam\",\n",
            "        \"tenacity\": 5\n",
            "    },\n",
            "    \"kfold\": 10,\n",
            "    \"task_path\": \"/content/drive/My Drive/685_project/REval/data/TACRED/\",\n",
            "    \"usepytorch\": true\n",
            "}\n",
            "2020-12-04 06:54:59,587 INFO     Tasks: ['ArgumentAddGrammarRole_Head', 'ArgumentAddGrammarRole_Tail', 'ArgumentGrammarRole_ControlHead', 'ArgumentGrammarRole_ControlTail']\n",
            "2020-12-04 06:54:59,642 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRole\n",
            "2020-12-04 06:54:59,642 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:55:13,350 INFO     Computed embeddings\n",
            "2020-12-04 06:55:13,350 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:55:28,478 INFO     [('reg:1e-05', 69.11), ('reg:0.0001', 69.11), ('reg:0.001', 69.33), ('reg:0.01', 69.33)]\n",
            "2020-12-04 06:55:28,479 INFO     Validation : best param found is reg = 0.001 with score             69.33\n",
            "2020-12-04 06:55:28,479 INFO     Evaluating...\n",
            "2020-12-04 06:55:32,484 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRole\n",
            "2020-12-04 06:55:32,494 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:55:32,610 INFO     Computed embeddings\n",
            "2020-12-04 06:55:32,611 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:55:51,569 INFO     [('reg:1e-05', 77.95), ('reg:0.0001', 78.4), ('reg:0.001', 78.4), ('reg:0.01', 76.39)]\n",
            "2020-12-04 06:55:51,570 INFO     Validation : best param found is reg = 0.0001 with score             78.4\n",
            "2020-12-04 06:55:51,570 INFO     Evaluating...\n",
            "2020-12-04 06:55:56,969 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRoleControl\n",
            "2020-12-04 06:55:56,979 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:55:57,006 INFO     Computed embeddings\n",
            "2020-12-04 06:55:57,006 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:56:16,959 INFO     [('reg:1e-05', 37.78), ('reg:0.0001', 37.56), ('reg:0.001', 36.44), ('reg:0.01', 35.11)]\n",
            "2020-12-04 06:56:16,959 INFO     Validation : best param found is reg = 1e-05 with score             37.78\n",
            "2020-12-04 06:56:16,960 INFO     Evaluating...\n",
            "2020-12-04 06:56:22,537 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRoleControl\n",
            "2020-12-04 06:56:22,547 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:56:22,576 INFO     Computed embeddings\n",
            "2020-12-04 06:56:22,576 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:56:40,074 INFO     [('reg:1e-05', 34.74), ('reg:0.0001', 36.08), ('reg:0.001', 36.75), ('reg:0.01', 36.08)]\n",
            "2020-12-04 06:56:40,074 INFO     Validation : best param found is reg = 0.001 with score             36.75\n",
            "2020-12-04 06:56:40,074 INFO     Evaluating...\n",
            "Probing Task Results: \n",
            "    ArgumentAddGrammarRole_Head\n",
            "        devacc: 69.33\n",
            "        testacc: 70.6\n",
            "        testF1: 33.57\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentAddGrammarRole_Tail\n",
            "        devacc: 78.4\n",
            "        testacc: 74.36\n",
            "        testF1: 48.3\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "    ArgumentGrammarRole_ControlHead\n",
            "        devacc: 37.78\n",
            "        testacc: 32.91\n",
            "        testF1: 32.32\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentGrammarRole_ControlTail\n",
            "        devacc: 36.75\n",
            "        testacc: 38.46\n",
            "        testF1: 37.37\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "2020-12-04 06:56:44,994 INFO     removing temporary unarchived model dir at /tmp/tmphvl8acy1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVv_ahr_gpq2",
        "outputId": "3679f705-bb64-4ba6-d06e-794d0d2d13ba"
      },
      "source": [
        "# New Probing Task Evaluation on TACRED \n",
        "!python '/content/drive/My Drive/685_project/REval/REval_modified/KLnew_probing_task_evaluation.py' \\\n",
        "  --model-dir '/content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_elmo/' \\\n",
        "  --data-dir '/content/drive/My Drive/685_project/REval/data/TACRED/' \\\n",
        "  --dataset tacred --cuda-device 0 --batch-size 64 --cache-representations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.2) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "2020-12-04 06:58:06,902 INFO     loading archive file /content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_elmo/model.tar.gz\n",
            "2020-12-04 06:58:06,904 INFO     extracting archive file /content/drive/My Drive/685_project/RelEx/models/baseline_self_attention_tacred_elmo/model.tar.gz to temp dir /tmp/tmp9llg5wdw\n",
            "2020-12-04 06:58:17,452 INFO     instantiating registered subclass basic_relation_classifier of <class 'allennlp.models.model.Model'>\n",
            "2020-12-04 06:58:17,452 INFO     vocabulary.type = default\n",
            "2020-12-04 06:58:17,452 INFO     instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
            "2020-12-04 06:58:17,452 INFO     Loading token dictionary from /tmp/tmp9llg5wdw/vocabulary.\n",
            "2020-12-04 06:58:17,485 INFO     instantiating class <class 'allennlp.models.model.Model'> from params {'f1_average': 'micro', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'word_dropout': 0, 'encoding_dropout': 0.5, 'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'type': 'basic_relation_classifier', 'ignore_label': 'no_relation', 'verbose_metrics': False, 'embedding_dropout': 0.5, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}, 'elmo': {'dropout': 0.5, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab'}\n",
            "2020-12-04 06:58:17,485 INFO     model.type = basic_relation_classifier\n",
            "2020-12-04 06:58:17,486 INFO     instantiating class <class 'relex.models.relation_classification.basic_relation_classifier.BasicRelationClassifier'> from params {'f1_average': 'micro', 'offset_embedder_head': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'text_encoder': {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'}, 'offset_embedder_tail': {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'}, 'word_dropout': 0, 'encoding_dropout': 0.5, 'classifier_feedforward': {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1}, 'ignore_label': 'no_relation', 'verbose_metrics': False, 'embedding_dropout': 0.5, 'text_field_embedder': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}, 'elmo': {'dropout': 0.5, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab'}\n",
            "2020-12-04 06:58:17,486 INFO     instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}, 'elmo': {'dropout': 0.5, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab'}\n",
            "2020-12-04 06:58:17,486 INFO     model.text_field_embedder.type = basic\n",
            "2020-12-04 06:58:17,486 INFO     model.text_field_embedder.embedder_to_indexer_map = None\n",
            "2020-12-04 06:58:17,486 INFO     model.text_field_embedder.allow_unmatched_keys = False\n",
            "2020-12-04 06:58:17,486 INFO     model.text_field_embedder.token_embedders = None\n",
            "2020-12-04 06:58:17,486 INFO     instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2020-12-04 06:58:17,486 INFO     model.text_field_embedder.tokens.type = embedding\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.num_embeddings = None\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.vocab_namespace = tokens\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.embedding_dim = 300\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.pretrained_file = None\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.projection_dim = None\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.trainable = False\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.padding_index = None\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.max_norm = None\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.norm_type = 2.0\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
            "2020-12-04 06:58:17,487 INFO     model.text_field_embedder.tokens.sparse = False\n",
            "2020-12-04 06:58:17,575 INFO     instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0.5, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab'}\n",
            "2020-12-04 06:58:17,575 INFO     model.text_field_embedder.elmo.type = elmo_token_embedder\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.options_file = /tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.options_file\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.weight_file = /tmp/tmp9llg5wdw/fta/model.text_field_embedder.elmo.weight_file\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.requires_grad = False\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.do_layer_norm = False\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.dropout = 0.5\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.namespace_to_cache = None\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.projection_dim = None\n",
            "2020-12-04 06:58:17,576 INFO     model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
            "2020-12-04 06:58:17,576 INFO     Initializing ELMo\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "2020-12-04 06:58:24,961 INFO     instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final', 'type': 'seq2seq_pool'} and extras {'vocab'}\n",
            "2020-12-04 06:58:24,961 INFO     model.text_encoder.type = seq2seq_pool\n",
            "2020-12-04 06:58:24,961 INFO     instantiating class <class 'relex.modules.seq2vec_encoders.seq2seq_pool_encoder.Seq2SeqPoolEncoder'> from params {'encoder': {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False}, 'pooling': 'final'} and extras {'vocab'}\n",
            "2020-12-04 06:58:24,961 INFO     instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'type': 'stacked_self_attention', 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-12-04 06:58:24,961 INFO     model.text_encoder.encoder.type = stacked_self_attention\n",
            "2020-12-04 06:58:24,961 INFO     instantiating class <class 'allennlp.modules.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder'> from params {'attention_dropout_prob': 0.1, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 512, 'hidden_dim': 256, 'input_dim': 1384, 'num_attention_heads': 8, 'num_layers': 8, 'projection_dim': 256, 'residual_dropout_prob': 0.2, 'use_positional_encoding': False} and extras {'vocab'}\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.input_dim = 1384\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.hidden_dim = 256\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.projection_dim = 256\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.feedforward_hidden_dim = 512\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.num_layers = 8\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.num_attention_heads = 8\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.use_positional_encoding = False\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.dropout_prob = 0.1\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.residual_dropout_prob = 0.2\n",
            "2020-12-04 06:58:24,962 INFO     model.text_encoder.encoder.attention_dropout_prob = 0.1\n",
            "2020-12-04 06:58:24,962 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,962 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,970 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,970 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,975 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,975 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,979 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,979 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,983 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,984 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,988 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,988 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,992 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,993 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,997 INFO     instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:24,997 INFO     instantiating registered subclass linear of <class 'allennlp.nn.activations.Activation'>\n",
            "2020-12-04 06:58:25,001 INFO     model.text_encoder.pooling = final\n",
            "2020-12-04 06:58:25,001 INFO     model.text_encoder.pooling_scope = None\n",
            "2020-12-04 06:58:25,001 INFO     instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': ['linear'], 'dropout': [0], 'hidden_dims': [42], 'input_dim': 256, 'num_layers': 1} and extras {'vocab'}\n",
            "2020-12-04 06:58:25,001 INFO     model.classifier_feedforward.input_dim = 256\n",
            "2020-12-04 06:58:25,001 INFO     model.classifier_feedforward.num_layers = 1\n",
            "2020-12-04 06:58:25,001 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:58:25,001 INFO     model.classifier_feedforward.hidden_dims = [42]\n",
            "2020-12-04 06:58:25,002 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:58:25,002 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params ['linear'] and extras {'vocab'}\n",
            "2020-12-04 06:58:25,002 INFO     model.classifier_feedforward.activations = ['linear']\n",
            "2020-12-04 06:58:25,002 INFO     instantiating class <class 'allennlp.nn.activations.Activation'> from params linear and extras {'vocab'}\n",
            "2020-12-04 06:58:25,002 INFO     type = linear\n",
            "2020-12-04 06:58:25,002 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:58:25,002 INFO     model.classifier_feedforward.dropout = [0]\n",
            "2020-12-04 06:58:25,003 INFO     model.word_dropout = 0\n",
            "2020-12-04 06:58:25,003 INFO     model.embedding_dropout = 0.5\n",
            "2020-12-04 06:58:25,003 INFO     model.encoding_dropout = 0.5\n",
            "2020-12-04 06:58:25,003 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:58:25,003 INFO     model.offset_embedder_head.type = relative\n",
            "2020-12-04 06:58:25,003 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:58:25,003 INFO     model.offset_embedder_head.n_position = 100\n",
            "2020-12-04 06:58:25,003 INFO     model.offset_embedder_head.embedding_dim = 30\n",
            "2020-12-04 06:58:25,004 INFO     instantiating class <class 'relex.modules.offset_embedders.offset_embedder.OffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100, 'type': 'relative'} and extras {'vocab'}\n",
            "2020-12-04 06:58:25,004 INFO     model.offset_embedder_tail.type = relative\n",
            "2020-12-04 06:58:25,004 INFO     instantiating class <class 'relex.modules.offset_embedders.relative_offset_embedder.RelativeOffsetEmbedder'> from params {'embedding_dim': 30, 'n_position': 100} and extras {'vocab'}\n",
            "2020-12-04 06:58:25,004 INFO     model.offset_embedder_tail.n_position = 100\n",
            "2020-12-04 06:58:25,004 INFO     model.offset_embedder_tail.embedding_dim = 30\n",
            "2020-12-04 06:58:25,005 INFO     model.verbose_metrics = False\n",
            "2020-12-04 06:58:25,005 INFO     model.ignore_label = no_relation\n",
            "2020-12-04 06:58:25,005 INFO     model.f1_average = micro\n",
            "2020-12-04 06:58:25,005 INFO     model.use_adjacency = False\n",
            "2020-12-04 06:58:25,005 INFO     model.use_entity_offsets = False\n",
            "2020-12-04 06:58:25,005 INFO     Initializing parameters\n",
            "2020-12-04 06:58:25,006 INFO     Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2020-12-04 06:58:25,006 INFO        classifier_feedforward._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,006 INFO        classifier_feedforward._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,006 INFO        offset_embedder_head._embedding.weight\n",
            "2020-12-04 06:58:25,006 INFO        offset_embedder_tail._embedding.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_0._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_0._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_0._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_0._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_1._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_1._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_1._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_1._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_2._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_2._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_2._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_2._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_3._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_3._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_3._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_3._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,007 INFO        text_encoder._encoder.feedforward_4._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_4._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_4._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_4._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_5._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_5._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_5._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_5._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_6._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_6._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_6._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_6._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_7._linear_layers.0.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_7._linear_layers.0.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_7._linear_layers.1.bias\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_7._linear_layers.1.weight\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_layer_norm_0.beta\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_layer_norm_0.gamma\n",
            "2020-12-04 06:58:25,008 INFO        text_encoder._encoder.feedforward_layer_norm_1.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_1.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_2.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_2.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_3.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_3.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_4.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_4.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_5.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_5.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_6.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_6.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_7.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.feedforward_layer_norm_7.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.layer_norm_0.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.layer_norm_0.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.layer_norm_1.beta\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.layer_norm_1.gamma\n",
            "2020-12-04 06:58:25,009 INFO        text_encoder._encoder.layer_norm_2.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_2.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_3.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_3.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_4.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_4.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_5.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_5.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_6.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_6.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_7.beta\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.layer_norm_7.gamma\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_0._combined_projection.bias\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_0._combined_projection.weight\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_0._output_projection.bias\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_0._output_projection.weight\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_1._combined_projection.bias\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_1._combined_projection.weight\n",
            "2020-12-04 06:58:25,010 INFO        text_encoder._encoder.self_attention_1._output_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_1._output_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_2._combined_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_2._combined_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_2._output_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_2._output_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_3._combined_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_3._combined_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_3._output_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_3._output_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_4._combined_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_4._combined_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_4._output_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_4._output_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_5._combined_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_5._combined_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_5._output_projection.bias\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_5._output_projection.weight\n",
            "2020-12-04 06:58:25,011 INFO        text_encoder._encoder.self_attention_6._combined_projection.bias\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_6._combined_projection.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_6._output_projection.bias\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_6._output_projection.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_7._combined_projection.bias\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_7._combined_projection.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_7._output_projection.bias\n",
            "2020-12-04 06:58:25,012 INFO        text_encoder._encoder.self_attention_7._output_projection.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
            "2020-12-04 06:58:25,012 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
            "2020-12-04 06:58:25,012 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
            "2020-12-04 06:58:25,016 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
            "2020-12-04 06:58:25,016 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
            "2020-12-04 06:58:25,017 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
            "2020-12-04 06:58:25,018 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
            "2020-12-04 06:58:25,019 INFO        text_field_embedder.token_embedder_tokens.weight\n",
            "2020-12-04 06:58:33,544 INFO     instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'tacred'} and extras set()\n",
            "2020-12-04 06:58:33,545 INFO     dataset_reader.type = tacred\n",
            "2020-12-04 06:58:33,545 INFO     instantiating class <class 'relex.dataset_readers.tacred.TacredDatasetReader'> from params {'masking_mode': 'NER+Grammar', 'max_len': 100, 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2020-12-04 06:58:33,545 INFO     dataset_reader.max_len = 100\n",
            "2020-12-04 06:58:33,545 INFO     dataset_reader.masking_mode = NER+Grammar\n",
            "2020-12-04 06:58:33,545 INFO     dataset_reader.lazy = False\n",
            "2020-12-04 06:58:33,545 INFO     instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras set()\n",
            "2020-12-04 06:58:33,545 INFO     dataset_reader.token_indexers.elmo.type = elmo_characters\n",
            "2020-12-04 06:58:33,546 INFO     instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras set()\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.elmo.tokens_to_add = None\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.elmo.token_min_padding_length = 0\n",
            "2020-12-04 06:58:33,546 INFO     instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.type = single_id\n",
            "2020-12-04 06:58:33,546 INFO     instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2020-12-04 06:58:33,546 INFO     dataset_reader.dep_pruning = 1\n",
            "2020-12-04 06:58:33,547 INFO     instantiating registered subclass relation_classifier of <class 'allennlp.predictors.predictor.Predictor'>\n",
            "2020-12-04 06:58:33,547 INFO     Parameters: {\n",
            "    \"batch_size\": 64,\n",
            "    \"classifier\": {\n",
            "        \"batch_size\": 64,\n",
            "        \"dropout\": 0,\n",
            "        \"epoch_size\": 15,\n",
            "        \"nhid\": 256,\n",
            "        \"optim\": \"adam\",\n",
            "        \"tenacity\": 5\n",
            "    },\n",
            "    \"kfold\": 10,\n",
            "    \"task_path\": \"/content/drive/My Drive/685_project/REval/data/TACRED/\",\n",
            "    \"usepytorch\": true\n",
            "}\n",
            "2020-12-04 06:58:33,547 INFO     Tasks: ['ArgumentAddGrammarRole_Head', 'ArgumentAddGrammarRole_Tail', 'ArgumentGrammarRole_ControlHead', 'ArgumentGrammarRole_ControlTail']\n",
            "2020-12-04 06:58:33,678 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRole\n",
            "2020-12-04 06:58:33,678 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:58:45,797 INFO     Computed embeddings\n",
            "2020-12-04 06:58:45,797 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:59:03,619 INFO     [('reg:1e-05', 70.67), ('reg:0.0001', 70.0), ('reg:0.001', 70.67), ('reg:0.01', 70.22)]\n",
            "2020-12-04 06:59:03,619 INFO     Validation : best param found is reg = 1e-05 with score             70.67\n",
            "2020-12-04 06:59:03,619 INFO     Evaluating...\n",
            "2020-12-04 06:59:08,152 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRole\n",
            "2020-12-04 06:59:08,162 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:59:08,533 INFO     Computed embeddings\n",
            "2020-12-04 06:59:08,534 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:59:24,953 INFO     [('reg:1e-05', 75.72), ('reg:0.0001', 75.5), ('reg:0.001', 75.95), ('reg:0.01', 75.28)]\n",
            "2020-12-04 06:59:24,954 INFO     Validation : best param found is reg = 0.001 with score             75.95\n",
            "2020-12-04 06:59:24,954 INFO     Evaluating...\n",
            "2020-12-04 06:59:29,873 INFO     Loaded 1350 train - 450 dev - 398 test for ArgumentHeadAddGrammaticalRoleControl\n",
            "2020-12-04 06:59:29,882 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:59:29,909 INFO     Computed embeddings\n",
            "2020-12-04 06:59:29,909 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 06:59:45,465 INFO     [('reg:1e-05', 35.56), ('reg:0.0001', 35.78), ('reg:0.001', 36.44), ('reg:0.01', 37.78)]\n",
            "2020-12-04 06:59:45,465 INFO     Validation : best param found is reg = 0.01 with score             37.78\n",
            "2020-12-04 06:59:45,465 INFO     Evaluating...\n",
            "2020-12-04 06:59:49,408 INFO     Loaded 1351 train - 449 dev - 390 test for ArgumentTailAddGrammaticalRoleControl\n",
            "2020-12-04 06:59:49,418 INFO     Computing embeddings for train/dev/test\n",
            "2020-12-04 06:59:49,447 INFO     Computed embeddings\n",
            "2020-12-04 06:59:49,447 INFO     Training pytorch-MLP-nhid256-adam-bs64 with standard validation..\n",
            "2020-12-04 07:00:04,666 INFO     [('reg:1e-05', 33.85), ('reg:0.0001', 33.63), ('reg:0.001', 35.63), ('reg:0.01', 36.3)]\n",
            "2020-12-04 07:00:04,666 INFO     Validation : best param found is reg = 0.01 with score             36.3\n",
            "2020-12-04 07:00:04,667 INFO     Evaluating...\n",
            "Probing Task Results: \n",
            "    ArgumentAddGrammarRole_Head\n",
            "        devacc: 70.67\n",
            "        testacc: 70.35\n",
            "        testF1: 37.06\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentAddGrammarRole_Tail\n",
            "        devacc: 75.95\n",
            "        testacc: 70.77\n",
            "        testF1: 33.86\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "    ArgumentGrammarRole_ControlHead\n",
            "        devacc: 37.78\n",
            "        testacc: 32.91\n",
            "        testF1: 30.59\n",
            "        ndev: 450\n",
            "        ntest: 398\n",
            "    ArgumentGrammarRole_ControlTail\n",
            "        devacc: 36.3\n",
            "        testacc: 33.08\n",
            "        testF1: 16.57\n",
            "        ndev: 449\n",
            "        ntest: 390\n",
            "2020-12-04 07:00:09,707 INFO     removing temporary unarchived model dir at /tmp/tmp9llg5wdw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2Tm6_M74d3"
      },
      "source": [
        "# Part 4: New Probing Task Evaluation \n",
        "\n",
        "(appositional modifier + noun compound modifier) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbg3qNSm7tNd"
      },
      "source": [
        "# New Probing Task (appositional modifier + noun compound modifier) Evaluation on Self-Attention\n",
        "# Data process\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LENdm5AS812d"
      },
      "source": [
        "annotated = None\n",
        "annotated = pd.read_excel('/content/drive/My Drive/685_project/REval/KL_XG_df.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2JaLPP2YLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1b9a25-2172-42fb-cd7c-6acadee9f224"
      },
      "source": [
        "print(len(annotated))\n",
        "print(annotated.columns)\n",
        "annotated = annotated.loc[annotated.Head_add.notnull()]\n",
        "annotated = annotated.reset_index(drop=True)\n",
        "print(len(annotated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "Index(['index', 'id', 'relation', 'Head_gram_role', 'Tail_gram_role',\n",
            "       'sentence', 'subject', 'object', 'Head_add', 'Tail_add'],\n",
            "      dtype='object')\n",
            "2201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN0wwTcn2bkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66439134-117a-4b53-b23f-c8c5730e89aa"
      },
      "source": [
        "head_GR = None; tail_GR = None\n",
        "head_GR = pd.read_csv('/content/drive/My Drive/685_project/REval/data/TACRED/argument_head_grammatical_role.txt', sep='\\t', header=None)\n",
        "tail_GR = pd.read_csv('/content/drive/My Drive/685_project/REval/data/TACRED/argument_tail_grammatical_role.txt', sep='\\t', header=None)\n",
        "print(len(head_GR))\n",
        "print(head_GR.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "105914\n",
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsUu9X_m2cFU"
      },
      "source": [
        "appos_nn_head = None; appos_nn_tail = None\n",
        "appos_nn_head = head_GR.copy(deep=True)\n",
        "appos_nn_tail = tail_GR.copy(deep=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXkfQpU2eWL"
      },
      "source": [
        "# Create dataframe for new grammar roles \n",
        "appos_nn_head[2] = 99\n",
        "appos_nn_tail[2] = 99\n",
        "for i, row in annotated.iterrows():\n",
        "  try:\n",
        "    j = appos_nn_head.loc[appos_nn_head[1] == row['id']].index.values[0]\n",
        "  except:   pass\n",
        "  appos_nn_head.loc[j, 2] = row['Head_add']\n",
        "  try:\n",
        "    k = appos_nn_tail.loc[appos_nn_tail[1] == row['id']].index.values[0]\n",
        "  except:   pass\n",
        "  appos_nn_tail.loc[k, 2] = row['Tail_add']\n",
        "appos_nn_head = appos_nn_head.loc[appos_nn_head[2] != 99]\n",
        "appos_nn_tail = appos_nn_tail.loc[appos_nn_tail[2] != 99]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORFCUa12f9W"
      },
      "source": [
        "# Randomly shuffle the data, before splitting into train, valid, and test sets\n",
        "appos_nn_head = appos_nn_head.sample(frac=1)\n",
        "appos_nn_tail = appos_nn_tail.sample(frac=1)\n",
        "appos_nn_head = appos_nn_head.reset_index(drop=True)\n",
        "appos_nn_tail = appos_nn_tail.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhGTNyPS2qG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "26287fdf-90a8-41ad-b0fc-c1e3f39ab403"
      },
      "source": [
        "appos_nn_head[1645:1655]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1645</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9845f68b6a8</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>45</td>\n",
              "      <td>59</td>\n",
              "      <td>59</td>\n",
              "      <td>O O O O O O O O O O NUMBER O O O O O O O O O O...</td>\n",
              "      <td>PRP VBZ JJ , CC NN VBZ JJ IN NN CD : DT NN VBZ...</td>\n",
              "      <td>nsubj ROOT xcomp punct cc nsubj conj xcomp cas...</td>\n",
              "      <td>2 0 2 2 2 7 2 7 10 7 10 2 14 15 2 30 19 19 30 ...</td>\n",
              "      <td>He sounds bad , but EVERYONE sounds bad in epi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb901f8da07c2</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>NUMBER O MISC O PERSON PERSON O LOCATION LOCAT...</td>\n",
              "      <td>CD JJ NNP , NNP NN IN NNP NNP , NNP NNP IN DT ...</td>\n",
              "      <td>nummod amod nsubj punct compound appos case co...</td>\n",
              "      <td>3 3 23 3 6 3 9 9 6 9 12 9 15 15 12 15 18 15 20...</td>\n",
              "      <td>Three former Chinese , Dang Ye-seo of South Ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3afb926b7b61275d7</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>O O O O O O O O O PERSON PERSON O O O O O O O ...</td>\n",
              "      <td>RB , RB IN JJ NN , NN NN NNP NNP VBZ PRP VBZ V...</td>\n",
              "      <td>advmod punct advmod case amod nmod punct compo...</td>\n",
              "      <td>12 12 12 6 6 3 12 11 11 11 12 0 15 15 12 17 15...</td>\n",
              "      <td>Meanwhile , back on commercial radio , industr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1648</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb90f7ad1b113</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>46</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>O O DURATION DURATION DURATION DURATION O O O ...</td>\n",
              "      <td>`` IN DT JJ CD NNS , NN NNS , NNS CC NN NNS VB...</td>\n",
              "      <td>punct case det amod nummod nmod punct compound...</td>\n",
              "      <td>38 6 6 6 6 17 17 9 17 9 9 9 14 9 17 17 38 19 1...</td>\n",
              "      <td>`` For the last 21 months , food manufacturers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1649</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb903ca041c54</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "      <td>O PERSON O O O O O O O O O O O O PERSON PERSON...</td>\n",
              "      <td>SYM NNP POS FW FW MD `` RB RB VB IN '' PRP$ SY...</td>\n",
              "      <td>dep nmod:poss case compound nsubj aux punct ad...</td>\n",
              "      <td>5 5 2 5 10 10 10 10 10 0 22 22 22 18 16 18 16 ...</td>\n",
              "      <td>* Aniston 's ex Sculfor would ` rather not tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1650</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9cc5a407211</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>16</td>\n",
              "      <td>PERSON O O O O O O O O PERSON PERSON O O O MIS...</td>\n",
              "      <td>NNP VBZ DT NN IN DT NN IN NNP NNP NNP IN DT RB...</td>\n",
              "      <td>nsubj cop det ROOT case det nmod case compound...</td>\n",
              "      <td>4 4 4 0 7 7 4 11 11 11 7 17 17 15 17 17 7 4</td>\n",
              "      <td>Scott is an underdog in the race against Congr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1651</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a89f600495b75df6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>O DURATION DURATION O O O O ORGANIZATION ORGAN...</td>\n",
              "      <td>IN CD NNS , PRP VBD DT NNP IN NNP CC NNP NNPS ...</td>\n",
              "      <td>case nummod nmod punct nsubj ROOT det dobj cas...</td>\n",
              "      <td>3 3 6 6 6 0 8 6 10 8 10 13 10 8 17 17 8 19 17 ...</td>\n",
              "      <td>For 27 years , she headed the Institute for De...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3afb9266afd8af8d9</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>MISC O O PERSON PERSON O DATE O O LOCATION O O...</td>\n",
              "      <td>NNP NNP NNP NNP NNP VBD NNP IN DT NNP NN WRB P...</td>\n",
              "      <td>compound compound compound compound nsubj dep ...</td>\n",
              "      <td>5 5 5 5 6 26 6 11 11 11 6 16 16 16 16 11 19 19...</td>\n",
              "      <td>Chadian Prime Minister Pascal Yoadimnadji died...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb974e4b98986</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>33</td>\n",
              "      <td>36</td>\n",
              "      <td>O O O O O O O O O O PERSON O O O ORGANIZATION ...</td>\n",
              "      <td>DT NN MD RB VB PRP IN JJ NN TO NNP : DT JJ NNP...</td>\n",
              "      <td>det nsubj aux advmod ROOT dobj case amod nmod ...</td>\n",
              "      <td>2 5 5 5 0 5 9 9 5 11 9 11 16 16 16 11 11 20 20...</td>\n",
              "      <td>The issue could potentially put him in stark c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1654</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9fc548dff8e</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>25</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>O O LOCATION O LOCATION O O O TIME TIME O O O ...</td>\n",
              "      <td>IN DT NNS , NNP : PRP VBZ CD NN IN DT NN , CC ...</td>\n",
              "      <td>case det ROOT punct root punct nsubj cop nummo...</td>\n",
              "      <td>3 3 0 3 4 10 10 10 10 5 13 13 10 10 10 18 18 1...</td>\n",
              "      <td>IN THE EVERGLADES , Florida -- It 's 7 a.m. in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0   ...                                                 11\n",
              "1645  tr  ...  He sounds bad , but EVERYONE sounds bad in epi...\n",
              "1646  tr  ...  Three former Chinese , Dang Ye-seo of South Ko...\n",
              "1647  tr  ...  Meanwhile , back on commercial radio , industr...\n",
              "1648  tr  ...  `` For the last 21 months , food manufacturers...\n",
              "1649  tr  ...  * Aniston 's ex Sculfor would ` rather not tal...\n",
              "1650  tr  ...  Scott is an underdog in the race against Congr...\n",
              "1651  tr  ...  For 27 years , she headed the Institute for De...\n",
              "1652  tr  ...  Chadian Prime Minister Pascal Yoadimnadji died...\n",
              "1653  tr  ...  The issue could potentially put him in stark c...\n",
              "1654  tr  ...  IN THE EVERGLADES , Florida -- It 's 7 a.m. in...\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZpwPSjE2rYI"
      },
      "source": [
        "# Assign some of the sentences to validation and test sets\n",
        "for i, row in appos_nn_head.iterrows():\n",
        "  if i >= 1350 and i < 1800:\n",
        "    appos_nn_head.loc[i, 0] = 'va'\n",
        "  elif i >= 1800:\n",
        "    appos_nn_head.loc[i, 0] = 'te'\n",
        "\n",
        "for j, row in appos_nn_tail.iterrows():\n",
        "  if j > 1350 and j < 1800:\n",
        "    appos_nn_tail.loc[j, 0] = 'va'\n",
        "  elif j >= 1800:\n",
        "    appos_nn_tail.loc[j, 0] = 'te'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fl4cZPt2wet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c46c4e-a6ae-4409-d254-bda6228f5e6a"
      },
      "source": [
        "print(len(appos_nn_tail.loc[appos_nn_head[0] == 'te']))\n",
        "print(len(appos_nn_tail.loc[appos_nn_head[0] == 'va']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "390\n",
            "450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xoUYrq2yH4"
      },
      "source": [
        "# output appositional/noun compound modifier data\n",
        "appos_nn_head.to_csv(r'appos_nn_head.txt', index=False,  sep='\\t', header=None)\n",
        "appos_nn_tail.to_csv(r'appos_nn_tail.txt', index=False,  sep='\\t', header=None)\n",
        "!cp appos_nn_head.txt '/content/drive/My Drive/685_project/REval/data/'\n",
        "!cp appos_nn_tail.txt '/content/drive/My Drive/685_project/REval/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzVB3FJh2yiZ"
      },
      "source": [
        "# Create control task for appos_nn_head and appos_nn_tail\n",
        "control_head = None; control_tail = None\n",
        "control_head = appos_nn_head.copy(deep=True)\n",
        "control_tail = appos_nn_tail.copy(deep=True)\n",
        "labels = [0, 5, 6]\n",
        "\n",
        "for i, row in control_head.iterrows():\n",
        "  ran_i = np.random.randint(3)\n",
        "  control_head.loc[i, 2] = labels[ran_i]\n",
        "\n",
        "for j, row in control_tail.iterrows():\n",
        "  ran_j = np.random.randint(3)\n",
        "  control_tail.loc[j, 2] = labels[ran_j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PPm5dkH22EE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "0059059c-d4b6-4842-8822-50b17519b75a"
      },
      "source": [
        "control_tail[300:306]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9592e1dd17e</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>O PERSON O O O O O O O O O O O O PERSON PERSON...</td>\n",
              "      <td>SYM NNP POS FW FW MD `` RB RB VB IN '' PRP$ SY...</td>\n",
              "      <td>dep nmod:poss case compound nsubj aux punct ad...</td>\n",
              "      <td>5 5 2 5 10 10 10 10 10 0 22 22 22 18 16 18 16 ...</td>\n",
              "      <td>* Aniston 's ex Sculfor would ` rather not tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9b43c5825f9</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>O O O O O ORDINAL O O O O O O PERSON PERSON O ...</td>\n",
              "      <td>IN DT NN IN PRP$ JJ NN -LRB- DT NN NN VBN NNP ...</td>\n",
              "      <td>case det ROOT case nmod:poss amod nmod punct d...</td>\n",
              "      <td>3 3 0 7 7 7 3 37 11 11 37 11 14 12 11 11 18 16...</td>\n",
              "      <td>After the arrival of her second child -LRB- a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9f210b5812f</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>LOCATION O O O O PERSON PERSON PERSON O O ORDI...</td>\n",
              "      <td>NNP POS JJ NNP NNP NNP NNP NNP VBD PRP$ JJ JJ ...</td>\n",
              "      <td>nmod:poss case amod compound compound compound...</td>\n",
              "      <td>8 1 8 8 8 8 8 9 0 13 13 13 9 9 17 17 9 17 22 2...</td>\n",
              "      <td>Iran 's new Foreign Minister Ali Akbar Salehi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb948f775a458</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>O O O O ORGANIZATION O PERSON O DATE NUMBER O ...</td>\n",
              "      <td>DT NN NN NN NNP VBD NNP , RB CD , VBD VBN IN N...</td>\n",
              "      <td>det compound compound compound nsubj ROOT nsub...</td>\n",
              "      <td>5 5 5 5 6 0 13 7 10 7 7 13 6 16 16 13 22 22 22...</td>\n",
              "      <td>The government news agency Telam said Almiron ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9b6c51cbef4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>O ORGANIZATION O PERSON PERSON O O O O MISC O ...</td>\n",
              "      <td>JJ NNP NNP NNP NNP , WP VBD VBN JJ NN IN DT JJ...</td>\n",
              "      <td>amod compound compound compound nsubj punct ns...</td>\n",
              "      <td>5 5 5 5 25 5 9 9 5 11 9 15 15 15 9 23 23 23 23...</td>\n",
              "      <td>Former U.N. Secretary-General Kurt Waldheim , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>tr</td>\n",
              "      <td>61b3a65fb9aea549d752</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>O O O O O O O O O O O O O PERSON PERSON O NUMB...</td>\n",
              "      <td>`` PRP VBD RB IN NNS JJ PRP , '' VBD PRP$ NN N...</td>\n",
              "      <td>punct nsubj aux neg case ccomp xcomp dobj punc...</td>\n",
              "      <td>11 6 6 6 6 11 6 7 11 11 0 15 15 15 11 15 15 11</td>\n",
              "      <td>`` He did n't like people insulting him , '' s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0   ...                                                 11\n",
              "300  tr  ...  * Aniston 's ex Sculfor would ` rather not tal...\n",
              "301  tr  ...  After the arrival of her second child -LRB- a ...\n",
              "302  tr  ...  Iran 's new Foreign Minister Ali Akbar Salehi ...\n",
              "303  tr  ...  The government news agency Telam said Almiron ...\n",
              "304  tr  ...  Former U.N. Secretary-General Kurt Waldheim , ...\n",
              "305  tr  ...  `` He did n't like people insulting him , '' s...\n",
              "\n",
              "[6 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzXE-8nJ24jH"
      },
      "source": [
        "# Output control task data\n",
        "control_head.to_csv(r'control_head.txt', index=False, sep='\\t', header=None)\n",
        "control_tail.to_csv(r'control_tail.txt', index=False, sep='\\t', header=None)\n",
        "!cp control_head.txt '/content/drive/My Drive/685_project/REval/data/'\n",
        "!cp control_tail.txt '/content/drive/My Drive/685_project/REval/data/'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}